services:
  # Database services
  postgres:
    image: postgres:15
    restart: unless-stopped
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=postgres"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    volumes:
      - redis_data:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=redis"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Langfuse - LLM observability and analytics
  langfuse:
    image: langfuse/langfuse:2
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "${LANGFUSE_PORT:-3000}:3000"
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/langfuse
      NEXTAUTH_SECRET: mysecret
      SALT: mysalt
      NEXTAUTH_URL: http://${HOST:-localhost}:${LANGFUSE_PORT:-3000}
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-true}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: ${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-false}
    volumes:
      - langfuse_data:/app/data
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=langfuse"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f next-server"]
      interval: 30s
      timeout: 10s
      retries: 3

  # FlowiseAI - Visual AI workflow builder
  flowise:
    image: flowiseai/flowise:latest
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "${FLOWISE_PORT:-3001}:3000"
    environment:
      # Basic Configuration
      PORT: 3000
      
      # Database Configuration
      DATABASE_TYPE: postgres
      DATABASE_HOST: postgres
      DATABASE_PORT: 5432
      DATABASE_USER: postgres
      DATABASE_PASSWORD: postgres
      DATABASE_NAME: flowise
      
      # Storage Configuration
      SECRETKEY_PATH: /root/.flowise
      LOG_PATH: /root/.flowise/logs
      BLOB_STORAGE_PATH: /root/.flowise/storage
      
      # Logging
      DEBUG: ${DEBUG:-false}
      LOG_LEVEL: ${LOG_LEVEL:-info}
      
      # Tool Function Dependencies
      TOOL_FUNCTION_BUILTIN_DEP: ${TOOL_FUNCTION_BUILTIN_DEP:-crypto,fs}
      TOOL_FUNCTION_EXTERNAL_DEP: ${TOOL_FUNCTION_EXTERNAL_DEP:-moment,lodash}
      
      # Settings
      CORS_ORIGINS: "*"
      IFRAME_ORIGINS: "*"
      SHOW_COMMUNITY_NODES: true
      DISABLE_FLOWISE_TELEMETRY: ${DISABLE_FLOWISE_TELEMETRY:-false}
      
      # JWT Auth Configuration (v3.0.1+ Authentication System)
      JWT_AUTH_TOKEN_SECRET: ${JWT_AUTH_TOKEN_SECRET:-AABBCCDDAABBCCDDAABBCCDDAABBCCDDAABBCCDD}
      JWT_REFRESH_TOKEN_SECRET: ${JWT_REFRESH_TOKEN_SECRET:-AABBCCDDAABBCCDDAABBCCDDAABBCCDDAABBCCDDAABBCCDD}
      JWT_ISSUER: ${JWT_ISSUER:-AI-Dev-Local}
      JWT_AUDIENCE: ${JWT_AUDIENCE:-AI-Dev-Local}
      JWT_TOKEN_EXPIRY_IN_MINUTES: ${JWT_TOKEN_EXPIRY_IN_MINUTES:-60}
      JWT_REFRESH_TOKEN_EXPIRY_IN_MINUTES: ${JWT_REFRESH_TOKEN_EXPIRY_IN_MINUTES:-129600}
      EXPRESS_SESSION_SECRET: ${EXPRESS_SESSION_SECRET:-flowise-ai-dev-local}
      EXPIRE_AUTH_TOKENS_ON_RESTART: ${EXPIRE_AUTH_TOKENS_ON_RESTART:-false}
      
      # Application Configuration
      APP_URL: ${APP_URL:-http://${HOST:-localhost}:${FLOWISE_PORT:-3001}}
      
      # Legacy Username/Password (Required for initial admin account setup)
      FLOWISE_USERNAME: ${FLOWISE_USERNAME:-admin}
      FLOWISE_PASSWORD: ${FLOWISE_PASSWORD:-admin123}
      
      # Database Override
      OVERRIDE_DATABASE: ${OVERRIDE_DATABASE:-true}
      FLOWISE_SECRETKEY_OVERWRITE: ${FLOWISE_SECRETKEY_OVERWRITE:-mySecretKey123}
      
      # Security Configuration
      PASSWORD_RESET_TOKEN_EXPIRY_IN_MINS: ${PASSWORD_RESET_TOKEN_EXPIRY_IN_MINS:-15}
      PASSWORD_SALT_HASH_ROUNDS: ${PASSWORD_SALT_HASH_ROUNDS:-12}
      TOKEN_HASH_SECRET: ${TOKEN_HASH_SECRET:-ai-dev-local-token-hash-secret-key}
      
      # API Configuration
      APIKEY_PATH: ${APIKEY_PATH:-/root/.flowise}
      EXECUTION_MODE: ${EXECUTION_MODE:-main}
      
      # Security and Access Control
      LANGCHAIN_TRACING_V2: ${LANGCHAIN_TRACING_V2:-false}
      NUMBER_OF_PROXIES: ${NUMBER_OF_PROXIES:-1}
      
      # Performance Configuration
      CHATFLOW_WORKER_TIMEOUT: ${CHATFLOW_WORKER_TIMEOUT:-30000}
      DEPLOY_TIMEOUT: ${DEPLOY_TIMEOUT:-120000}
      MAX_UPLOAD_SIZE: ${MAX_UPLOAD_SIZE:-50mb}
      
    volumes:
      - flowise_data:/root/.flowise
      - ./configs/flowise:/root/.flowise/config:ro  # Read-only config mount
    entrypoint: /bin/sh -c "sleep 3; node /root/.flowise/config/init.js || true; flowise start"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=flowise"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f flowise"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Open WebUI - Chat interface for LLMs
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    restart: unless-stopped
    ports:
      - "${OPENWEBUI_PORT:-8081}:8080"
    environment:
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      OPENAI_API_BASE_URL: ${OPENAI_API_BASE_URL:-http://litellm:4000/v1}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-sk-1234567890abcdef}
      WEBUI_SECRET_KEY: ${WEBUI_SECRET_KEY:-t0p-s3cr3t}
      WEBUI_JWT_SECRET_KEY: ${WEBUI_JWT_SECRET_KEY:-jwt-s3cr3t}
      DEFAULT_MODELS: ${DEFAULT_MODELS:-gpt-3.5-turbo,gpt-4}
      DEFAULT_USER_ROLE: ${DEFAULT_USER_ROLE:-admin}
      ENABLE_SIGNUP: ${ENABLE_SIGNUP:-true}
      ENABLE_LOGIN_FORM: ${ENABLE_LOGIN_FORM:-true}
      WEBUI_NAME: ${WEBUI_NAME:-AI Dev Local}
    volumes:
      - open_webui_data:/app/backend/data
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=open-webui"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # LiteLLM Proxy - Unified API for multiple LLM providers
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "${LITELLM_PORT:-4000}:4000"
    environment:
      # Core LiteLLM Settings
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-*******************}
      DATABASE_URL: ${LITELLM_DATABASE_URL:-postgresql://postgres:postgres@postgres:5432/litellm}
      
      # Redis Configuration
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
      
      # UI Authentication
      UI_USERNAME: ${LITELLM_UI_USERNAME:-admin}
      UI_PASSWORD: ${LITELLM_UI_PASSWORD:-admin123}
      
      # Langfuse Integration
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY:-}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY:-}
      LANGFUSE_HOST: ${LANGFUSE_HOST:-http://langfuse:3000}
      
      # API Keys for LLM Providers
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      GEMINI_API_KEY: ${GEMINI_API_KEY:-}
      COHERE_API_KEY: ${COHERE_API_KEY:-}
      
      # Logging and Debug
      LITELLM_LOG: ${LITELLM_LOG:-INFO}
      LITELLM_DEBUG: ${LITELLM_DEBUG:-false}
    volumes:
      - ./configs/litellm_config.yaml:/app/config.yaml
      - litellm_data:/app/data
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "1"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=litellm"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f litellm"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Ollama - Local LLM server (optional)
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    environment:
      OLLAMA_ORIGINS: ${OLLAMA_ORIGINS:-*}
      OLLAMA_HOST: ${OLLAMA_HOST:-0.0.0.0:11434}
      OLLAMA_MODELS: ${OLLAMA_MODELS:-/root/.ollama/models}
      # GPU settings - set OLLAMA_GPU=true in .env if you have NVIDIA GPU
      OLLAMA_GPU: ${OLLAMA_GPU:-false}
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/ollama-init.sh:/ollama-init.sh
    profiles:
      - ollama
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=ollama"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    # GPU support - uncomment if OLLAMA_GPU=true in .env
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # AI Dev Dashboard - Single web page to access all services
  dashboard:
    build:
      context: ./docker/dashboard
      dockerfile: Dockerfile
      args:
        GIT_TAG: ${GIT_TAG:-unknown}
        BUILD_DATE: ${BUILD_DATE:-unknown}
    restart: unless-stopped
    ports:
      - "${DASHBOARD_PORT:-3002}:80"
    environment:
      LANGFUSE_URL: http://${HOST:-localhost}:${LANGFUSE_PORT:-3000}
      FLOWISE_URL: http://${HOST:-localhost}:${FLOWISE_PORT:-3001}
      OPENWEBUI_URL: http://${HOST:-localhost}:${OPENWEBUI_PORT:-8081}
      LITELLM_URL: http://${HOST:-localhost}:${LITELLM_PORT:-4000}
      OLLAMA_URL: http://${HOST:-localhost}:${OLLAMA_PORT:-11434}
      DASHBOARD_TITLE: ${DASHBOARD_TITLE:-AI Dev Local Dashboard}
      APP_VERSION: ${GIT_TAG:-v0.2.1}
      BUILD_DATE: ${BUILD_DATE:-unknown}
    volumes:
      - ./.git:/app/.git:ro  # Mount git directory for version detection
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=dashboard"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MkDocs Publisher - Publishes documentation
  mkdocs:
    build:
      context: ./docker/mkdocs
      dockerfile: Dockerfile
    restart: unless-stopped
    volumes:
      - .:/docs
    ports:
      - "${MKDOCS_PORT:-8000}:8000"
    working_dir: /docs
    command: serve --dev-addr=0.0.0.0:8000
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=mkdocs"
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f mkdocs"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres_data:
  redis_data:
  langfuse_data:
  flowise_data:
  open_webui_data:
  litellm_data:
  ollama_data:

networks:
  default:
    name: ai-dev-local
    driver: bridge
