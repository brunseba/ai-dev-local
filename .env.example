# =============================================================================
# AI Dev Local Environment Configuration
# =============================================================================
# 
# This file contains all configuration variables for the AI Dev Local development environment.
# Copy this file to .env and customize the values according to your setup.
# 
# Quick Start:
# 1. Copy this file: cp .env.example .env
# 2. Edit .env with your API keys and preferred settings
# 3. Run: ai-dev-local config validate
# 4. Start services: ai-dev-local start
#
# For help managing this file, use:
# - ai-dev-local config show        # View current settings
# - ai-dev-local config set KEY VAL # Update a setting
# - ai-dev-local config validate    # Check configuration
# - ai-dev-local config edit        # Open in editor

# =============================================================================
# LLM Provider API Keys
# =============================================================================
# Configure API keys for various Large Language Model providers.
# At minimum, you need OPENAI_API_KEY for basic functionality.

# OpenAI API Key (REQUIRED)
# Get your key from: https://platform.openai.com/api-keys
# Used for: GPT models (gpt-3.5-turbo, gpt-4, etc.)
OPENAI_API_KEY=***************************

# Anthropic API Key (OPTIONAL)
# Get your key from: https://console.anthropic.com/
# Used for: Claude models (claude-3-sonnet, claude-3-haiku, etc.)
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# Google Gemini API Key (OPTIONAL)
# Get your key from: https://aistudio.google.com/app/apikey
# Used for: Gemini models (gemini-pro, gemini-pro-vision, etc.)
GEMINI_API_KEY=your-gemini-api-key-here

# Cohere API Key (OPTIONAL)
# Get your key from: https://dashboard.cohere.ai/api-keys
# Used for: Cohere models (command, command-light, etc.)
COHERE_API_KEY=your-cohere-api-key-here

# =============================================================================
# Host and Port Configuration
# =============================================================================
# Configure the hostname and ports for all services.
# HOST can be localhost, a specific hostname, or IP address.

HOST=localhost         # Hostname or IP address (localhost, 0.0.0.0, your-domain.com)

# Service Ports - Customize the ports that each service binds to
POSTGRES_PORT=5432     # PostgreSQL database
REDIS_PORT=6379        # Redis cache/queue
LANGFUSE_PORT=3000     # Langfuse LLM analytics dashboard
FLOWISE_PORT=3001      # FlowiseAI visual workflow builder
OPENWEBUI_PORT=8081    # Open WebUI chat interface
LITELLM_PORT=4000      # LiteLLM unified proxy
OLLAMA_PORT=11434      # Ollama local LLM server
DASHBOARD_PORT=3002    # AI Dev Local main dashboard
MKDOCS_PORT=8000       # Documentation site

# =============================================================================
# Langfuse Analytics Configuration
# =============================================================================
# Langfuse provides observability and analytics for your LLM applications.
# Dashboard: http://${HOST}:${LANGFUSE_PORT}

TELEMETRY_ENABLED=true
LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES=false

# Langfuse API Keys (generate these in the Langfuse dashboard)
LANGFUSE_PUBLIC_KEY=pk-your-langfuse-public-key
LANGFUSE_SECRET_KEY=***************************
LANGFUSE_HOST=http://${HOST}:${LANGFUSE_PORT}

# =============================================================================
# FlowiseAI Configuration
# =============================================================================
# FlowiseAI is a visual drag-and-drop tool for building LLM workflows.
# Dashboard: http://${HOST}:${FLOWISE_PORT}

# General Settings
DEBUG=false
LOG_LEVEL=info
DISABLE_FLOWISE_TELEMETRY=false

# Tool Function Dependencies
TOOL_FUNCTION_BUILTIN_DEP=crypto,fs
TOOL_FUNCTION_EXTERNAL_DEP=moment,lodash

# JWT Authentication (change these for security)
JWT_AUTH_TOKEN_SECRET=AABBCCDDAABBCCDDAABBCCDDAABBCCDDAABBCCDD
JWT_REFRESH_TOKEN_SECRET=AABBCCDDAABBCCDDAABBCCDDAABBCCDDAABBCCDDAABBCCDD
JWT_ISSUER=AI-Dev-Local
JWT_AUDIENCE=AI-Dev-Local
JWT_TOKEN_EXPIRY_IN_MINUTES=360
JWT_REFRESH_TOKEN_EXPIRY_IN_MINUTES=43200

# Silent Setup Configuration
FLOWISE_USERNAME=admin
FLOWISE_PASSWORD=admin123
OVERRIDE_DATABASE=true
FLOWISE_SECRETKEY_OVERWRITE=mySecretKey123

# API and Execution Configuration
APIKEY_PATH=/root/.flowise
EXECUTION_MODE=main

# Security and Access Control
LANGCHAIN_TRACING_V2=false
NUMBER_OF_PROXIES=1

# Performance Configuration
CHATFLOW_WORKER_TIMEOUT=30000
DEPLOY_TIMEOUT=120000
MAX_UPLOAD_SIZE=50mb

# =============================================================================
# Ollama Local LLM Server Configuration
# =============================================================================
# Ollama runs LLMs locally on your machine for privacy and offline usage.
# Start with --ollama: ai-dev-local start --ollama
# Dashboard: http://${HOST}:${OLLAMA_PORT}

OLLAMA_BASE_URL=http://host.docker.internal:${OLLAMA_PORT}
OLLAMA_ORIGINS=*
OLLAMA_HOST=0.0.0.0:${OLLAMA_PORT}
OLLAMA_MODELS=/root/.ollama/models

# Models to auto-pull on initialization (comma-separated)
# Popular options: llama2, codellama, mistral, phi, neural-chat, starcode
OLLAMA_AUTO_PULL_MODELS=llama2:7b,codellama:7b,mistral:7b,phi:2.7b

# GPU Support (set to true if you have NVIDIA GPU with CUDA)
OLLAMA_GPU=false

# =============================================================================
# Open WebUI Configuration
# =============================================================================
# Open WebUI provides a ChatGPT-like interface for various LLM providers.
# Dashboard: http://${HOST}:${OPENWEBUI_PORT}

OPENAI_API_BASE_URL=http://litellm:${LITELLM_PORT}/v1

# Security Keys (REQUIRED - change these!)
WEBUI_SECRET_KEY=your-webui-secret-key
WEBUI_JWT_SECRET_KEY=your-jwt-secret-key

# Default Models (shown in the interface)
DEFAULT_MODELS=gpt-3.5-turbo,gpt-4,claude-3-sonnet,llama2:7b,codellama:7b

# User Management
DEFAULT_USER_ROLE=admin
ENABLE_SIGNUP=true
ENABLE_LOGIN_FORM=true
WEBUI_NAME=AI Dev Local

# =============================================================================
# LiteLLM Proxy Configuration
# =============================================================================
# LiteLLM provides a unified API interface for multiple LLM providers.
# Dashboard: http://${HOST}:${LITELLM_PORT}

# Master API Key (REQUIRED - change this!)
LITELLM_MASTER_KEY=**************************

# Database Configuration
LITELLM_DATABASE_URL=postgresql://postgres:postgres@postgres:${POSTGRES_PORT}/litellm

# UI Authentication
LITELLM_UI_USERNAME=admin
LITELLM_UI_PASSWORD=admin123

# Logging and Debug
LITELLM_LOG=INFO
LITELLM_DEBUG=false

# =============================================================================
# Redis Configuration
# =============================================================================
# Redis is used for caching and queuing across services.

# Password (optional - leave empty for no password)
REDIS_PASSWORD=

# =============================================================================
# Dashboard Configuration
# =============================================================================
# Settings for the main AI Dev Local dashboard.
# Dashboard: http://${HOST}:${DASHBOARD_PORT}

DASHBOARD_TITLE=AI Dev Local Dashboard

# =============================================================================
# Documentation Configuration
# =============================================================================
# Settings for the MkDocs documentation site.
# Documentation: http://${HOST}:${MKDOCS_PORT}

MKDOCS_DEV_ADDR=0.0.0.0:${MKDOCS_PORT}
MKDOCS_SITE_URL=http://${HOST}:${MKDOCS_PORT}

# =============================================================================
# Database Configuration
# =============================================================================
# PostgreSQL database settings (usually don't need to change these)

POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres

# =============================================================================
# MCP (Model Context Protocol) Configuration
# =============================================================================
# MCP enables AI agents to securely access external data sources and tools.
# These settings are for advanced integrations.

# Git Configuration for MCP Git Server
GIT_AUTHOR_NAME=AI Dev Local
GIT_AUTHOR_EMAIL=dev@local.ai

# Timezone for MCP Time Server
TIMEZONE=UTC

# GitHub Integration (OPTIONAL)
# Get a Personal Access Token from: https://github.com/settings/tokens
GITHUB_PERSONAL_ACCESS_TOKEN=your-github-pat-here
GITHUB_TOOLSETS=repos,issues,pull_requests,actions,code_security,context
GITHUB_READ_ONLY=false

# GitLab Integration (OPTIONAL)
# Get an access token from: https://gitlab.com/-/profile/personal_access_tokens
GITLAB_TOKEN=your-gitlab-token-here
GITLAB_URL=https://gitlab.com

# SonarQube Integration (OPTIONAL)
# Configure if you have a SonarQube instance for code quality analysis
SONARQUBE_URL=http://localhost:9000
SONARQUBE_TOKEN=your-sonarqube-token-here

# =============================================================================
# Advanced Development Settings
# =============================================================================
# These settings are for advanced users and development purposes.

# Tool Function Dependencies (for FlowiseAI custom functions)
TOOL_FUNCTION_BUILTIN_DEP=crypto,fs
TOOL_FUNCTION_EXTERNAL_DEP=moment,lodash
