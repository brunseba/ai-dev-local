{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Dev Local","text":"<p>Welcome to AI Dev Local - a comprehensive AI lab for local development with various AI services and Model Context Protocol (MCP) integrations.</p>"},{"location":"#overview","title":"Overview","text":"<p>AI Dev Local provides a unified platform to run and manage multiple AI services locally, enabling developers to:</p> <ul> <li>Observe and analyze LLM interactions with Langfuse</li> <li>Build visual workflows with FlowiseAI</li> <li>Chat with models through Open WebUI</li> <li>Proxy multiple LLM providers with LiteLLM</li> <li>Integrate development tools via MCP servers</li> </ul>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#system-context-diagram","title":"System Context Diagram","text":"<pre><code>C4Context\n    title AI Dev Local - System Context\n\n    Person(dev, \"AI Developer\", \"Develops AI applications and experiments with models\")\n    Person(researcher, \"AI Researcher\", \"Conducts experiments and analyzes model performance\")\n    Person(devops, \"DevOps Engineer\", \"Manages infrastructure and deployment pipelines\")\n    Person(pm, \"Product Manager\", \"Tracks AI feature usage and performance metrics\")\n\n    System(aidl, \"AI Dev Local\", \"Local AI development platform with integrated services\")\n\n    System_Ext(llm_providers, \"LLM Providers\", \"OpenAI, Anthropic, Local models, etc.\")\n    System_Ext(gitlab, \"GitLab\", \"Source code management and CI/CD\")\n    System_Ext(github, \"GitHub\", \"Source code management and collaboration\")\n    System_Ext(sonarqube, \"SonarQube\", \"Code quality and security analysis\")\n    System_Ext(ide, \"IDE/Editor\", \"VS Code, Cursor, Codium with MCP support\")\n\n    Rel(dev, aidl, \"Uses CLI, builds workflows, chats with models\")\n    Rel(researcher, aidl, \"Analyzes performance, tracks experiments\")\n    Rel(devops, aidl, \"Manages services, monitors infrastructure\")\n    Rel(pm, aidl, \"Reviews usage analytics, cost tracking\")\n\n    Rel(aidl, llm_providers, \"Proxies requests, aggregates responses\")\n    Rel(aidl, gitlab, \"Integrates via MCP for issues, CI/CD\")\n    Rel(aidl, github, \"Integrates via MCP for code reviews\")\n    Rel(aidl, sonarqube, \"Integrates via MCP for quality metrics\")\n    Rel(ide, aidl, \"Connects to MCP servers\")</code></pre>"},{"location":"#component-architecture","title":"Component Architecture","text":"<pre><code>graph TB\n    subgraph \"Developer Laptop\"\n        CLI[AI Dev Local CLI]\n        IDE[IDE/Editor&lt;br/&gt;\ud83d\udcbb VS Code, Cursor, etc.]\n        BROWSER[Web Browser&lt;br/&gt;\ud83c\udf10 Access to Web UIs]\n    end\n\n    subgraph \"Web Services\"\n        LF[Langfuse&lt;br/&gt;\ud83d\udcca Observability]\n        FW[FlowiseAI&lt;br/&gt;\ud83c\udfa8 Workflow Builder]\n        OW[Open WebUI&lt;br/&gt;\ud83d\udcac Chat Interface]\n        LL[LiteLLM Proxy&lt;br/&gt;\ud83d\ude80 API Gateway]\n        DOCS[Documentation&lt;br/&gt;\ud83d\udcd6 MkDocs Server]\n    end\n\n    subgraph \"MCP Servers\"\n        GL[GitLab MCP&lt;br/&gt;\ud83d\udccb Issues &amp; CI/CD]\n        GH[GitHub MCP&lt;br/&gt;\ud83d\udd00 Code Reviews]\n        SQ[SonarQube MCP&lt;br/&gt;\ud83d\udd0d Quality Analysis]\n    end\n\n    subgraph \"External Integrations\"\n        GLR[GitLab API]\n        GHR[GitHub API]\n        SQR[SonarQube API]\n        LLMP[LLM Providers&lt;br/&gt;OpenAI, Anthropic, etc.]\n    end\n\n    CLI --&gt; LF\n    CLI --&gt; FW\n    CLI --&gt; OW\n    CLI --&gt; LL\n    CLI --&gt; DOCS\n\n    CLI --&gt; GL\n    CLI --&gt; GH\n    CLI --&gt; SQ\n\n    BROWSER --&gt; LF\n    BROWSER --&gt; FW\n    BROWSER --&gt; OW\n    BROWSER --&gt; DOCS\n\n    GL --&gt; GLR\n    GH --&gt; GHR\n    SQ --&gt; SQR\n\n    FW --&gt; LL\n    OW --&gt; LL\n    LF --&gt; LL\n    LL --&gt; LLMP\n\n    IDE -.-&gt; GL\n    IDE -.-&gt; GH\n    IDE -.-&gt; SQ\n\n    style CLI fill:#e1f5fe\n    style IDE fill:#e3f2fd\n    style BROWSER fill:#f1f8e9\n    style LF fill:#f3e5f5\n    style FW fill:#e8f5e8\n    style OW fill:#fff3e0\n    style LL fill:#fce4ec\n    style DOCS fill:#e8eaf6</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get started with AI Dev Local in minutes:</p> <pre><code># Install with pipx\npipx install ai-dev-local\n\n# Start all services\nai-dev-local start\n\n# Check status\nai-dev-local status\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#observability","title":"\ud83d\udd0d Observability","text":"<ul> <li>Track LLM usage and performance with Langfuse</li> <li>Monitor costs and latency across providers</li> <li>Debug and optimize AI workflows</li> </ul>"},{"location":"#visual-workflows","title":"\ud83c\udfa8 Visual Workflows","text":"<ul> <li>Build AI workflows with drag-and-drop interface</li> <li>Connect multiple AI services and APIs</li> <li>Test and iterate on complex AI pipelines</li> </ul>"},{"location":"#chat-interface","title":"\ud83d\udcac Chat Interface","text":"<ul> <li>Modern web interface for chatting with AI models</li> <li>Support for multiple model providers</li> <li>File uploads and conversation management</li> </ul>"},{"location":"#api-gateway","title":"\ud83d\ude80 API Gateway","text":"<ul> <li>Unified API for multiple LLM providers</li> <li>Load balancing and rate limiting</li> <li>Cost tracking and usage analytics</li> </ul>"},{"location":"#development-integration","title":"\ud83d\udd27 Development Integration","text":"<ul> <li>GitLab integration for issue tracking and CI/CD</li> <li>GitHub integration for code reviews and discussions</li> <li>SonarQube integration for code quality analysis</li> <li>IDE-side MCP servers for direct integration with VS Code, Codium, Cursor, and other editors</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Choose your setup approach:</p>"},{"location":"#quick-start-options","title":"Quick Start Options","text":"<ul> <li>Installation Guide - Full stack deployment</li> <li>Quick Start Tutorial - Get up and running fast</li> <li>IDE MCP Setup - Direct IDE integration (recommended for development)</li> </ul>"},{"location":"#configuration-troubleshooting","title":"Configuration &amp; Troubleshooting","text":"<ul> <li>Configuration Guide - Complete configuration reference</li> <li>LiteLLM Troubleshooting - Fix \"Invalid HTTP request\" warnings and API key issues</li> </ul>"},{"location":"#mcp-integration-approaches","title":"MCP Integration Approaches","text":"<ol> <li>IDE-side Integration - Run MCP servers on-demand within your editor</li> <li>Docker Compose Stack - Full containerized MCP server deployment</li> <li>Hybrid Approach - Combine both for maximum flexibility</li> </ol>"},{"location":"#version","title":"Version","text":"<p>Current version: v0.2.1</p> <p>Last updated: July 27, 2025</p>"},{"location":"CONFIGURATION/","title":"Configuration Guide","text":"<p>This guide explains how to configure AI Dev Local for your specific needs.</p>"},{"location":"CONFIGURATION/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Initialize configuration: <pre><code>ai-dev-local config init\n</code></pre></p> </li> <li> <p>Set your OpenAI API key: <pre><code>ai-dev-local config set OPENAI_API_KEY your-actual-api-key-here\n</code></pre></p> </li> <li> <p>Validate configuration: <pre><code>ai-dev-local config validate\n</code></pre></p> </li> <li> <p>Start services: <pre><code>ai-dev-local start\n</code></pre></p> </li> </ol>"},{"location":"CONFIGURATION/#configuration-management-commands","title":"Configuration Management Commands","text":""},{"location":"CONFIGURATION/#initialize-configuration","title":"Initialize Configuration","text":"<p><pre><code>ai-dev-local config init\n</code></pre> Creates <code>.env</code> file from <code>.env.example</code> template.</p>"},{"location":"CONFIGURATION/#set-configuration-values","title":"Set Configuration Values","text":"<p><pre><code>ai-dev-local config set KEY VALUE\n</code></pre> Updates a specific configuration value.</p>"},{"location":"CONFIGURATION/#view-configuration","title":"View Configuration","text":"<pre><code>ai-dev-local config show           # Show all settings\nai-dev-local config show API_KEY   # Show specific setting\n</code></pre>"},{"location":"CONFIGURATION/#validate-configuration","title":"Validate Configuration","text":"<p><pre><code>ai-dev-local config validate\n</code></pre> Checks if required settings are configured.</p>"},{"location":"CONFIGURATION/#edit-configuration","title":"Edit Configuration","text":"<p><pre><code>ai-dev-local config edit\n</code></pre> Opens <code>.env</code> file in your default editor.</p>"},{"location":"CONFIGURATION/#required-configuration","title":"Required Configuration","text":""},{"location":"CONFIGURATION/#openai-api-key-required","title":"OpenAI API Key (Required)","text":"<p><pre><code>OPENAI_API_KEY=your-openai-api-key\n</code></pre> - Get from: https://platform.openai.com/api-keys - Used for: GPT models (gpt-3.5-turbo, gpt-4, etc.)</p>"},{"location":"CONFIGURATION/#security-keys-required","title":"Security Keys (Required)","text":"<p><pre><code>WEBUI_SECRET_KEY=your-secure-secret-key\nWEBUI_JWT_SECRET_KEY=your-jwt-secret-key\nLITELLM_MASTER_KEY=your-litellm-master-key\n</code></pre> - Change default values for security - Use random, strong keys</p>"},{"location":"CONFIGURATION/#optional-llm-provider-api-keys","title":"Optional LLM Provider API Keys","text":""},{"location":"CONFIGURATION/#anthropic-claude-models","title":"Anthropic (Claude Models)","text":"<p><pre><code>ANTHROPIC_API_KEY=your-anthropic-api-key\n</code></pre> - Get from: https://console.anthropic.com/ - Enables: Claude models (claude-3-sonnet, claude-3-haiku, etc.)</p>"},{"location":"CONFIGURATION/#google-gemini","title":"Google Gemini","text":"<p><pre><code>GEMINI_API_KEY=your-gemini-api-key\n</code></pre> - Get from: https://aistudio.google.com/app/apikey - Enables: Gemini models (gemini-pro, gemini-pro-vision, etc.)</p>"},{"location":"CONFIGURATION/#cohere","title":"Cohere","text":"<p><pre><code>COHERE_API_KEY=your-cohere-api-key\n</code></pre> - Get from: https://dashboard.cohere.ai/api-keys - Enables: Cohere models (command, command-light, etc.)</p>"},{"location":"CONFIGURATION/#port-configuration","title":"Port Configuration","text":"<p>All service ports can be customized:</p> <pre><code>POSTGRES_PORT=5432     # PostgreSQL database\nREDIS_PORT=6379        # Redis cache/queue\nLANGFUSE_PORT=3000     # Langfuse analytics\nFLOWISE_PORT=3001      # FlowiseAI workflows\nOPENWEBUI_PORT=8081    # Open WebUI chat\nLITELLM_PORT=4000      # LiteLLM proxy\nOLLAMA_PORT=11434      # Ollama local LLM\nDASHBOARD_PORT=3002    # Main dashboard\nMKDOCS_PORT=8000       # Documentation\n</code></pre>"},{"location":"CONFIGURATION/#handling-port-conflicts","title":"Handling Port Conflicts","text":"<p>If you have port conflicts, update the relevant port:</p> <pre><code>ai-dev-local config set LANGFUSE_PORT 3030\nai-dev-local config set FLOWISE_PORT 3031\n</code></pre>"},{"location":"CONFIGURATION/#service-specific-configuration","title":"Service-Specific Configuration","text":""},{"location":"CONFIGURATION/#ollama-local-llms","title":"Ollama (Local LLMs)","text":"<p>Configure local LLM models:</p> <pre><code>OLLAMA_AUTO_PULL_MODELS=llama2:7b,codellama:7b,mistral:7b,phi:2.7b\nOLLAMA_GPU=false  # Set to true for NVIDIA GPU support\n</code></pre> <p>Start with Ollama: <pre><code>ai-dev-local start --ollama\nai-dev-local ollama init  # Pull configured models\n</code></pre></p>"},{"location":"CONFIGURATION/#langfuse-analytics","title":"Langfuse Analytics","text":"<p>Configure observability for your LLM applications:</p> <pre><code>LANGFUSE_PUBLIC_KEY=pk-your-public-key\nLANGFUSE_SECRET_KEY=sk-your-secret-key\nTELEMETRY_ENABLED=true\n</code></pre>"},{"location":"CONFIGURATION/#flowiseai-workflows","title":"FlowiseAI Workflows","text":"<p>Configure visual AI workflow builder:</p> <pre><code>DEBUG=false\nLOG_LEVEL=info\nDISABLE_FLOWISE_TELEMETRY=false\n</code></pre>"},{"location":"CONFIGURATION/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"CONFIGURATION/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<p>Configure advanced AI agent integrations:</p> <pre><code># GitHub integration\nGITHUB_PERSONAL_ACCESS_TOKEN=your-github-pat\nGITHUB_TOOLSETS=repos,issues,pull_requests,actions\n\n# GitLab integration\nGITLAB_TOKEN=your-gitlab-token\nGITLAB_URL=https://gitlab.com\n\n# Git configuration\nGIT_AUTHOR_NAME=Your Name\nGIT_AUTHOR_EMAIL=your@email.com\n</code></pre>"},{"location":"CONFIGURATION/#database-settings","title":"Database Settings","text":"<p>Usually don't need to change:</p> <pre><code>POSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres\nPOSTGRES_DB=postgres\n</code></pre>"},{"location":"CONFIGURATION/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"CONFIGURATION/#development-environment","title":"Development Environment","text":"<pre><code>DEBUG=true\nLOG_LEVEL=debug\nTELEMETRY_ENABLED=false\n</code></pre>"},{"location":"CONFIGURATION/#production-environment","title":"Production Environment","text":"<pre><code>DEBUG=false\nLOG_LEVEL=info\nTELEMETRY_ENABLED=true\n# Use strong, unique secrets\n</code></pre>"},{"location":"CONFIGURATION/#troubleshooting-configuration","title":"Troubleshooting Configuration","text":""},{"location":"CONFIGURATION/#check-current-configuration","title":"Check Current Configuration","text":"<pre><code>ai-dev-local config show\n</code></pre>"},{"location":"CONFIGURATION/#validate-setup","title":"Validate Setup","text":"<pre><code>ai-dev-local config validate\n</code></pre>"},{"location":"CONFIGURATION/#reset-configuration","title":"Reset Configuration","text":"<pre><code>rm .env\nai-dev-local config init\n</code></pre>"},{"location":"CONFIGURATION/#common-issues","title":"Common Issues","text":"<ol> <li>Missing API Keys: Run <code>ai-dev-local config validate</code></li> <li>Port Conflicts: Change conflicting ports in <code>.env</code></li> <li>Permission Issues: Check file permissions on <code>.env</code></li> <li>Invalid Values: Use <code>ai-dev-local config set</code> to fix</li> <li>LiteLLM Warning Messages: See LiteLLM Troubleshooting below</li> </ol>"},{"location":"CONFIGURATION/#litellm-troubleshooting","title":"LiteLLM Troubleshooting","text":"<p>If you see warning messages like <code>\"Invalid HTTP request received.\"</code> in the LiteLLM logs, this is usually caused by API authentication issues during health checks.</p>"},{"location":"CONFIGURATION/#symptoms","title":"Symptoms","text":"<ul> <li>Log messages: <code>{\"message\": \"Invalid HTTP request received.\", \"level\": \"WARNING\"}</code></li> <li>LiteLLM reports \"unhealthy_endpoints\" in health checks</li> <li>Models appear unavailable in the UI</li> </ul>"},{"location":"CONFIGURATION/#common-causes","title":"Common Causes","text":"<ol> <li>Placeholder API Keys: Your <code>.env</code> file still contains example values</li> <li>Invalid API Keys: API keys are incorrect or expired</li> <li>Missing API Keys: Required keys are empty or not set</li> </ol>"},{"location":"CONFIGURATION/#diagnosis","title":"Diagnosis","text":"<p>Check LiteLLM health status: <pre><code># Check if LiteLLM is accessible\ncurl -H \"Authorization: Bearer $(grep LITELLM_MASTER_KEY .env | cut -d'=' -f2)\" \\\n     http://localhost:4000/health\n</code></pre></p> <p>This will show which endpoints are healthy/unhealthy and why.</p>"},{"location":"CONFIGURATION/#solutions","title":"Solutions","text":"<p>1. Update Placeholder API Keys Replace example values in your <code>.env</code> file: <pre><code># BAD - placeholder values\nOPENAI_API_KEY=*********************\nANTHROPIC_API_KEY=your-anthropic-api-key-here\nGEMINI_API_KEY=your-gemini-api-key-here\n\n# GOOD - real API keys\nOPENAI_API_KEY=sk-proj-abc123...\nANTHROPIC_API_KEY=sk-ant-api03-xyz789...\nGEMINI_API_KEY=AIzaSy123...\n</code></pre></p> <p>2. Get Valid API Keys - OpenAI: https://platform.openai.com/api-keys - Anthropic: https://console.anthropic.com/ - Google Gemini: https://aistudio.google.com/app/apikey - Cohere: https://dashboard.cohere.ai/api-keys</p> <p>3. Update and Restart Services <pre><code># Update API keys\nai-dev-local config set OPENAI_API_KEY sk-proj-your-real-key\nai-dev-local config set ANTHROPIC_API_KEY sk-ant-your-real-key\n\n# Restart LiteLLM to pick up new keys\ndocker-compose restart litellm\n\n# Verify health\nai-dev-local status\n</code></pre></p> <p>4. Disable Unused Providers If you don't plan to use certain providers, you can comment them out in <code>configs/litellm_config.yaml</code>: <pre><code>model_list:\n  # OpenAI Models (keep these if you have OPENAI_API_KEY)\n  - model_name: gpt-4\n    litellm_params:\n      model: openai/gpt-4\n      api_key: os.environ/OPENAI_API_KEY\n\n  # Comment out unused providers to avoid health check failures\n  # - model_name: claude-3-opus\n  #   litellm_params:\n  #     model: anthropic/claude-3-opus-20240229\n  #     api_key: os.environ/ANTHROPIC_API_KEY\n</code></pre></p>"},{"location":"CONFIGURATION/#expected-behavior-after-fix","title":"Expected Behavior After Fix","text":"<ul> <li>No more \"Invalid HTTP request received\" warnings</li> <li>Health check shows <code>\"healthy_endpoints\"</code> instead of empty array</li> <li>Models become available in Open WebUI and other interfaces</li> <li>LiteLLM proxy responds correctly to API requests</li> </ul>"},{"location":"CONFIGURATION/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Never commit <code>.env</code> to version control</li> <li>Use strong, unique secrets for all keys</li> <li>Regularly rotate API keys</li> <li>Limit API key permissions where possible</li> <li>Use environment-specific configurations</li> </ol>"},{"location":"CONFIGURATION/#configuration-templates","title":"Configuration Templates","text":""},{"location":"CONFIGURATION/#minimal-setup-openai-only","title":"Minimal Setup (OpenAI only)","text":"<pre><code>OPENAI_API_KEY=your-openai-key\nWEBUI_SECRET_KEY=your-secret-key\nLITellm_MASTER_KEY=your-master-key\n</code></pre>"},{"location":"CONFIGURATION/#full-setup-all-providers","title":"Full Setup (All Providers)","text":"<pre><code>OPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\nGEMINI_API_KEY=your-gemini-key\nCOHERE_API_KEY=your-cohere-key\nWEBUI_SECRET_KEY=your-secret-key\nLITELLM_MASTER_KEY=your-master-key\nLANGFUSE_PUBLIC_KEY=your-langfuse-public-key\nLANGFUSE_SECRET_KEY=your-langfuse-secret-key\n</code></pre>"},{"location":"CONFIGURATION/#local-only-setup-with-ollama","title":"Local-Only Setup (with Ollama)","text":"<pre><code>WEBUI_SECRET_KEY=your-secret-key\nLITELLM_MASTER_KEY=your-master-key\nOLLAMA_AUTO_PULL_MODELS=llama2:7b,codellama:7b\nOLLAMA_GPU=true  # if you have NVIDIA GPU\n</code></pre> <p>For more help, run <code>ai-dev-local --help</code> or check the main documentation.</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/","title":"Documentation Updates Summary","text":"<p>This document summarizes all the documentation updates made to reflect the new IDE MCP integration capabilities.</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#files-updated","title":"Files Updated","text":""},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#1-docsindexmd-main-documentation-index","title":"1. <code>docs/index.md</code> - Main Documentation Index","text":"<p>Changes Made: - Added IDE-side MCP servers as a key development integration feature - Updated Getting Started section with three setup approaches:   - IDE MCP Setup (recommended for development)   - Installation Guide (full stack deployment)   - Quick Start Tutorial (get up and running fast) - Added MCP Integration Approaches section explaining:   - IDE-side Integration (on-demand servers)   - Docker Compose Stack (full containerized deployment)   - Hybrid Approach (combining both methods)</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#2-docschangelogmd-project-changelog","title":"2. <code>docs/changelog.md</code> - Project Changelog","text":"<p>Changes Made: - Added comprehensive IDE MCP Configuration Files section documenting:   - VS Code/Codium workspace MCP configuration (<code>.vscode/mcp.json</code>)   - Global IDE MCP configuration template (<code>configs/ide-mcp/vscode-mcp.json</code>)   - Support for GitHub, GitLab, and SonarQube MCP servers - Added Comprehensive IDE MCP Setup Guide section documenting:   - Step-by-step setup instructions for multiple IDEs   - Token configuration and security guidelines   - Troubleshooting and debugging information   - Usage examples and best practices - Added Enhanced MCP Integration Documentation section covering:   - Updated Phase 3 MCP integration plan   - Architecture diagrams and implementation roadmap   - Security considerations and deployment options</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#3-docsphase_3_mcp_integrationmd-mcp-implementation-plan","title":"3. <code>docs/PHASE_3_MCP_INTEGRATION.md</code> - MCP Implementation Plan","text":"<p>Changes Made: - Restructured Implementation Plan into three approaches:   - Approach 1: IDE-Side Integration (\u2705 Complete)   - Approach 2: Docker Compose Stack (\ud83d\udd04 Planned)    - Approach 3: Hybrid Setup (\ud83d\udd04 Planned) - Added comprehensive status tracking and configuration details - Updated Next Steps with phased implementation:   - Phase 3.1: IDE Integration (Complete)   - Phase 3.2: Docker Compose Stack (Planned)   - Phase 3.3: Advanced Features (Future) - Added Getting Started section with quick start instructions - Added Current Implementation Status with clear progress indicators</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#4-readmemd-main-project-readme","title":"4. <code>README.md</code> - Main Project README","text":"<p>Changes Made: - Added IDE Integration as a key feature - Enhanced Quick Start section with two options:   - Option 1: Full Stack Deployment (existing approach)   - Option 2: IDE MCP Integration (new recommended approach for development) - Updated MCP Services table with IDE Support column showing configuration status - Added IDE Integration note with link to detailed setup guide - Enhanced Project Structure section showing new configuration files:   - <code>docs/IDE_MCP_SETUP.md</code> - IDE MCP integration guide   - <code>docs/PHASE_3_MCP_INTEGRATION.md</code> - MCP implementation plan   - <code>configs/ide-mcp/vscode-mcp.json</code> - Global VS Code/Codium MCP config   - <code>.vscode/mcp.json</code> - Workspace MCP configuration</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#5-docside_mcp_setupmd-new-comprehensive-setup-guide","title":"5. <code>docs/IDE_MCP_SETUP.md</code> - New Comprehensive Setup Guide","text":"<p>Created: A complete 200+ line guide covering: - Overview and benefits of IDE-side MCP integration - Prerequisites (IDE requirements, Docker, access tokens) - Three setup methods (workspace-specific, global, other IDEs) - Detailed usage examples for each MCP server - Configuration options and customization - Troubleshooting section with common issues - Security considerations - Alternative remote MCP server options - Next steps and advanced features</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#key-improvements","title":"Key Improvements","text":""},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#1-user-experience","title":"1. User Experience","text":"<ul> <li>Clear separation between full stack deployment and IDE integration</li> <li>Step-by-step instructions for multiple IDE setups</li> <li>Comprehensive troubleshooting guide</li> </ul>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#2-documentation-structure","title":"2. Documentation Structure","text":"<ul> <li>Logical progression from overview to detailed implementation</li> <li>Cross-references between related documents</li> <li>Status indicators showing what's complete vs. planned</li> </ul>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#3-configuration-management","title":"3. Configuration Management","text":"<ul> <li>Pre-built configuration files for immediate use</li> <li>Both workspace-specific and global setup options</li> <li>Support for multiple IDEs and editors</li> </ul>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#4-security-focus","title":"4. Security Focus","text":"<ul> <li>Detailed token setup instructions</li> <li>Security considerations and best practices</li> <li>Secure token management through IDE input prompts</li> </ul>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#mcp-server-configurations","title":"MCP Server Configurations","text":"<p>The documentation now reflects the actual configuration files:</p>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#global-configuration-configside-mcpvscode-mcpjson","title":"Global Configuration (<code>configs/ide-mcp/vscode-mcp.json</code>)","text":"<ul> <li>Standard Docker container execution</li> <li>No workspace mounting (for general use)</li> <li>Secure token input prompts</li> </ul>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#workspace-configuration-vscodemcpjson","title":"Workspace Configuration (<code>.vscode/mcp.json</code>)","text":"<ul> <li>Workspace-specific Docker container execution  </li> <li>Automatic workspace mounting at <code>/workspace</code></li> <li>Same secure token management</li> </ul>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#supported-mcp-servers","title":"Supported MCP Servers","text":"<ol> <li>GitHub MCP Server (<code>ghcr.io/github/github-mcp-server</code>)</li> <li>Toolsets: repos, issues, pull_requests, actions, code_security, context</li> <li>GitLab MCP Server (<code>zereight/gitlab-mcp</code>)</li> <li>Default GitLab.com integration with custom URL support</li> <li>SonarQube MCP Server (<code>sonarsource/sonarqube-mcp-server</code>)</li> <li>Local SonarQube instance support (localhost:9000)</li> </ol>"},{"location":"DOCUMENTATION_UPDATES_SUMMARY/#next-steps","title":"Next Steps","text":"<p>The documentation is now ready to support: 1. Immediate IDE integration using the provided configuration files 2. Future Docker Compose implementation (Phase 3.2) 3. Advanced MCP server additions (Phase 3.3) 4. Team collaboration features and multi-user configurations</p> <p>All documentation cross-references are properly linked and the user journey from initial setup to advanced usage is clearly defined.</p>"},{"location":"IDE_MCP_SETUP/","title":"IDE-Side MCP Setup Guide","text":"<p>This guide explains how to use MCP servers directly within your IDE (VS Code, Codium, Cursor, etc.) rather than running them as containerized services.</p>"},{"location":"IDE_MCP_SETUP/#overview","title":"Overview","text":"<p>Instead of running MCP servers in Docker containers as part of your development stack, you can configure your IDE to run MCP servers on-demand. This approach provides:</p> <ul> <li>Direct IDE integration with GitHub, GitLab, and SonarQube</li> <li>Per-project configuration flexibility</li> <li>Resource efficiency (servers only run when needed)</li> <li>Easier debugging and development workflow</li> </ul>"},{"location":"IDE_MCP_SETUP/#mcp-servers-from-inputsreadmemd","title":"MCP Servers from inputs/README.md","text":"<p>Based on your <code>inputs/README.md</code>, we'll set up these MCP servers:</p> <ol> <li>GitHub MCP Server - Official GitHub integration</li> <li>GitLab MCP Server - GitLab repository management  </li> <li>SonarQube MCP Server - Code quality analysis</li> </ol>"},{"location":"IDE_MCP_SETUP/#prerequisites","title":"Prerequisites","text":""},{"location":"IDE_MCP_SETUP/#1-ide-requirements","title":"1. IDE Requirements","text":"<ul> <li>VS Code: Version 1.101+ (for MCP support)</li> <li>Codium: Latest version with MCP extension support</li> <li>Cursor: Latest version with MCP support</li> <li>Windsurf: Latest version</li> </ul>"},{"location":"IDE_MCP_SETUP/#2-docker","title":"2. Docker","text":"<p>All MCP servers run in Docker containers, so you need: - Docker Desktop installed and running - Access to pull public Docker images</p>"},{"location":"IDE_MCP_SETUP/#3-access-tokens","title":"3. Access Tokens","text":""},{"location":"IDE_MCP_SETUP/#github-personal-access-token","title":"GitHub Personal Access Token","text":"<ol> <li>Go to GitHub Settings &gt; Developer settings &gt; Personal access tokens</li> <li>Create a fine-grained personal access token with these permissions:</li> <li>Repository permissions: Read access to code, issues, pull requests, actions</li> <li>Account permissions: Read access to user profile</li> <li>Save the token securely</li> </ol>"},{"location":"IDE_MCP_SETUP/#gitlab-personal-access-token-optional","title":"GitLab Personal Access Token (Optional)","text":"<ol> <li>Go to GitLab Settings &gt; Access Tokens</li> <li>Create a token with these scopes:</li> <li><code>read_api</code>, <code>read_repository</code>, <code>read_user</code></li> <li>Save the token securely</li> </ol>"},{"location":"IDE_MCP_SETUP/#sonarqube-token-optional","title":"SonarQube Token (Optional)","text":"<ol> <li>In your SonarQube instance, go to User &gt; My Account &gt; Security</li> <li>Generate a new token with appropriate permissions</li> <li>Save the token securely</li> </ol>"},{"location":"IDE_MCP_SETUP/#setup-instructions","title":"Setup Instructions","text":""},{"location":"IDE_MCP_SETUP/#method-1-workspace-specific-configuration-recommended","title":"Method 1: Workspace-Specific Configuration (Recommended)","text":"<p>This method configures MCP servers for a specific project/workspace.</p> <ol> <li> <p>Copy the workspace MCP configuration:    <pre><code># The .vscode/mcp.json file is already created in your project\n# You can customize it for your specific needs\n</code></pre></p> </li> <li> <p>Open your project in VS Code/Codium</p> </li> <li> <p>Enable GitHub Copilot Agent Mode (if using VS Code with Copilot):</p> </li> <li>Open Command Palette (<code>Ctrl+Shift+P</code> / <code>Cmd+Shift+P</code>)</li> <li>Type \"Copilot: Toggle Agent Mode\"</li> <li> <p>Enable Agent Mode</p> </li> <li> <p>The IDE will prompt for tokens when first using MCP features</p> </li> </ol>"},{"location":"IDE_MCP_SETUP/#method-2-global-user-configuration","title":"Method 2: Global User Configuration","text":"<p>This method configures MCP servers globally for all projects.</p>"},{"location":"IDE_MCP_SETUP/#for-vs-code","title":"For VS Code:","text":"<ol> <li>Open VS Code Settings (<code>Ctrl+,</code> / <code>Cmd+,</code>)</li> <li>Open Settings JSON (click the <code>{}</code> icon in the top right)</li> <li>Add the MCP configuration from <code>configs/ide-mcp/vscode-mcp.json</code></li> <li>Save and restart VS Code</li> </ol>"},{"location":"IDE_MCP_SETUP/#for-codium","title":"For Codium:","text":"<ol> <li>Open Codium Settings (<code>Ctrl+,</code> / <code>Cmd+,</code>)</li> <li>Open Settings JSON</li> <li>Add the same MCP configuration (Codium follows VS Code's format)</li> <li>Save and restart Codium</li> </ol>"},{"location":"IDE_MCP_SETUP/#method-3-using-github-copilot-in-other-ides","title":"Method 3: Using GitHub Copilot in Other IDEs","text":"<p>For JetBrains IDEs, Visual Studio, Eclipse, etc. with GitHub Copilot:</p> <ol> <li>Add MCP configuration to your IDE's Copilot settings</li> <li>Use the format from <code>configs/ide-mcp/vscode-mcp.json</code> (adapt syntax as needed)</li> <li>Restart your IDE</li> </ol>"},{"location":"IDE_MCP_SETUP/#usage-examples","title":"Usage Examples","text":""},{"location":"IDE_MCP_SETUP/#github-mcp-server-capabilities","title":"GitHub MCP Server Capabilities","text":"<p>Once configured, you can ask your AI assistant to:</p> <pre><code># Repository Management\n\"Show me the recent commits in this repository\"\n\"Create a new issue for the bug we just found\"\n\"List all open pull requests\"\n\n# Code Analysis\n\"Analyze the security findings in this repo\"\n\"Show me all Dependabot alerts\"\n\"Check the status of GitHub Actions workflows\"\n\n# Project Management\n\"Create a pull request for my current branch\"\n\"Add a comment to issue #123\"\n\"Merge pull request #456\"\n</code></pre>"},{"location":"IDE_MCP_SETUP/#gitlab-mcp-server-capabilities","title":"GitLab MCP Server Capabilities","text":"<pre><code># Repository Operations\n\"Show me merge requests for this project\"\n\"Create a new issue in GitLab\"\n\"Check pipeline status\"\n\n# Project Management\n\"List project milestones\"\n\"Show recent activity\"\n\"Create a merge request\"\n</code></pre>"},{"location":"IDE_MCP_SETUP/#sonarqube-mcp-server-capabilities","title":"SonarQube MCP Server Capabilities","text":"<pre><code># Code Quality\n\"Show code quality metrics for this project\"\n\"List all security vulnerabilities\"\n\"Analyze code coverage reports\"\n\n# Issue Management\n\"Show critical issues in the codebase\"\n\"Generate a quality gate report\"\n\"List technical debt items\"\n</code></pre>"},{"location":"IDE_MCP_SETUP/#configuration-options","title":"Configuration Options","text":""},{"location":"IDE_MCP_SETUP/#github-server-options","title":"GitHub Server Options","text":"<p>You can customize the GitHub MCP server by modifying the environment variables:</p> <pre><code>{\n  \"env\": {\n    \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${input:github_token}\",\n    \"GITHUB_TOOLSETS\": \"repos,issues,pull_requests,actions,code_security,context\",\n    \"GITHUB_READ_ONLY\": \"false\",\n    \"GITHUB_HOST\": \"https://github.com\"  // For GitHub Enterprise\n  }\n}\n</code></pre> <p>Available toolsets: - <code>context</code> - User context and GitHub info (recommended) - <code>repos</code> - Repository operations - <code>issues</code> - Issue management - <code>pull_requests</code> - PR management - <code>actions</code> - GitHub Actions/CI-CD - <code>code_security</code> - Security scanning - <code>discussions</code> - GitHub Discussions - <code>notifications</code> - Notification management - <code>orgs</code> - Organization management - <code>users</code> - User operations</p>"},{"location":"IDE_MCP_SETUP/#gitlab-server-options","title":"GitLab Server Options","text":"<pre><code>{\n  \"env\": {\n    \"GITLAB_TOKEN\": \"${input:gitlab_token}\",\n    \"GITLAB_URL\": \"https://gitlab.com\",  // Change for self-hosted\n    \"GITLAB_API_VERSION\": \"v4\"\n  }\n}\n</code></pre>"},{"location":"IDE_MCP_SETUP/#sonarqube-server-options","title":"SonarQube Server Options","text":"<pre><code>{\n  \"env\": {\n    \"SONARQUBE_TOKEN\": \"${input:sonarqube_token}\", \n    \"SONARQUBE_URL\": \"http://localhost:9000\",  // Your SonarQube instance\n    \"SONARQUBE_ORGANIZATION\": \"your-org\"       // For SonarCloud\n  }\n}\n</code></pre>"},{"location":"IDE_MCP_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"IDE_MCP_SETUP/#common-issues","title":"Common Issues","text":"<ol> <li>\"Docker image not found\"</li> <li>Ensure Docker is running</li> <li> <p>Pull images manually: <code>docker pull ghcr.io/github/github-mcp-server</code></p> </li> <li> <p>\"Authentication failed\"</p> </li> <li>Verify your access tokens are correct and not expired</li> <li> <p>Check token permissions match requirements</p> </li> <li> <p>\"MCP server not responding\"</p> </li> <li>Check Docker logs: <code>docker logs &lt;container-id&gt;</code></li> <li> <p>Verify network connectivity to GitHub/GitLab/SonarQube</p> </li> <li> <p>\"VS Code not recognizing MCP configuration\"</p> </li> <li>Ensure VS Code version is 1.101+</li> <li>Restart VS Code after configuration changes</li> <li>Check the JSON syntax is valid</li> </ol>"},{"location":"IDE_MCP_SETUP/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging by adding to your MCP server configuration:</p> <pre><code>{\n  \"env\": {\n    \"DEBUG\": \"true\",\n    \"LOG_LEVEL\": \"debug\"\n  }\n}\n</code></pre>"},{"location":"IDE_MCP_SETUP/#testing-mcp-connection","title":"Testing MCP Connection","text":"<p>You can test MCP servers directly:</p> <pre><code># Test GitHub MCP server\ndocker run -it --rm -e GITHUB_PERSONAL_ACCESS_TOKEN=your_token ghcr.io/github/github-mcp-server\n\n# Test GitLab MCP server  \ndocker run -it --rm -e GITLAB_TOKEN=your_token zereight/gitlab-mcp\n\n# Test SonarQube MCP server\ndocker run -it --rm -e SONARQUBE_TOKEN=your_token sonarsource/sonarqube-mcp-server\n</code></pre>"},{"location":"IDE_MCP_SETUP/#security-considerations","title":"Security Considerations","text":"<ol> <li>Token Storage: IDE configurations store tokens securely, but avoid committing them to version control</li> <li>Workspace Isolation: Use workspace-specific configurations for sensitive projects</li> <li>Read-Only Mode: Enable read-only mode for GitHub server in production environments</li> <li>Network Access: MCP servers run in containers with network access to external services</li> </ol>"},{"location":"IDE_MCP_SETUP/#alternative-remote-mcp-servers","title":"Alternative: Remote MCP Servers","text":"<p>For GitHub, you can also use the hosted remote MCP server:</p> <pre><code>{\n  \"servers\": {\n    \"github\": {\n      \"type\": \"http\",\n      \"url\": \"https://api.githubcopilot.com/mcp/\"\n    }\n  }\n}\n</code></pre> <p>This doesn't require running Docker containers locally but requires GitHub Copilot subscription.</p>"},{"location":"IDE_MCP_SETUP/#next-steps","title":"Next Steps","text":"<ol> <li>Choose your setup method (workspace-specific recommended)</li> <li>Configure your IDE with the appropriate MCP settings</li> <li>Obtain required access tokens for the services you want to use</li> <li>Test the integration by asking your AI assistant to interact with GitHub/GitLab/SonarQube</li> <li>Explore advanced features like automated issue creation, PR management, and code quality analysis</li> </ol> <p>The IDE-side MCP integration provides powerful capabilities for AI-assisted development workflows while keeping the setup flexible and resource-efficient.</p>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/","title":"MCP Servers Functionality Table","text":"<p>This table provides a comprehensive overview of all MCP servers configured in <code>docker-compose.mcp.yml</code>, including their functionalities, configurations, and available primitives (tools/resources).</p>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#mcp-servers-overview","title":"MCP Servers Overview","text":"Server Name Description Image Port Key Environment Variables Available Primitives Git Server Git repository operations <code>mcp/git:latest</code> 9001:8000 <code>GIT_AUTHOR_NAME</code>, <code>GIT_AUTHOR_EMAIL</code>, <code>GIT_COMMITTER_NAME</code>, <code>GIT_COMMITTER_EMAIL</code> Tools: <code>git_add</code>, <code>git_commit</code>, <code>git_push</code>, <code>git_pull</code>, <code>git_clone</code>, <code>git_status</code>, <code>git_diff</code>, <code>git_log</code>, <code>git_branch</code>, <code>git_checkout</code>, <code>git_merge</code>, <code>git_reset</code>, <code>git_stash</code>Resources: <code>repository_info</code>, <code>commit_history</code>, <code>branch_list</code>, <code>file_changes</code> Filesystem Secure file operations <code>mcp/filesystem:latest</code> 9002:8000 <code>ALLOWED_DIRECTORIES</code>, <code>READ_ONLY_DIRECTORIES</code> Tools: <code>read_file</code>, <code>write_file</code>, <code>create_directory</code>, <code>list_directory</code>, <code>move_file</code>, <code>copy_file</code>, <code>delete_file</code>, <code>get_file_info</code>, <code>search_files</code>Resources: <code>file_tree</code>, <code>directory_structure</code>, <code>file_metadata</code> Fetch Server Web content retrieval <code>mcp/fetch:latest</code> 9003:8000 <code>USER_AGENT</code>, <code>MAX_RESPONSE_SIZE</code>, <code>TIMEOUT</code> Tools: <code>fetch</code>, <code>fetch_html</code>, <code>fetch_text</code>, <code>fetch_json</code>, <code>fetch_pdf</code>Resources: <code>web_content</code>, <code>html_structure</code>, <code>extracted_text</code>, <code>page_metadata</code> Memory Server Persistent knowledge graph <code>mcp/memory:latest</code> 9004:8000 <code>MEMORY_STORE_PATH</code>, <code>MAX_MEMORY_SIZE</code> Tools: <code>create_entities</code>, <code>create_relations</code>, <code>add_observations</code>, <code>delete_entities</code>, <code>delete_relations</code>, <code>delete_observations</code>, <code>read_graph</code>, <code>search_nodes</code>, <code>open_nodes</code>Resources: <code>knowledge_graph</code>, <code>entity_relations</code>, <code>observation_history</code> Time Server Time and timezone utilities <code>mcp/time:latest</code> 9005:8000 <code>DEFAULT_TIMEZONE</code> Tools: <code>get_current_time</code>, <code>convert_timezone</code>, <code>format_datetime</code>, <code>parse_datetime</code>, <code>add_time</code>, <code>subtract_time</code>, <code>get_timezone_info</code>Resources: <code>current_time</code>, <code>timezone_list</code>, <code>time_formats</code> PostgreSQL Database operations <code>postgres:15-alpine</code> + MCP wrapper 9006:8000 <code>DATABASE_URL</code>, <code>POSTGRES_HOST</code>, <code>POSTGRES_PORT</code>, <code>POSTGRES_DB</code>, <code>POSTGRES_USER</code>, <code>POSTGRES_PASSWORD</code> Tools: <code>query</code>, <code>execute</code>, <code>list_tables</code>, <code>describe_table</code>, <code>create_table</code>, <code>drop_table</code>, <code>insert</code>, <code>update</code>, <code>delete</code>Resources: <code>schema_info</code>, <code>table_structure</code>, <code>query_results</code>, <code>database_stats</code> Everything Reference/test server <code>mcp/everything:latest</code> 9007:8000 <code>DEMO_MODE</code> Tools: <code>echo</code>, <code>add</code>, <code>longRunningOperation</code>, <code>get_prompt</code>, <code>sampleLLM</code>Resources: <code>demo_resource</code>, <code>sample_data</code>, <code>test_endpoints</code> GitHub Official GitHub integration with HTTP wrapper Custom build (<code>./docker/mcp-github</code>) + <code>ghcr.io/github/github-mcp-server:latest</code> 9008:8000 <code>GITHUB_PERSONAL_ACCESS_TOKEN</code>, <code>GITHUB_TOOLSETS</code>, <code>GITHUB_READ_ONLY</code> GitLab GitLab integration <code>zereight/gitlab-mcp:latest</code> 9009:8000 <code>GITLAB_TOKEN</code>, <code>GITLAB_URL</code>, <code>GITLAB_API_VERSION</code> Tools: <code>get_project</code>, <code>list_projects</code>, <code>create_project</code>, <code>get_issue</code>, <code>list_issues</code>, <code>create_issue</code>, <code>get_merge_request</code>, <code>list_merge_requests</code>, <code>create_merge_request</code>, <code>get_pipeline</code>, <code>list_pipelines</code>, <code>create_branch</code>, <code>list_branches</code>, <code>get_file</code>, <code>create_file</code>, <code>update_file</code>, <code>delete_file</code>, <code>get_user</code>, <code>list_users</code>Resources: <code>project_info</code>, <code>issue_details</code>, <code>merge_request_details</code>, <code>pipeline_status</code>, <code>file_contents</code>, <code>user_profile</code> SonarQube Code quality analysis <code>sonarsource/sonarqube-mcp-server:latest</code> 9010:8000 <code>SONARQUBE_URL</code>, <code>SONARQUBE_TOKEN</code>, <code>SONARQUBE_ORGANIZATION</code> Tools: <code>get_project</code>, <code>list_projects</code>, <code>get_issues</code>, <code>search_issues</code>, <code>get_measures</code>, <code>get_metrics</code>, <code>get_hotspots</code>, <code>get_duplications</code>, <code>get_coverage</code>, <code>get_tests</code>, <code>get_components</code>, <code>search_components</code>, <code>get_rules</code>, <code>search_rules</code>Resources: <code>project_metrics</code>, <code>code_issues</code>, <code>security_hotspots</code>, <code>code_coverage</code>, <code>duplications</code>, <code>test_results</code>, <code>quality_gate_status</code> MCP Gateway Central routing and management Custom build (<code>./docker/mcp-gateway</code>) 9000:8080 <code>MCP_SERVERS</code>, <code>GATEWAY_PORT</code> Tools: <code>route_request</code>, <code>list_servers</code>, <code>get_server_status</code>, <code>proxy_call</code>Resources: <code>server_registry</code>, <code>routing_table</code>, <code>health_status</code>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#server-categories","title":"Server Categories","text":""},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#core-development-tools","title":"\ud83d\udd27 Core Development Tools","text":"<ul> <li>Git Server: Version control operations</li> <li>Filesystem Server: File system operations with security controls</li> <li>Fetch Server: External content retrieval</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#data-storage","title":"\ud83d\udcbe Data &amp; Storage","text":"<ul> <li>Memory Server: Knowledge graph and persistent memory</li> <li>PostgreSQL Server: Database operations and queries</li> <li>Time Server: Temporal utilities and timezone handling</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#platform-integration","title":"\ud83c\udfe2 Platform Integration","text":"<ul> <li>GitHub Server: Complete GitHub ecosystem integration</li> <li>GitLab Server: GitLab repository and CI/CD management</li> <li>SonarQube Server: Code quality and security analysis</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#infrastructure","title":"\ud83d\udee0\ufe0f Infrastructure","text":"<ul> <li>Everything Server: Testing and reference implementation</li> <li>MCP Gateway: Central routing and server management</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#github-server-toolsets","title":"GitHub Server Toolsets","text":"<p>The GitHub server supports modular toolsets that can be configured via <code>GITHUB_TOOLSETS</code>:</p> Toolset Description Key Tools <code>context</code> User context and GitHub info <code>get_user</code>, <code>get_authenticated_user</code> <code>repos</code> Repository operations <code>create_repository</code>, <code>get_repository</code>, <code>list_repositories</code>, <code>fork_repository</code> <code>issues</code> Issue management <code>create_issue</code>, <code>get_issue</code>, <code>list_issues</code>, <code>update_issue</code>, <code>create_comment</code> <code>pull_requests</code> PR management <code>create_pull_request</code>, <code>get_pull_request</code>, <code>list_pull_requests</code>, <code>merge_pull_request</code> <code>actions</code> GitHub Actions/CI-CD <code>list_workflows</code>, <code>get_workflow_run</code>, <code>trigger_workflow</code> <code>code_security</code> Security scanning <code>list_security_alerts</code>, <code>get_vulnerability_alert</code>, <code>list_dependabot_alerts</code> <code>discussions</code> GitHub Discussions <code>create_discussion</code>, <code>get_discussion</code>, <code>list_discussions</code> <code>notifications</code> Notification management <code>list_notifications</code>, <code>mark_as_read</code> <code>orgs</code> Organization management <code>get_organization</code>, <code>list_org_members</code>, <code>list_org_repositories</code> <code>users</code> User operations <code>get_user</code>, <code>list_users</code>, <code>follow_user</code>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#configuration-notes","title":"Configuration Notes","text":""},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#volume-mounts","title":"Volume Mounts","text":"<ul> <li>Git Server: Read-only workspace access, Git config, SSH keys</li> <li>Filesystem Server: Full workspace access with security controls</li> <li>Memory Server: Persistent data volume (<code>mcp_memory_data</code>)</li> <li>PostgreSQL: Configuration directory mount</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#network-configuration","title":"Network Configuration","text":"<ul> <li>All servers run on the <code>ai-dev-mcp</code> bridge network</li> <li>Subnet: <code>172.20.0.0/16</code> (masked for security)</li> <li>Internal communication on port 8000</li> <li>External access via mapped ports (9001-9010, 9000 for gateway)</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#health-checks","title":"Health Checks","text":"<ul> <li>Core MCP Servers: Python module import tests</li> <li>External Services: HTTP health endpoint checks (<code>/health</code>)</li> <li>PostgreSQL: Database readiness checks (<code>pg_isready</code>)</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#security-considerations","title":"Security Considerations","text":"<ul> <li>Filesystem Server: Directory access controls and read-only restrictions</li> <li>GitHub/GitLab/SonarQube: Token-based authentication</li> <li>Memory Server: Size limits (1GB default)</li> <li>Fetch Server: Response size limits (10MB default)</li> </ul>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#usage-examples","title":"Usage Examples","text":""},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#git-operations","title":"Git Operations","text":"<pre><code># Via MCP client\nmcp-call git_status\nmcp-call git_commit --message \"Update documentation\"\n</code></pre>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#file-operations","title":"File Operations","text":"<pre><code># Via MCP client\nmcp-call read_file --path \"/workspace/README.md\"\nmcp-call list_directory --path \"/workspace\"\n</code></pre>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#github-integration","title":"GitHub Integration","text":"<pre><code># Via MCP client\nmcp-call create_issue --title \"Bug Report\" --body \"Description\"\nmcp-call list_pull_requests --repository \"owner/repo\"\n</code></pre>"},{"location":"MCP_SERVERS_FUNCTIONALITY_TABLE/#knowledge-graph","title":"Knowledge Graph","text":"<pre><code># Via MCP client\nmcp-call create_entities --entities '[{\"name\": \"project\", \"entityType\": \"software\"}]'\nmcp-call add_observations --observations '[{\"content\": \"Project uses Python\"}]'\n</code></pre> <p>This comprehensive table provides all the necessary information to understand and utilize the MCP servers configured in your Docker Compose setup.</p>"},{"location":"PHASE_3_MCP_INTEGRATION/","title":"Phase 3: MCP (Model Context Protocol) Integration","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#overview","title":"Overview","text":"<p>Model Context Protocol (MCP) enables AI assistants to securely connect to external data sources and tools. This phase integrates essential MCP servers into our AI development stack to enhance AI agent capabilities.</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#mcp-architecture","title":"MCP Architecture","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"AI Consumers\"\n        Claude[\"\ud83e\udd16 Claude Desktop\"]\n        VSCode[\"\ud83d\udcdd VS Code/Codium\"]\n        Cursor[\"\ud83d\uddb1\ufe0f Cursor\"]\n        FlowiseAI[\"\ud83c\udf0a FlowiseAI\"]\n        LiteLLM[\"\u26a1 LiteLLM Proxy\"]\n        OpenWebUI[\"\ud83c\udf10 Open WebUI\"]\n    end\n\n    subgraph \"MCP Transport Layer\"\n        Transport[\"\ud83d\udce1 MCP Transport\\n(stdio, SSE, WebSocket)\"]\n    end\n\n    subgraph \"MCP Servers\"\n        subgraph \"Core Development\"\n            Git[\"\ud83d\udcc2 Git Server\"]\n            FS[\"\ud83d\udcc1 Filesystem\"]\n            Fetch[\"\ud83c\udf10 Fetch Server\"]\n        end\n\n        subgraph \"Platform Integration\"\n            GitHub[\"\ud83d\udc19 GitHub\"]\n            GitLab[\"\ud83e\udd8a GitLab\"]\n            SonarQube[\"\ud83d\udd0d SonarQube\"]\n        end\n\n        subgraph \"Data &amp; Storage\"\n            PostgreSQL[\"\ud83d\udc18 PostgreSQL\"]\n            Memory[\"\ud83e\udde0 Memory\"]\n            Time[\"\u23f0 Time\"]\n        end\n    end\n\n    subgraph \"External Services\"\n        GHRepo[\"GitHub.com\"]\n        GLRepo[\"GitLab.com\"]\n        SQInstance[\"SonarQube Instance\"]\n        Database[\"PostgreSQL DB\"]\n    end\n\n    Claude --&gt; Transport\n    VSCode --&gt; Transport\n    Cursor --&gt; Transport\n    FlowiseAI --&gt; Transport\n    LiteLLM --&gt; Transport\n    OpenWebUI --&gt; Transport\n\n    Transport --&gt; Git\n    Transport --&gt; FS\n    Transport --&gt; Fetch\n    Transport --&gt; GitHub\n    Transport --&gt; GitLab\n    Transport --&gt; SonarQube\n    Transport --&gt; PostgreSQL\n    Transport --&gt; Memory\n    Transport --&gt; Time\n\n    GitHub &lt;--&gt; GHRepo\n    GitLab &lt;--&gt; GLRepo\n    SonarQube &lt;--&gt; SQInstance\n    PostgreSQL &lt;--&gt; Database</code></pre>"},{"location":"PHASE_3_MCP_INTEGRATION/#mcp-protocol-flow","title":"MCP Protocol Flow","text":"<pre><code>sequenceDiagram\n    participant Client as AI Client\n    participant MCP as MCP Transport\n    participant Server as MCP Server\n    participant External as External Service\n\n    Client-&gt;&gt;MCP: Initialize connection\n    MCP-&gt;&gt;Server: Handshake\n    Server-&gt;&gt;MCP: Capabilities\n    MCP-&gt;&gt;Client: Available tools/resources\n\n    Client-&gt;&gt;MCP: Call tool/resource\n    MCP-&gt;&gt;Server: Execute request\n    Server-&gt;&gt;External: API call (if needed)\n    External-&gt;&gt;Server: Response\n    Server-&gt;&gt;MCP: Result\n    MCP-&gt;&gt;Client: Response</code></pre>"},{"location":"PHASE_3_MCP_INTEGRATION/#selected-mcp-servers","title":"Selected MCP Servers","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#1-core-development-tools","title":"1. Core Development Tools","text":"<ul> <li>Git MCP Server: Repository management, version control operations</li> <li>Filesystem MCP Server: Secure file operations with access controls</li> <li>Fetch MCP Server: Web content retrieval and conversion</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#2-database-storage","title":"2. Database &amp; Storage","text":"<ul> <li>PostgreSQL MCP Server: Database schema inspection and queries</li> <li>Memory MCP Server: Persistent knowledge graph storage</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#3-development-platform-integration","title":"3. Development Platform Integration","text":"<ul> <li>GitHub MCP Server: Official GitHub server for repos, issues, PRs, actions, code security</li> <li>GitLab MCP Server: GitLab integration for repository management</li> <li>SonarQube MCP Server: Code quality analysis and security scanning</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#4-utility-services","title":"4. Utility Services","text":"<ul> <li>Time MCP Server: Time and timezone utilities</li> <li>Everything MCP Server: Reference server for testing and demonstrations</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#implementation-plan","title":"Implementation Plan","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#approach-1-ide-side-integration-recommended-for-development","title":"Approach 1: IDE-Side Integration (Recommended for Development)","text":"<p>Status: \u2705 Complete</p> <p>Direct IDE integration provides the most flexible and resource-efficient approach for development workflows:</p> <ol> <li>Configuration Files Created:</li> <li>Workspace-specific: <code>.vscode/mcp.json</code></li> <li>Global template: <code>configs/ide-mcp/vscode-mcp.json</code></li> <li> <p>Support for VS Code, Codium, Cursor, and other MCP-compatible editors</p> </li> <li> <p>Supported MCP Servers:</p> </li> <li>GitHub MCP Server (<code>ghcr.io/github/github-mcp-server</code>)</li> <li>GitLab MCP Server (<code>zereight/gitlab-mcp</code>)</li> <li> <p>SonarQube MCP Server (<code>sonarsource/sonarqube-mcp-server</code>)</p> </li> <li> <p>Key Benefits:</p> </li> <li>On-demand server execution (only when needed)</li> <li>Direct workspace mounting for seamless file access</li> <li>Secure token management through IDE input prompts</li> <li>No additional infrastructure requirements</li> </ol>"},{"location":"PHASE_3_MCP_INTEGRATION/#approach-2-docker-compose-stack-for-productionteam-use","title":"Approach 2: Docker Compose Stack (For Production/Team Use)","text":"<p>Status: \ud83d\udd04 Planned</p> <p>Full containerized deployment for team environments and production use:</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#docker-compose-architecture","title":"Docker Compose Architecture","text":"<pre><code>graph TB\n    subgraph \"AI Dev Local Stack\"\n        subgraph \"Server Services\"\n            Langfuse[\"\ud83d\udcca Langfuse\\nPort 3000\"]\n            FlowiseAI[\"\ud83c\udf0a FlowiseAI\\nPort 3001\"]\n            OpenWebUI[\"\ud83c\udf10 Open WebUI\\nPort 8080\"]\n            LiteLLM[\"\u26a1 LiteLLM Proxy\\nPort 4000\"]\n        end\n\n        subgraph \"MCP Gateway\"\n            Gateway[\"\ud83d\udeaa MCP Gateway\\nPort 9000\"]\n        end\n\n        subgraph \"MCP Servers\"\n            Git[\"\ud83d\udcc2 Git\\nPort 9001\"]\n            FS[\"\ud83d\udcc1 Filesystem\\nPort 9002\"]\n            Fetch[\"\ud83c\udf10 Fetch\\nPort 9003\"]\n            Memory[\"\ud83e\udde0 Memory\\nPort 9004\"]\n            Time[\"\u23f0 Time\\nPort 9005\"]\n            PostgreSQL[\"\ud83d\udc18 PostgreSQL\\nPort 9006\"]\n            Everything[\"\ud83d\udee0\ufe0f Everything\\nPort 9007\"]\n            GitHub[\"\ud83d\udc19 GitHub\\nPort 9008\"]\n            GitLab[\"\ud83e\udd8a GitLab\\nPort 9009\"]\n            SonarQube[\"\ud83d\udd0d SonarQube\\nPort 9010\"]\n        end\n    end\n\n    subgraph \"Network\"\n        MCPNetwork[\"ai-dev-mcp\\n172.20.0.0/16\"]\n    end\n\n    FlowiseAI &lt;--&gt; Gateway\n    LiteLLM &lt;--&gt; Gateway\n    OpenWebUI &lt;--&gt; Gateway\n    Langfuse &lt;--&gt; LiteLLM\n\n    Gateway --&gt; Git\n    Gateway --&gt; FS\n    Gateway --&gt; Fetch\n    Gateway --&gt; Memory\n    Gateway --&gt; Time\n    Gateway --&gt; PostgreSQL\n    Gateway --&gt; Everything\n    Gateway --&gt; GitHub\n    Gateway --&gt; GitLab\n    Gateway --&gt; SonarQube\n\n    Git -.-&gt; MCPNetwork\n    FS -.-&gt; MCPNetwork\n    Fetch -.-&gt; MCPNetwork\n    Memory -.-&gt; MCPNetwork\n    Time -.-&gt; MCPNetwork\n    PostgreSQL -.-&gt; MCPNetwork\n    Everything -.-&gt; MCPNetwork\n    GitHub -.-&gt; MCPNetwork\n    GitLab -.-&gt; MCPNetwork\n    SonarQube -.-&gt; MCPNetwork</code></pre>"},{"location":"PHASE_3_MCP_INTEGRATION/#step-1-mcp-infrastructure-setup","title":"Step 1: MCP Infrastructure Setup","text":"<p>Create MCP server orchestration using Docker Compose to manage multiple MCP servers with proper isolation and networking.</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#step-2-core-server-integration","title":"Step 2: Core Server Integration","text":"<p>Start with essential servers that directly support development workflows: 1. Git server for repository operations 2. Filesystem server for code file access 3. PostgreSQL server for database interactions</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#step-3-enhanced-capabilities","title":"Step 3: Enhanced Capabilities","text":"<p>Add specialized servers for enhanced AI functionality: 1. Memory server for persistent context 2. Fetch server for web research 3. Time server for scheduling and logging</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#step-4-client-integration","title":"Step 4: Client Integration","text":"<p>Configure MCP clients to connect AI services (Claude, LiteLLM, FlowiseAI) to the MCP servers through proper transport protocols.</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#approach-3-hybrid-setup","title":"Approach 3: Hybrid Setup","text":"<p>Status: \ud83d\udd04 Planned</p> <p>Combine both approaches for maximum flexibility: - IDE-side integration for development and debugging - Docker Compose stack for shared services and production features - Seamless switching between approaches based on use case</p>"},{"location":"PHASE_3_MCP_INTEGRATION/#flowiseai-mcp-integration","title":"FlowiseAI MCP Integration","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#flowiseai-as-mcp-consumer","title":"FlowiseAI as MCP Consumer","text":"<p>FlowiseAI can consume MCP servers to enhance its workflow capabilities:</p> <pre><code>graph LR\n    subgraph \"FlowiseAI Workflows\"\n        Workflow1[\"\ud83c\udfaf Code Review\\nWorkflow\"]\n        Workflow2[\"\ud83d\udd0d Quality Analysis\\nWorkflow\"]\n        Workflow3[\"\ud83d\udcca Project Insights\\nWorkflow\"]\n    end\n\n    subgraph \"MCP Tools\"\n        GitHub[\"\ud83d\udc19 GitHub Tools\"]\n        SonarQube[\"\ud83d\udd0d SonarQube Tools\"]\n        Memory[\"\ud83e\udde0 Memory Tools\"]\n        FS[\"\ud83d\udcc1 File Tools\"]\n    end\n\n    Workflow1 --&gt; GitHub\n    Workflow1 --&gt; FS\n    Workflow2 --&gt; SonarQube\n    Workflow2 --&gt; GitHub\n    Workflow3 --&gt; Memory\n    Workflow3 --&gt; GitHub\n    Workflow3 --&gt; SonarQube</code></pre>"},{"location":"PHASE_3_MCP_INTEGRATION/#integration-benefits","title":"Integration Benefits","text":"<ol> <li>Dynamic Workflows: FlowiseAI can dynamically access development tools</li> <li>Real-time Data: Direct access to repositories, issues, and quality metrics</li> <li>Contextual Processing: Persistent memory for workflow context</li> <li>Automated Analysis: Combine multiple MCP servers for comprehensive analysis</li> </ol>"},{"location":"PHASE_3_MCP_INTEGRATION/#example-flowiseai-workflows","title":"Example FlowiseAI Workflows","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#code-review-workflow","title":"Code Review Workflow","text":"<pre><code>workflow:\n  name: \"Automated Code Review\"\n  steps:\n    1. fetch_pull_request:  # GitHub MCP\n        tool: \"get_pull_request\"\n        params: {pr_number: \"{{input.pr_number}}\"}\n\n    2. analyze_changes:     # Filesystem MCP\n        tool: \"read_file\"\n        params: {path: \"{{step1.changed_files}}\"}\n\n    3. quality_check:       # SonarQube MCP\n        tool: \"get_issues\"\n        params: {project_key: \"{{input.project}}\"}\n\n    4. store_context:       # Memory MCP\n        tool: \"add_observations\"\n        params: {content: \"{{steps.1-3.results}}\"}\n</code></pre>"},{"location":"PHASE_3_MCP_INTEGRATION/#benefits-for-ai-development","title":"Benefits for AI Development","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#for-ai-agents","title":"For AI Agents","text":"<ul> <li>Secure Tool Access: Controlled access to development tools</li> <li>Persistent Memory: Knowledge graphs for context retention</li> <li>Multi-modal Capabilities: Text, file, database, and web interactions</li> <li>Workflow Orchestration: FlowiseAI can orchestrate complex multi-step processes</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#for-developers","title":"For Developers","text":"<ul> <li>Enhanced AI Assistance: AI can directly interact with your development environment</li> <li>Workflow Integration: Seamless integration with existing development tools</li> <li>Extensible Architecture: Easy to add new capabilities through additional MCP servers</li> <li>Visual Workflow Design: FlowiseAI provides drag-and-drop workflow creation</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#for-projects","title":"For Projects","text":"<ul> <li>Code Understanding: AI can analyze repositories, understand project structure</li> <li>Database Interactions: Direct database queries and schema exploration</li> <li>Research Capabilities: Web content fetching for documentation and research</li> <li>Automated Workflows: FlowiseAI can automate complex development processes</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#security-considerations","title":"Security Considerations","text":"<ul> <li>Sandboxed Execution: Each MCP server runs in isolated containers</li> <li>Access Controls: Filesystem server with configurable access permissions</li> <li>Network Isolation: Proper Docker networking to limit server communication</li> <li>Audit Logging: All MCP interactions logged for security monitoring</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#getting-started","title":"Getting Started","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#quick-start-with-ide-integration","title":"Quick Start with IDE Integration","text":"<ol> <li>Follow the IDE Setup Guide: See <code>docs/IDE_MCP_SETUP.md</code> for detailed instructions</li> <li>Configure Your IDE: Use the provided configuration files in <code>configs/ide-mcp/</code> or <code>.vscode/</code></li> <li>Obtain Access Tokens: GitHub, GitLab, and SonarQube personal access tokens</li> <li>Test Integration: Ask your AI assistant to interact with your repositories</li> </ol>"},{"location":"PHASE_3_MCP_INTEGRATION/#current-implementation-status","title":"Current Implementation Status","text":"<ul> <li>\u2705 IDE MCP Configuration: Complete with VS Code/Codium support</li> <li>\u2705 Documentation: Comprehensive setup and usage guides</li> <li>\u2705 Security Framework: Token-based authentication and secure configurations</li> <li>\ud83d\udd04 Docker Compose Stack: Planned for Phase 3.2</li> <li>\ud83d\udd04 Additional MCP Servers: Git, Filesystem, PostgreSQL, Memory servers planned</li> </ul>"},{"location":"PHASE_3_MCP_INTEGRATION/#next-steps","title":"Next Steps","text":""},{"location":"PHASE_3_MCP_INTEGRATION/#phase-31-ide-integration-complete","title":"Phase 3.1: IDE Integration (Complete)","text":"<ol> <li>\u2705 IDE MCP Configuration: Created workspace and global configurations</li> <li>\u2705 Documentation: Comprehensive setup guide with troubleshooting</li> <li>\u2705 Security Guidelines: Token management and access control documentation</li> </ol>"},{"location":"PHASE_3_MCP_INTEGRATION/#phase-32-docker-compose-stack-planned","title":"Phase 3.2: Docker Compose Stack (Planned)","text":"<ol> <li>Infrastructure Setup: Create MCP Docker Compose configuration</li> <li>Server Deployment: Deploy core MCP servers with proper configuration</li> <li>Client Integration: Configure AI services to use containerized MCP servers</li> <li>FlowiseAI Integration: Configure FlowiseAI workflows to consume MCP servers</li> <li>Testing &amp; Validation: Verify MCP functionality across the stack</li> </ol>"},{"location":"PHASE_3_MCP_INTEGRATION/#phase-33-advanced-features-future","title":"Phase 3.3: Advanced Features (Future)","text":"<ol> <li>Additional MCP Servers: Git, Filesystem, PostgreSQL, Memory, Fetch, Time servers</li> <li>Hybrid Configuration: Seamless switching between IDE and containerized approaches</li> <li>Team Collaboration: Multi-user MCP server configurations</li> <li>Advanced Security: RBAC, audit logging, and access controls</li> <li>FlowiseAI Templates: Pre-built workflow templates for common development tasks</li> <li>Cross-Server Orchestration: Complex workflows spanning multiple MCP servers</li> </ol> <p>This MCP integration transforms your AI development environment into a truly intelligent, context-aware system where AI agents can actively participate in development workflows with secure access to all necessary tools and data sources.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#added","title":"Added","text":""},{"location":"changelog/#changed","title":"Changed","text":""},{"location":"changelog/#fixed","title":"Fixed","text":""},{"location":"changelog/#021-2025-01-27","title":"0.2.1 - 2025-01-27","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Comprehensive Installation Guide (<code>docs/getting-started/installation.md</code>):</li> <li>Multiple installation methods (pipx, pip, development)</li> <li>System setup instructions for Docker and prerequisites</li> <li>Platform-specific guidance (macOS, Ubuntu/Debian, Windows)</li> <li>Post-installation verification and troubleshooting</li> <li>Quick Start Tutorial (<code>docs/getting-started/quick-start.md</code>):</li> <li>Step-by-step tutorial for new users</li> <li>Basic configuration and service launch</li> <li>Overview of key features and services</li> <li>Service Documentation:</li> <li>Langfuse service guide (<code>docs/services/langfuse.md</code>)</li> <li>FlowiseAI service guide (<code>docs/services/flowiseai.md</code>)</li> <li>Open WebUI service guide (<code>docs/services/open-webui.md</code>)</li> <li>LiteLLM Proxy service guide (<code>docs/services/litellm.md</code>)</li> <li>LiteLLM Troubleshooting Section:</li> <li>Detailed troubleshooting for \"Invalid HTTP request\" warnings</li> <li>API key configuration and validation steps</li> <li>Health check diagnostics and solutions</li> <li>Common issues and their resolutions</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Updated main documentation index with troubleshooting references</li> <li>Improved navigation structure in documentation</li> <li>Enhanced user experience with better cross-references</li> </ul>"},{"location":"changelog/#020-2024-07-27","title":"0.2.0 - 2024-07-27","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Initial project structure with Python 3.10+ support</li> <li>CLI framework with Click</li> <li>Pre-commit hooks for code quality</li> <li>MkDocs documentation with Material theme</li> <li>GitHub Actions workflows for CI/CD</li> <li>Docker support preparation</li> <li>Unit testing framework with pytest</li> <li>IDE MCP Configuration Files:</li> <li>VS Code/Codium workspace MCP configuration (<code>.vscode/mcp.json</code>)</li> <li>Global IDE MCP configuration template (<code>configs/ide-mcp/vscode-mcp.json</code>)</li> <li>Support for GitHub, GitLab, and SonarQube MCP servers</li> <li>Comprehensive IDE MCP Setup Guide (<code>docs/IDE_MCP_SETUP.md</code>):</li> <li>Step-by-step setup instructions for multiple IDEs</li> <li>Token configuration and security guidelines</li> <li>Troubleshooting and debugging information</li> <li>Usage examples and best practices</li> <li>Enhanced MCP Integration Documentation:</li> <li>Updated Phase 3 MCP integration plan</li> <li>Architecture diagrams and implementation roadmap</li> <li>Security considerations and deployment options</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":""},{"location":"changelog/#deprecated","title":"Deprecated","text":""},{"location":"changelog/#removed","title":"Removed","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":""},{"location":"changelog/#security","title":"Security","text":""},{"location":"changelog/#010-2024-07-27","title":"0.1.0 - 2024-07-27","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Initial project setup</li> <li>Basic CLI structure</li> <li>Documentation framework</li> <li>Development tooling</li> </ul>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>AI Dev Local CLI provides various commands to manage and operate AI services effectively.</p>"},{"location":"cli-reference/#command-schema-overview","title":"Command Schema Overview","text":"<p>The AI Dev Local CLI follows a hierarchical command structure with clear categories and consistent patterns:</p> <pre><code>graph LR\n    A[ai-dev-local] --&gt; B[Service Management]\n    A --&gt; C[Configuration Management]\n    A --&gt; D[Ollama Management]\n    A --&gt; E[Browser Commands]\n    A --&gt; F[Utility Commands]\n\n    %% Service Management\n    B --&gt; B1[start]\n    B --&gt; B2[stop]\n    B --&gt; B3[status]\n    B --&gt; B4[logs]\n    B1 --&gt; B1a[\"--ollama&lt;br/&gt;--build\"]\n    B4 --&gt; B4a[\"[SERVICE]\"]\n\n    %% Configuration Management\n    C --&gt; C0[config]\n    C0 --&gt; C1[init]\n    C0 --&gt; C2[set]\n    C0 --&gt; C3[show]\n    C0 --&gt; C4[validate]\n    C0 --&gt; C5[list]\n    C0 --&gt; C6[edit]\n    C2 --&gt; C2a[\"&amp;lt;KEY&amp;gt; &amp;lt;VALUE&amp;gt;\"]\n    C3 --&gt; C3a[\"[KEY]\"]\n    C5 --&gt; C5a[\"--category\"]\n\n    %% Ollama Management\n    D --&gt; D0[ollama]\n    D0 --&gt; D1[init]\n    D0 --&gt; D2[list-available]\n    D0 --&gt; D3[models]\n    D0 --&gt; D4[pull]\n    D0 --&gt; D5[remove]\n    D0 --&gt; D6[sync-litellm]\n    D1 --&gt; D1a[\"--models\"]\n    D2 --&gt; D2a[\"--search&lt;br/&gt;--category&lt;br/&gt;--format\"]\n    D4 --&gt; D4a[\"&amp;lt;MODEL&amp;gt;\"]\n    D5 --&gt; D5a[\"&amp;lt;MODEL&amp;gt;\"]\n    D6 --&gt; D6a[\"--dry-run&lt;br/&gt;--no-backup\"]\n\n    %% Browser Commands\n    E --&gt; E1[dashboard]\n    E --&gt; E2[docs]\n    E --&gt; E3[docs-reload]\n\n    %% Utility Commands\n    F --&gt; F1[version]\n    F --&gt; F2[--help]\n\n    %% Styling\n    classDef rootClass fill:#1e3a8a,stroke:#1e40af,stroke-width:3px,color:#fff\n    classDef categoryClass fill:#059669,stroke:#047857,stroke-width:2px,color:#fff\n    classDef commandClass fill:#2563eb,stroke:#1d4ed8,stroke-width:2px,color:#fff\n    classDef subCommandClass fill:#7c3aed,stroke:#6d28d9,stroke-width:2px,color:#fff\n    classDef optionClass fill:#dc2626,stroke:#b91c1c,stroke-width:1px,color:#fff\n    classDef argumentClass fill:#ea580c,stroke:#c2410c,stroke-width:1px,color:#fff\n\n    class A rootClass\n    class B,C,D,E,F categoryClass\n    class B1,B2,B3,B4,E1,E2,E3,F1,F2 commandClass\n    class C0,D0 commandClass\n    class C1,C2,C3,C4,C5,C6,D1,D2,D3,D4,D5,D6 subCommandClass\n    class B1a,B4a,C5a,D1a,D2a,D6a optionClass\n    class C2a,C3a,D4a,D5a argumentClass</code></pre>"},{"location":"cli-reference/#command-categories","title":"Command Categories","text":"Category Purpose Key Commands Service Management Control Docker services <code>start</code>, <code>stop</code>, <code>status</code>, <code>logs</code> Configuration Manage .env settings <code>config set</code>, <code>config show</code>, <code>config validate</code> Ollama Management Local AI model operations <code>ollama pull</code>, <code>ollama sync-litellm</code> Browser Integration Quick access to UIs <code>dashboard</code>, <code>docs</code>, <code>docs-reload</code> Utility Version and help <code>version</code>, <code>--help</code>"},{"location":"cli-reference/#common-patterns","title":"Common Patterns","text":"<ul> <li>Hierarchical Structure: Commands are grouped logically (<code>config</code>, <code>ollama</code>)</li> <li>Consistent Options: Similar flags across related commands (<code>--dry-run</code>, <code>--category</code>)</li> <li>Progressive Disclosure: Basic commands work with defaults, advanced options available</li> <li>Context-Sensitive Help: <code>--help</code> available at every level</li> <li>Safe Operations: Destructive actions require confirmation or offer <code>--dry-run</code></li> </ul>"},{"location":"cli-reference/#quick-reference","title":"Quick Reference","text":"<pre><code># Essential workflow\nai-dev-local config init                    # Setup\nai-dev-local config set OPENAI_API_KEY ... # Configure\nai-dev-local start                          # Launch\nai-dev-local dashboard                      # Access\n\n# Local AI models\nai-dev-local start --ollama                 # Start with Ollama\nai-dev-local ollama list-available --category code  # Browse\nai-dev-local ollama pull codellama:7b       # Install\nai-dev-local ollama sync-litellm            # Integrate\n\n# Configuration management\nai-dev-local config validate                # Check setup\nai-dev-local config list --category api-keys # View keys\nai-dev-local config edit                    # Manual edit\n</code></pre>"},{"location":"cli-reference/#main-commands","title":"Main Commands","text":""},{"location":"cli-reference/#ai-dev-local-help","title":"<code>ai-dev-local --help</code>","text":"<p>Get help for any command or subcommand.</p> <pre><code>ai-dev-local --help\nai-dev-local config --help\nai-dev-local ollama --help\n</code></pre>"},{"location":"cli-reference/#service-management","title":"Service Management","text":""},{"location":"cli-reference/#ai-dev-local-start","title":"<code>ai-dev-local start</code>","text":"<p>Start all AI Dev Local services.</p> <pre><code># Start core services\nai-dev-local start\n\n# Start with Ollama for local models\nai-dev-local start --ollama\n\n# Build images before starting\nai-dev-local start --build\n\n# Combine options\nai-dev-local start --ollama --build\n</code></pre> <p>Options: - <code>--ollama</code>: Include Ollama service for local LLM models - <code>--build</code>: Build Docker images before starting services</p> <p>Example Output: <pre><code>\ud83d\ude80 Starting AI Dev Local services...\n\ud83d\udccb Version: v0.2.1-3-g1a2b3c4\n\ud83d\udcc5 Build Date: 2025-01-27T15:30:42Z\n\u2705 Services started successfully!\n\n\ud83d\udccb Service URLs:\n  \u2022 Dashboard: http://localhost:3002\n  \u2022 Langfuse: http://localhost:3000\n  \u2022 FlowiseAI: http://localhost:3001\n  \u2022 Open WebUI: http://localhost:8081\n  \u2022 LiteLLM Proxy: http://localhost:4000\n  \u2022 Documentation: http://localhost:8000\n  \u2022 Ollama: http://localhost:11434\n</code></pre></p>"},{"location":"cli-reference/#ai-dev-local-stop","title":"<code>ai-dev-local stop</code>","text":"<p>Stop all running AI services cleanly.</p> <pre><code>ai-dev-local stop\n</code></pre> <p>Example Output: <pre><code>\ud83d\uded1 Stopping AI Dev Local services...\n\u2705 Services stopped successfully!\n</code></pre></p>"},{"location":"cli-reference/#ai-dev-local-status","title":"<code>ai-dev-local status</code>","text":"<p>Show the current status of all services.</p> <pre><code>ai-dev-local status\n</code></pre> <p>Example Output: <pre><code>\ud83d\udcca Service Status:\n      Name                    Command               State           Ports\n-------------------------------------------------------------------------\nai-dev-local_dashboard_1    /docker-entrypoint.sh   Up          0.0.0.0:3002-&gt;80/tcp\nai-dev-local_flowise_1      docker-entrypoint.sh     Up          0.0.0.0:3001-&gt;3000/tcp\nai-dev-local_langfuse_1     docker-entrypoint.sh     Up          0.0.0.0:3000-&gt;3000/tcp\nai-dev-local_litellm_1      python -m litellm        Up          0.0.0.0:4000-&gt;4000/tcp\nai-dev-local_mkdocs_1       mkdocs serve             Up          0.0.0.0:8000-&gt;8000/tcp\nai-dev-local_ollama_1       /bin/ollama serve        Up          0.0.0.0:11434-&gt;11434/tcp\nai-dev-local_open-webui_1   bash start.sh            Up          0.0.0.0:8081-&gt;8080/tcp\nai-dev-local_postgres_1     docker-entrypoint.sh     Up          0.0.0.0:5432-&gt;5432/tcp\nai-dev-local_redis_1        docker-entrypoint.sh     Up          0.0.0.0:6379-&gt;6379/tcp\n</code></pre></p>"},{"location":"cli-reference/#ai-dev-local-logs-service","title":"<code>ai-dev-local logs [SERVICE]</code>","text":"<p>Show logs for services. Optionally specify a specific service.</p> <pre><code># Show logs for all services\nai-dev-local logs\n\n# Show logs for specific service\nai-dev-local logs litellm\nai-dev-local logs langfuse\nai-dev-local logs flowise\n</code></pre>"},{"location":"cli-reference/#browser-commands","title":"Browser Commands","text":""},{"location":"cli-reference/#ai-dev-local-docs","title":"<code>ai-dev-local docs</code>","text":"<p>Open documentation in your default browser.</p> <pre><code>ai-dev-local docs\n</code></pre>"},{"location":"cli-reference/#ai-dev-local-docs-reload","title":"<code>ai-dev-local docs-reload</code>","text":"<p>Reload/update MkDocs documentation service.</p> <pre><code>ai-dev-local docs-reload\n</code></pre> <p>Features: - Checks if MkDocs service is running - Stops the current MkDocs service - Rebuilds the MkDocs Docker image to include documentation changes - Starts the service with the updated image - Shows updated documentation URL - Optionally opens documentation in browser</p> <p>Use Cases: - After editing documentation files (markdown, mkdocs.yml, etc.) - When documentation changes aren't reflected in the browser - To rebuild documentation with updated content - After adding new documentation pages or assets</p> <p>Prerequisites: MkDocs service must be running (<code>ai-dev-local start</code>)</p> <p>Note: This command rebuilds the Docker image, which may take a few moments depending on the size of your documentation.</p>"},{"location":"cli-reference/#ai-dev-local-dashboard","title":"<code>ai-dev-local dashboard</code>","text":"<p>Open the main dashboard in your default browser.</p> <pre><code>ai-dev-local dashboard\n</code></pre>"},{"location":"cli-reference/#version-information","title":"Version Information","text":""},{"location":"cli-reference/#ai-dev-local-version","title":"<code>ai-dev-local version</code>","text":"<p>Display current version information.</p> <pre><code>ai-dev-local version\n</code></pre> <p>Example Output: <pre><code>\ud83c\udff7\ufe0f  Current Version: v0.2.1-3-g1a2b3c4 (development)\n</code></pre></p> <p>Shows version from git tags with fallback hierarchy: 1. Exact git tag match 2. Latest git tag with commit info (development) 3. Package version from <code>__init__.py</code> (no git tags found)</p>"},{"location":"cli-reference/#configuration-management","title":"Configuration Management","text":""},{"location":"cli-reference/#ai-dev-local-config","title":"<code>ai-dev-local config</code>","text":"<p>Manage configuration and .env file settings.</p>"},{"location":"cli-reference/#ai-dev-local-config-init","title":"<code>ai-dev-local config init</code>","text":"<p>Initialize .env file from .env.example template.</p> <pre><code>ai-dev-local config init\n</code></pre> <p>Example Output: <pre><code>\u2705 Created .env from .env.example\n\n\ud83d\udcdd Next steps:\n  1. Edit .env file with your API keys and settings\n  2. Use 'ai-dev-local config set' to update specific values\n  3. Use 'ai-dev-local config show' to view current settings\n</code></pre></p>"},{"location":"cli-reference/#ai-dev-local-config-set-key-value","title":"<code>ai-dev-local config set &lt;KEY&gt; &lt;VALUE&gt;</code>","text":"<p>Set a configuration value in the .env file.</p> <pre><code># Set API keys\nai-dev-local config set OPENAI_API_KEY *********************\nai-dev-local config set ANTHROPIC_API_KEY ********************\n\n# Set ports\nai-dev-local config set LANGFUSE_PORT 3030\nai-dev-local config set DASHBOARD_PORT 3003\n\n# Set service options\nai-dev-local config set DEBUG true\nai-dev-local config set OLLAMA_GPU true\n</code></pre> <p>Example Output: <pre><code>\u2705 Set LANGFUSE_PORT=3030\n</code></pre></p> <p>Features: - Automatically creates .env file if it doesn't exist - Preserves inline comments when updating values - Intelligently places new keys in appropriate sections - Supports all configuration categories</p>"},{"location":"cli-reference/#ai-dev-local-config-show-key","title":"<code>ai-dev-local config show [KEY]</code>","text":"<p>Show configuration values from .env file.</p> <pre><code># Show all configuration\nai-dev-local config show\n\n# Show specific key (sensitive values are masked)\nai-dev-local config show OPENAI_API_KEY\nai-dev-local config show LANGFUSE_PORT\n</code></pre> <p>Features: - Automatically masks sensitive values (API keys, secrets, passwords, tokens) - Shows all non-comment configuration lines - Displays specific key values when requested</p>"},{"location":"cli-reference/#ai-dev-local-config-validate","title":"<code>ai-dev-local config validate</code>","text":"<p>Validate .env file configuration.</p> <pre><code>ai-dev-local config validate\n</code></pre> <p>Validation includes: - Required Settings: OPENAI_API_KEY, WEBUI_SECRET_KEY, LITELLM_MASTER_KEY - Optional Settings: ANTHROPIC_API_KEY, GEMINI_API_KEY, COHERE_API_KEY, LANGFUSE_ - Status Report: Shows which settings are configured or missing - Recommendations:* Provides next steps for missing required settings</p>"},{"location":"cli-reference/#ai-dev-local-config-list-category-category","title":"<code>ai-dev-local config list [--category CATEGORY]</code>","text":"<p>List configuration variables by category.</p> <pre><code># Show all categories\nai-dev-local config list\n\n# Show specific category\nai-dev-local config list --category api-keys\nai-dev-local config list --category ports\nai-dev-local config list --category services\nai-dev-local config list --category mcp\n</code></pre> <p>Categories: - api-keys: LLM Provider API Keys (OpenAI, Anthropic, Gemini, Cohere) - ports: Host and Port Configuration (all service ports) - services: Service Configuration (secrets, settings, options) - mcp: Model Context Protocol settings (Git, GitHub, GitLab, SonarQube)</p>"},{"location":"cli-reference/#ai-dev-local-config-edit","title":"<code>ai-dev-local config edit</code>","text":"<p>Open .env file in your default editor.</p> <pre><code>ai-dev-local config edit\n</code></pre> <p>Editor Priority: 1. <code>$EDITOR</code> environment variable 2. <code>code</code> (VS Code) 3. <code>nano</code> 4. <code>vim</code> 5. <code>vi</code> 6. Manual edit instructions (fallback)</p>"},{"location":"cli-reference/#ollama-management","title":"Ollama Management","text":""},{"location":"cli-reference/#ai-dev-local-ollama","title":"<code>ai-dev-local ollama</code>","text":"<p>Manage Ollama local LLM server and models.</p>"},{"location":"cli-reference/#ai-dev-local-ollama-init-models-models","title":"<code>ai-dev-local ollama init [--models MODELS]</code>","text":"<p>Initialize Ollama with common models.</p> <pre><code># Use default models from OLLAMA_AUTO_PULL_MODELS\nai-dev-local ollama init\n\n# Specify custom models\nai-dev-local ollama init --models \"llama2:7b,codellama:7b,mistral:7b\"\n</code></pre> <p>Default Models: llama2:7b, codellama:7b, mistral:7b, phi:2.7b</p> <p>Prerequisites: Ollama service must be running (<code>ai-dev-local start --ollama</code>)</p>"},{"location":"cli-reference/#ai-dev-local-ollama-models","title":"<code>ai-dev-local ollama models</code>","text":"<p>List available Ollama models.</p> <pre><code>ai-dev-local ollama models\n</code></pre>"},{"location":"cli-reference/#ai-dev-local-ollama-pull-model","title":"<code>ai-dev-local ollama pull &lt;MODEL&gt;</code>","text":"<p>Pull a specific Ollama model.</p> <pre><code>ai-dev-local ollama pull llama2:13b\nai-dev-local ollama pull codellama:34b\nai-dev-local ollama pull mistral:instruct\n</code></pre>"},{"location":"cli-reference/#ai-dev-local-ollama-remove-model","title":"<code>ai-dev-local ollama remove &lt;MODEL&gt;</code>","text":"<p>Remove a specific Ollama model.</p> <pre><code>ai-dev-local ollama remove llama2:7b\nai-dev-local ollama remove codellama:7b\n</code></pre>"},{"location":"cli-reference/#ai-dev-local-ollama-list-available-options","title":"<code>ai-dev-local ollama list-available [OPTIONS]</code>","text":"<p>List all available models from Ollama library.</p> <pre><code># Show popular models (default)\nai-dev-local ollama list-available\n\n# Search for specific models\nai-dev-local ollama list-available --search llama\nai-dev-local ollama list-available --search code\n\n# Filter by category\nai-dev-local ollama list-available --category code\nai-dev-local ollama list-available --category embedding\nai-dev-local ollama list-available --category vision\nai-dev-local ollama list-available --category all\n\n# Different output formats\nai-dev-local ollama list-available --format table  # default\nai-dev-local ollama list-available --format list\nai-dev-local ollama list-available --format json\n</code></pre> <p>Options: - <code>--search, -s TEXT</code>: Search for models containing this term - <code>--category, -c [all|popular|code|embedding|vision]</code>: Filter by model category (default: popular) - <code>--format, -f [table|list|json]</code>: Output format (default: table)</p> <p>Categories: - popular: llama2, llama3, codellama, mistral, phi, gemma, qwen - code: codellama, codegemma, starcoder, wizard-coder, deepseek-coder - embedding: nomic-embed, mxbai-embed, all-minilm - vision: llava, moondream, bakllava</p> <p>Features: - Fetches real-time data from Ollama library registry - Shows model names, tags, pull counts, and descriptions - Fallback list of popular models if registry is unavailable - Sorted by popularity (download count)</p>"},{"location":"cli-reference/#ai-dev-local-ollama-sync-litellm-options","title":"<code>ai-dev-local ollama sync-litellm [OPTIONS]</code>","text":"<p>Sync LiteLLM configuration with currently available Ollama models.</p> <pre><code># Sync Ollama models to LiteLLM config\nai-dev-local ollama sync-litellm\n\n# Preview changes without applying them\nai-dev-local ollama sync-litellm --dry-run\n\n# Sync without creating backup\nai-dev-local ollama sync-litellm --no-backup\n</code></pre> <p>Options: - <code>--dry-run</code>: Show what would be changed without making modifications - <code>--backup / --no-backup</code>: Create backup of existing config (default: backup enabled)</p> <p>Prerequisites: - Ollama service must be running (<code>ai-dev-local start --ollama</code>) - Ollama models must be installed (<code>ai-dev-local ollama init</code> or <code>ai-dev-local ollama pull &lt;model&gt;</code>)</p> <p>Features: - Automatically detects all installed Ollama models - Updates LiteLLM configuration with current models - Removes outdated Ollama model entries - Creates timestamped backup before changes - Updates router group aliases for model routing - Preserves all non-Ollama model configurations - Provides detailed change summary before applying</p>"},{"location":"cli-reference/#service-urls","title":"Service URLs","text":"<p>When services are running, they are accessible at these default URLs:</p> Service URL Purpose Dashboard http://localhost:3002 Main control panel Langfuse http://localhost:3000 LLM observability FlowiseAI http://localhost:3001 Visual AI workflows Open WebUI http://localhost:8081 Chat interface LiteLLM Proxy http://localhost:4000 Unified LLM API Documentation http://localhost:8000 This documentation Ollama http://localhost:11434 Local LLM server"},{"location":"cli-reference/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The CLI manages a <code>.env</code> file with these main sections:</p>"},{"location":"cli-reference/#llm-provider-api-keys","title":"LLM Provider API Keys","text":"<pre><code>OPENAI_API_KEY=your-openai-key\nANTHROPIC_API_KEY=your-anthropic-key\nGEMINI_API_KEY=your-gemini-key\nCOHERE_API_KEY=your-cohere-key\n</code></pre>"},{"location":"cli-reference/#host-and-port-configuration","title":"Host and Port Configuration","text":"<pre><code>HOST=localhost\nPOSTGRES_PORT=5432\nREDIS_PORT=6379\nLANGFUSE_PORT=3000\nFLOWISE_PORT=3001\nOPENWEBUI_PORT=8081\nLITELLM_PORT=4000\nOLLAMA_PORT=11434\nDASHBOARD_PORT=3002\nMKDOCS_PORT=8000\n</code></pre>"},{"location":"cli-reference/#service-configuration","title":"Service Configuration","text":"<pre><code>WEBUI_SECRET_KEY=your-secret-key\nWEBUI_JWT_SECRET_KEY=your-jwt-secret\nLITELLM_MASTER_KEY=your-litellm-key\nDEBUG=false\nOLLAMA_GPU=false\nOLLAMA_AUTO_PULL_MODELS=llama2:7b,codellama:7b\n</code></pre>"},{"location":"cli-reference/#mcp-model-context-protocol","title":"MCP (Model Context Protocol)","text":"<pre><code>GIT_AUTHOR_NAME=Your Name\nGIT_AUTHOR_EMAIL=your@email.com\nGITHUB_PERSONAL_ACCESS_TOKEN=your-github-token\nGITLAB_TOKEN=your-gitlab-token\nSONARQUBE_URL=http://localhost:9000\nSONARQUBE_TOKEN=your-sonarqube-token\n</code></pre>"},{"location":"cli-reference/#examples","title":"Examples","text":""},{"location":"cli-reference/#quick-start-workflow","title":"Quick Start Workflow","text":"<pre><code># 1. Initialize configuration\nai-dev-local config init\n\n# 2. Set required API key\nai-dev-local config set OPENAI_API_KEY sk-proj-your-key\n\n# 3. Validate configuration\nai-dev-local config validate\n\n# 4. Start services\nai-dev-local start\n\n# 5. Check status\nai-dev-local status\n\n# 6. Open dashboard\nai-dev-local dashboard\n</code></pre>"},{"location":"cli-reference/#local-development-with-ollama","title":"Local Development with Ollama","text":"<pre><code># Start with Ollama\nai-dev-local start --ollama\n\n# Browse available models\nai-dev-local ollama list-available\nai-dev-local ollama list-available --category code\n\n# Initialize with models\nai-dev-local ollama init\n\n# List installed models\nai-dev-local ollama models\n\n# Add more models\nai-dev-local ollama pull llama2:13b\n\n# Sync Ollama models to LiteLLM for unified API access\nai-dev-local ollama sync-litellm\n\n# Restart LiteLLM to apply changes\ndocker-compose restart litellm\n</code></pre>"},{"location":"cli-reference/#configuration-management_1","title":"Configuration Management","text":"<pre><code># View current config\nai-dev-local config show\n\n# View specific category\nai-dev-local config list --category api-keys\n\n# Update settings\nai-dev-local config set LANGFUSE_PORT 3030\nai-dev-local config set DEBUG true\n\n# Edit in IDE\nai-dev-local config edit\n</code></pre>"},{"location":"cli-reference/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Check service status\nai-dev-local status\n\n# View logs for all services\nai-dev-local logs\n\n# View logs for specific service\nai-dev-local logs litellm\n\n# Validate configuration\nai-dev-local config validate\n\n# Check version\nai-dev-local version\n</code></pre>"},{"location":"cli-reference/#exit-codes","title":"Exit Codes","text":"<ul> <li>0: Success</li> <li>1: General error (failed operations, missing files, invalid configurations)</li> </ul> <p>All commands provide descriptive error messages and appropriate exit codes for scripting and automation.</p>"},{"location":"development/","title":"Development Guide","text":"<p>This guide provides information for contributing to and developing AI Dev Local.</p>"},{"location":"development/#development-environment-setup","title":"Development Environment Setup","text":""},{"location":"development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ with uv package manager</li> <li>Docker &amp; Docker Compose for containerized services</li> <li>Git for version control</li> <li>pipx for isolated Python package installation</li> <li>pre-commit for code quality hooks</li> </ul>"},{"location":"development/#initial-setup","title":"Initial Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/brunseba/ai-dev-local.git\ncd ai-dev-local\n</code></pre></p> </li> <li> <p>Install in development mode: <pre><code># Using uv (recommended)\nuv pip install -e .\n\n# Or using pip\npip install -e .\n</code></pre></p> </li> <li> <p>Install pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p> </li> <li> <p>Initialize development configuration: <pre><code>ai-dev-local config init\n</code></pre></p> </li> </ol>"},{"location":"development/#project-structure","title":"Project Structure","text":"<p>The project follows Python best practices with the root folder for code in <code>src/</code>:</p> <pre><code>ai-dev-local/\n\u251c\u2500\u2500 src/                    # Main source code\n\u251c\u2500\u2500 docs/                   # Documentation (MkDocs)\n\u251c\u2500\u2500 tests/                  # Unit tests\n\u251c\u2500\u2500 configs/                # Configuration templates\n\u251c\u2500\u2500 docker/                 # Docker configurations\n\u251c\u2500\u2500 scripts/                # Utility scripts\n\u251c\u2500\u2500 mkdocs.yml             # Documentation configuration\n\u251c\u2500\u2500 pyproject.toml         # Python project configuration\n\u2514\u2500\u2500 README.md              # Project overview\n</code></pre>"},{"location":"development/#development-workflow","title":"Development Workflow","text":""},{"location":"development/#git-workflow","title":"Git Workflow","text":"<p>The project uses conventional commits and follows these practices:</p> <ol> <li> <p>Use conventional commit format: <pre><code>git commit -m \"feat: add new feature\"\ngit commit -m \"fix: resolve bug in service\"\ngit commit -m \"docs: update documentation\"\n</code></pre></p> </li> <li> <p>Pre-commit hooks automatically run:</p> </li> <li>Code formatting and linting</li> <li>Type checking</li> <li> <p>Documentation validation</p> </li> <li> <p>Create tags for releases: <pre><code>git tag -a v0.x.x -m \"Release version 0.x.x\"\n</code></pre></p> </li> </ol>"},{"location":"development/#running-tests","title":"Running Tests","text":"<p>Unit tests are created by default using pytest:</p> <pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=src\n\n# Run specific test file\npytest tests/test_specific.py\n</code></pre>"},{"location":"development/#documentation","title":"Documentation","text":"<p>Documentation is managed with MkDocs and Material theme:</p> <pre><code># Serve documentation locally\nmkdocs serve\n\n# Build documentation\nmkdocs build\n\n# Deploy to GitHub Pages\nmkdocs gh-deploy\n</code></pre>"},{"location":"development/#code-quality","title":"Code Quality","text":"<p>The project uses several tools for code quality:</p> <ul> <li>Linting: flake8, pylint</li> <li>Formatting: black, isort</li> <li>Type checking: mypy</li> <li>Security: bandit</li> </ul> <p>Pre-commit hooks ensure all checks pass before commits.</p>"},{"location":"development/#packaging-and-distribution","title":"Packaging and Distribution","text":""},{"location":"development/#package-management","title":"Package Management","text":"<ul> <li>Package engine: uv</li> <li>CLI management: Click</li> <li>Deployment: pipx</li> </ul>"},{"location":"development/#building-and-publishing","title":"Building and Publishing","text":"<pre><code># Build package\npython -m build\n\n# Publish to PyPI (for maintainers)\npython -m twine upload dist/*\n</code></pre>"},{"location":"development/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"development/#issue-management","title":"Issue Management","text":"<ul> <li>GitHub labels map to conventional commit standards</li> <li>Issues are assigned to @me by default</li> <li>GitHub Pages is enabled for documentation</li> </ul>"},{"location":"development/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create a feature branch</li> <li>Make changes following coding standards</li> <li>Add/update tests as needed</li> <li>Update documentation</li> <li>Ensure all checks pass</li> <li>Submit pull request</li> </ol>"},{"location":"development/#github-actions","title":"GitHub Actions","text":"<p>The project includes workflows for:</p> <ul> <li>MkDocs publishing to GitHub Pages</li> <li>Continuous integration testing</li> <li>Automated releases and packaging</li> </ul>"},{"location":"development/#security-considerations","title":"Security Considerations","text":"<ul> <li>Never commit sensitive data like API keys</li> <li>Use environment variables for configuration</li> <li>Follow security best practices for Docker containers</li> <li>Keep dependencies up to date</li> </ul>"},{"location":"development/#support-and-community","title":"Support and Community","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>GitHub Discussions: Community support</li> <li>Documentation: Latest docs</li> </ul> <p>For questions or contributions, please refer to the project's GitHub repository.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you install and set up AI Dev Local on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing AI Dev Local, ensure you have the following prerequisites:</p>"},{"location":"getting-started/installation/#required","title":"Required","text":"<ul> <li>Python 3.10+ - AI Dev Local requires Python 3.10 or higher</li> <li>Docker &amp; Docker Compose - For running the containerized services</li> <li>pipx - For isolated Python package installation</li> </ul>"},{"location":"getting-started/installation/#recommended","title":"Recommended","text":"<ul> <li>Git - For version control integration</li> <li>curl - For API testing and health checks</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pipx-recommended","title":"Method 1: pipx (Recommended)","text":"<p>pipx installs Python packages in isolated environments, preventing dependency conflicts:</p> <pre><code># Install pipx if you haven't already\npython3 -m pip install --user pipx\npython3 -m pipx ensurepath\n\n# Install AI Dev Local\npipx install ai-dev-local\n\n# Verify installation\nai-dev-local --version\n</code></pre>"},{"location":"getting-started/installation/#method-2-pip-global-installation","title":"Method 2: pip (Global Installation)","text":"<p>Not Recommended</p> <p>Global pip installation can cause dependency conflicts. Use pipx instead.</p> <pre><code>pip install ai-dev-local\n</code></pre>"},{"location":"getting-started/installation/#method-3-development-installation","title":"Method 3: Development Installation","text":"<p>For development or contributing to the project:</p> <pre><code># Clone the repository\ngit clone https://github.com/brunseba/ai-dev-local.git\ncd ai-dev-local\n\n# Install in development mode\npip install -e .\n\n# Or using uv (faster)\nuv pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#system-setup","title":"System Setup","text":""},{"location":"getting-started/installation/#docker-installation","title":"Docker Installation","text":"<p>AI Dev Local requires Docker and Docker Compose:</p> macOSUbuntu/DebianWindows <pre><code># Install Docker Desktop\nbrew install --cask docker\n\n# Start Docker Desktop\nopen /Applications/Docker.app\n</code></pre> <pre><code># Install Docker\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\n\n# Install Docker Compose\nsudo apt-get update\nsudo apt-get install docker-compose-plugin\n\n# Add user to docker group\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> <ol> <li>Download and install Docker Desktop for Windows</li> <li>Ensure WSL2 is enabled</li> <li>Restart your system after installation</li> </ol>"},{"location":"getting-started/installation/#verify-docker-installation","title":"Verify Docker Installation","text":"<pre><code># Check Docker version\ndocker --version\n\n# Check Docker Compose version\ndocker compose version\n\n# Test Docker installation\ndocker run hello-world\n</code></pre>"},{"location":"getting-started/installation/#initial-setup","title":"Initial Setup","text":""},{"location":"getting-started/installation/#1-initialize-configuration","title":"1. Initialize Configuration","text":"<pre><code># Create initial configuration\nai-dev-local config init\n</code></pre> <p>This creates a <code>.env</code> file from the template with default values.</p>"},{"location":"getting-started/installation/#2-configure-api-keys","title":"2. Configure API Keys","text":"<p>Set your OpenAI API key (required):</p> <pre><code>ai-dev-local config set OPENAI_API_KEY your-actual-api-key-here\n</code></pre> <p>Optional: Configure additional LLM providers:</p> <pre><code>ai-dev-local config set ANTHROPIC_API_KEY your-anthropic-key\nai-dev-local config set GEMINI_API_KEY your-gemini-key\nai-dev-local config set COHERE_API_KEY your-cohere-key\n</code></pre>"},{"location":"getting-started/installation/#3-validate-configuration","title":"3. Validate Configuration","text":"<pre><code>ai-dev-local config validate\n</code></pre>"},{"location":"getting-started/installation/#4-start-services","title":"4. Start Services","text":"<pre><code># Start all core services\nai-dev-local start\n\n# Or start with Ollama for local models\nai-dev-local start --ollama\n</code></pre>"},{"location":"getting-started/installation/#5-verify-installation","title":"5. Verify Installation","text":"<pre><code># Check service status\nai-dev-local status\n\n# Access the dashboard\nopen http://localhost:3002\n</code></pre>"},{"location":"getting-started/installation/#post-installation","title":"Post-Installation","text":""},{"location":"getting-started/installation/#access-services","title":"Access Services","text":"<p>After successful installation, you can access:</p> Service URL Purpose Dashboard http://localhost:3002 Main control panel Open WebUI http://localhost:8081 Chat interface FlowiseAI http://localhost:3001 Visual workflows Langfuse http://localhost:3000 LLM observability LiteLLM http://localhost:4000 API gateway Documentation http://localhost:8000 This documentation"},{"location":"getting-started/installation/#default-credentials","title":"Default Credentials","text":"<p>Some services require login credentials:</p> Service Username Password FlowiseAI admin admin123 LiteLLM UI admin admin123 <p>Security</p> <p>Change default passwords in production environments using the Configuration Guide.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-installation-issues","title":"Common Installation Issues","text":""},{"location":"getting-started/installation/#permission-denied-errors","title":"Permission Denied Errors","text":"<pre><code># Fix Docker permissions (Linux/macOS)\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# Fix file permissions\nsudo chown -R $USER:$USER ~/.local/share/pipx\n</code></pre>"},{"location":"getting-started/installation/#port-conflicts","title":"Port Conflicts","text":"<pre><code># Check what's using a port\nlsof -i :3000\n\n# Change conflicting ports\nai-dev-local config set LANGFUSE_PORT 3030\n</code></pre>"},{"location":"getting-started/installation/#docker-issues","title":"Docker Issues","text":"<pre><code># Restart Docker daemon\nsudo systemctl restart docker  # Linux\n# Or restart Docker Desktop on macOS/Windows\n\n# Clean Docker system\ndocker system prune -a\n</code></pre>"},{"location":"getting-started/installation/#python-version-issues","title":"Python Version Issues","text":"<pre><code># Check Python version\npython3 --version\n\n# Install Python 3.10+ using pyenv\ncurl https://pyenv.run | bash\npyenv install 3.11.0\npyenv global 3.11.0\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the logs: <code>ai-dev-local logs</code></li> <li>Validate configuration: <code>ai-dev-local config validate</code></li> <li>Review documentation: Configuration Guide</li> <li>Search existing issues: GitHub Issues</li> <li>Report new issues: Create Issue</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After successful installation:</p> <ol> <li>Quick Start Tutorial - Learn the basics</li> <li>Configuration Guide - Customize your setup</li> <li>IDE MCP Setup - Integrate with your editor</li> </ol>"},{"location":"getting-started/installation/#uninstallation","title":"Uninstallation","text":"<p>To completely remove AI Dev Local:</p> <pre><code># Stop and remove all services\nai-dev-local stop\nai-dev-local down --volumes\n\n# Remove the CLI tool\npipx uninstall ai-dev-local\n\n# Clean up Docker resources\ndocker system prune -a\n\n# Remove configuration (optional)\nrm -rf .env .ai-dev-local/\n</code></pre>"},{"location":"getting-started/quick-start/","title":"Quick Start Tutorial","text":"<p>Welcome to the Quick Start Tutorial for AI Dev Local, your local development environment for powerful AI integrations.</p>"},{"location":"getting-started/quick-start/#objective","title":"Objective","text":"<p>In this tutorial, you'll:</p> <ul> <li>Install AI Dev Local</li> <li>Configure your environment</li> <li>Launch services to get started fast</li> </ul>"},{"location":"getting-started/quick-start/#step-1-installation","title":"Step 1: Installation","text":""},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.10+</li> <li>Docker</li> <li>pipx (for package installation)</li> </ol>"},{"location":"getting-started/quick-start/#install-ai-dev-local","title":"Install AI Dev Local","text":"<pre><code># Install with pipx\npipx install ai-dev-local\n\n# Verify installation\nai-dev-local --version\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-configuration","title":"Step 2: Configuration","text":""},{"location":"getting-started/quick-start/#initialize-configuration","title":"Initialize Configuration","text":"<p>Initialize the default settings:</p> <pre><code>ai-dev-local config init\n</code></pre>"},{"location":"getting-started/quick-start/#set-api-keys","title":"Set API Keys","text":"<p>For OpenAI models:</p> <pre><code>ai-dev-local config set OPENAI_API_KEY your-openai-api-key\n</code></pre> <p>(Optional) For other LLM providers:</p> <pre><code>ai-dev-local config set ANTHROPIC_API_KEY your-anthropic-api-key\n</code></pre> <p>Validate the setup:</p> <pre><code>ai-dev-local config validate\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-launch-services","title":"Step 3: Launch Services","text":""},{"location":"getting-started/quick-start/#start-the-environment","title":"Start the Environment","text":"<p>Launch the services:</p> <pre><code>ai-dev-local start\n</code></pre>"},{"location":"getting-started/quick-start/#verify-running-services","title":"Verify Running Services","text":"<p>Check that services are running smoothly:</p> <pre><code>ai-dev-local status\n</code></pre> <p>Navigate to the Dashboard:</p> <pre><code>Open in browser: http://localhost:3002/\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-explore-features","title":"Step 4: Explore Features","text":""},{"location":"getting-started/quick-start/#dashboard","title":"Dashboard","text":"<p>Use the Dashboard to monitor and control your services.</p> <ul> <li>URL: Dashboard</li> </ul>"},{"location":"getting-started/quick-start/#open-webui","title":"Open WebUI","text":"<p>Chat with AI models via Open WebUI.</p> <ul> <li>Open WebUI: Chat Interface</li> </ul>"},{"location":"getting-started/quick-start/#flowiseai","title":"FlowiseAI","text":"<p>Build AI workflows visually with FlowiseAI.</p> <ul> <li>FlowiseAI: Visual Builder</li> </ul>"},{"location":"getting-started/quick-start/#langfuse","title":"Langfuse","text":"<p>Track and debug LLM interactions using Langfuse.</p> <ul> <li>Langfuse: Observability</li> </ul>"},{"location":"getting-started/quick-start/#litellm","title":"LiteLLM","text":"<p>Access the unified API proxy for LLMs.</p> <ul> <li>LiteLLM: API Proxy</li> </ul>"},{"location":"getting-started/quick-start/#step-5-local-ai-models-optional","title":"Step 5: Local AI Models (Optional)","text":""},{"location":"getting-started/quick-start/#using-ollama-for-local-models","title":"Using Ollama for Local Models","text":"<p>If you want to run AI models locally instead of using cloud APIs:</p> <pre><code># Start services with Ollama\nai-dev-local start --ollama\n\n# Browse available models\nai-dev-local ollama list-available --category code\n\n# Install popular models\nai-dev-local ollama pull codellama:7b\nai-dev-local ollama pull llama2:7b\n\n# Sync models to LiteLLM for unified API access\nai-dev-local ollama sync-litellm\n\n# Restart LiteLLM to apply changes\ndocker-compose restart litellm\n</code></pre> <p>Benefits of Local Models: - Privacy: Your data stays local - Cost: No API usage charges - Speed: No network latency for inference - Offline: Works without internet connection</p>"},{"location":"getting-started/quick-start/#useful-commands","title":"Useful Commands","text":"<ul> <li>Stop Services: <code>ai-dev-local stop</code></li> <li>View Logs: <code>ai-dev-local logs</code></li> <li>Update Configuration: <code>ai-dev-local config set &lt;KEY&gt; &lt;VALUE&gt;</code></li> <li>Check Ollama Models: <code>ai-dev-local ollama models</code></li> <li>Open Documentation: <code>ai-dev-local docs</code></li> </ul>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Once you're familiar with the basics, check out:</p> <ul> <li>Configuration Guide for advanced settings.</li> <li>IDE MCP Setup to integrate with your development environment.</li> </ul>"},{"location":"mcp/deployment/","title":"MCP Deployment and Management Guide","text":"<p>This guide provides comprehensive information about deploying, managing, and integrating Model Context Protocol (MCP) servers in the AI Dev Local environment.</p>"},{"location":"mcp/deployment/#overview","title":"Overview","text":"<p>The MCP ecosystem in AI Dev Local consists of multiple specialized servers that provide different capabilities to AI agents and applications. Each server runs in its own containerized environment with proper isolation and networking.</p>"},{"location":"mcp/deployment/#architecture","title":"Architecture","text":""},{"location":"mcp/deployment/#mcp-network-topology","title":"MCP Network Topology","text":"<pre><code>graph TB\n    subgraph \"Client Applications\"\n        Claude[\ud83e\udd16 Claude Desktop]\n        VSCode[\ud83d\udcdd VS Code/Codium]\n        FlowiseAI[\ud83c\udf0a FlowiseAI]\n        OpenWebUI[\ud83c\udf10 Open WebUI]\n        LiteLLM[\u26a1 LiteLLM Proxy]\n    end\n\n    subgraph \"MCP Gateway Layer\"\n        Gateway[\ud83d\udeaa MCP Gateway&lt;br/&gt;Port 9000]\n    end\n\n    subgraph \"Core Development Servers\"\n        Git[\ud83d\udcc2 Git Server&lt;br/&gt;Port 9001]\n        FS[\ud83d\udcc1 Filesystem&lt;br/&gt;Port 9002]\n        Fetch[\ud83c\udf10 Fetch Server&lt;br/&gt;Port 9003]\n    end\n\n    subgraph \"Data &amp; Storage Servers\"\n        Memory[\ud83e\udde0 Memory Server&lt;br/&gt;Port 9004]\n        Time[\u23f0 Time Server&lt;br/&gt;Port 9005]\n        PostgreSQL[\ud83d\udc18 PostgreSQL&lt;br/&gt;Port 9006]\n    end\n\n    subgraph \"Platform Integration Servers\"\n        Everything[\ud83d\udee0\ufe0f Everything&lt;br/&gt;Port 9007]\n        GitHub[\ud83d\udc19 GitHub&lt;br/&gt;Port 9008]\n        GitLab[\ud83e\udd8a GitLab&lt;br/&gt;Port 9009]\n        SonarQube[\ud83d\udd0d SonarQube&lt;br/&gt;Port 9010]\n    end\n\n    subgraph \"External Services\"\n        GitHubAPI[GitHub.com API]\n        GitLabAPI[GitLab.com API]\n        SonarQubeAPI[SonarQube Instance]\n        Database[(PostgreSQL DB)]\n    end\n\n    Claude -.-&gt; Gateway\n    VSCode -.-&gt; Gateway\n    FlowiseAI --&gt; Gateway\n    OpenWebUI --&gt; Gateway\n    LiteLLM --&gt; Gateway\n\n    Gateway --&gt; Git\n    Gateway --&gt; FS\n    Gateway --&gt; Fetch\n    Gateway --&gt; Memory\n    Gateway --&gt; Time\n    Gateway --&gt; PostgreSQL\n    Gateway --&gt; Everything\n    Gateway --&gt; GitHub\n    Gateway --&gt; GitLab\n    Gateway --&gt; SonarQube\n\n    GitHub &lt;--&gt; GitHubAPI\n    GitLab &lt;--&gt; GitLabAPI\n    SonarQube &lt;--&gt; SonarQubeAPI\n    PostgreSQL &lt;--&gt; Database</code></pre>"},{"location":"mcp/deployment/#transport-protocols","title":"Transport Protocols","text":"<p>MCP servers support multiple transport protocols:</p> <ol> <li>HTTP/REST: Standard web API for most servers</li> <li>Server-Sent Events (SSE): Real-time streaming for compatible servers</li> <li>stdio: Direct process communication (bridged to HTTP via wrappers)</li> </ol>"},{"location":"mcp/deployment/#deployment-commands","title":"Deployment Commands","text":""},{"location":"mcp/deployment/#start-all-mcp-services","title":"Start All MCP Services","text":"<pre><code># Start the complete MCP stack\nai-dev-local mcp start\n\n# Start specific services\ndocker-compose -f docker-compose.mcp.yml up -d mcp-github mcp-gitlab\n</code></pre>"},{"location":"mcp/deployment/#check-service-status","title":"Check Service Status","text":"<pre><code># View all MCP service status\nai-dev-local mcp status\n\n# Check specific service logs\ndocker-compose -f docker-compose.mcp.yml logs -f mcp-github\n</code></pre>"},{"location":"mcp/deployment/#stop-services","title":"Stop Services","text":"<pre><code># Stop all MCP services\nai-dev-local mcp stop\n\n# Stop specific service\ndocker-compose -f docker-compose.mcp.yml stop mcp-github\n</code></pre>"},{"location":"mcp/deployment/#individual-server-configurations","title":"Individual Server Configurations","text":""},{"location":"mcp/deployment/#github-mcp-server","title":"GitHub MCP Server","text":"<p>Image: Custom HTTP wrapper + <code>ghcr.io/github/github-mcp-server:latest</code> Port: 9008 Transport: HTTP (via stdio bridge)</p> <pre><code>environment:\n  - GITHUB_PERSONAL_ACCESS_TOKEN=${GITHUB_PERSONAL_ACCESS_TOKEN}\n  - GITHUB_TOOLSETS=repos,issues,pull_requests,actions,code_security,context\n  - GITHUB_READ_ONLY=false\n</code></pre> <p>Key Features: - 70+ GitHub tools for comprehensive repository management - HTTP wrapper enables seamless integration with other services - Support for GitHub Actions, code security, and Copilot integration</p>"},{"location":"mcp/deployment/#gitlab-mcp-server","title":"GitLab MCP Server","text":"<p>Image: Built from <code>https://github.com/zereight/gitlab-mcp.git</code> Port: 9009 Transport: HTTP</p> <pre><code>environment:\n  - GITLAB_PERSONAL_ACCESS_TOKEN=${GITLAB_TOKEN}\n  - GITLAB_API_URL=https://gitlab.com/api/v4\n  - GITLAB_READ_ONLY_MODE=false\n</code></pre>"},{"location":"mcp/deployment/#postgresql-mcp-server","title":"PostgreSQL MCP Server","text":"<p>Image: <code>crystaldba/postgres-mcp:latest</code> Port: 9006 Transport: SSE</p> <pre><code>command: [\"--access-mode=restricted\", \"--transport=sse\"]\nenvironment:\n  - DATABASE_URI=postgresql://postgres:postgres@postgres:5432/postgres\n</code></pre>"},{"location":"mcp/deployment/#git-server","title":"Git Server","text":"<p>Image: <code>mcp/git:latest</code> Port: 9001 Transport: HTTP</p> <p>Volume Mounts: - Workspace (read-only): <code>.:/workspace:ro</code> - Git config: <code>~/.gitconfig:/root/.gitconfig:ro</code> - SSH keys: <code>~/.ssh:/root/.ssh:ro</code></p>"},{"location":"mcp/deployment/#filesystem-server","title":"Filesystem Server","text":"<p>Image: <code>mcp/filesystem:latest</code> Port: 9002 Transport: HTTP</p> <p>Security Configuration: <pre><code>environment:\n  - ALLOWED_DIRECTORIES=/workspace,/tmp/mcp-fs\n  - READ_ONLY_DIRECTORIES=/workspace/.git\n</code></pre></p>"},{"location":"mcp/deployment/#memory-server","title":"Memory Server","text":"<p>Image: <code>mcp/memory:latest</code> Port: 9004 Transport: HTTP</p> <p>Persistent Storage: <pre><code>volumes:\n  - mcp_memory_data:/data\nenvironment:\n  - MEMORY_STORE_PATH=/data/memory.db\n  - MAX_MEMORY_SIZE=1073741824  # 1GB\n</code></pre></p>"},{"location":"mcp/deployment/#network-configuration","title":"Network Configuration","text":""},{"location":"mcp/deployment/#docker-networks","title":"Docker Networks","text":"<ul> <li>Primary Network: <code>ai-dev-mcp</code> (bridge)</li> <li>External Network: <code>ai-dev-local</code> (connects to main services)</li> <li>Subnet: <code>192.168.100.0/24</code></li> </ul>"},{"location":"mcp/deployment/#port-mapping","title":"Port Mapping","text":"Service Internal Port External Port Protocol Gateway 8080 9000 HTTP Git 8000 9001 HTTP Filesystem 8000 9002 HTTP Fetch 8000 9003 HTTP Memory 8000 9004 HTTP Time 8000 9005 HTTP PostgreSQL 8000 9006 SSE Everything 8000 9007 HTTP GitHub 8000 9008 HTTP GitLab 8000 9009 HTTP SonarQube 8000 9010 HTTP"},{"location":"mcp/deployment/#health-monitoring","title":"Health Monitoring","text":""},{"location":"mcp/deployment/#health-check-endpoints","title":"Health Check Endpoints","text":"<p>All HTTP-based servers provide health endpoints:</p> <pre><code># Check individual server health\ncurl http://localhost:9008/health  # GitHub\ncurl http://localhost:9009/health  # GitLab\ncurl http://localhost:9010/health  # SonarQube\n</code></pre>"},{"location":"mcp/deployment/#monitoring-commands","title":"Monitoring Commands","text":"<pre><code># View real-time logs\ndocker-compose -f docker-compose.mcp.yml logs -f\n\n# Check resource usage\ndocker stats $(docker-compose -f docker-compose.mcp.yml ps -q)\n\n# Restart unhealthy services\ndocker-compose -f docker-compose.mcp.yml restart mcp-github\n</code></pre>"},{"location":"mcp/deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/deployment/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Service Not Starting <pre><code># Check logs for errors\ndocker-compose -f docker-compose.mcp.yml logs mcp-github\n\n# Rebuild if needed\ndocker-compose -f docker-compose.mcp.yml build --no-cache mcp-github\n</code></pre></p> </li> <li> <p>Authentication Failures <pre><code># Verify environment variables\ndocker-compose -f docker-compose.mcp.yml exec mcp-github env | grep GITHUB\n\n# Test token validity\ncurl -H \"Authorization: token $GITHUB_PERSONAL_ACCESS_TOKEN\" https://api.github.com/user\n</code></pre></p> </li> <li> <p>Network Connectivity Issues <pre><code># Check network configuration\ndocker network inspect ai-dev-mcp\n\n# Verify inter-service communication\ndocker-compose -f docker-compose.mcp.yml exec mcp-github curl http://mcp-gitlab:8000/health\n</code></pre></p> </li> </ol>"},{"location":"mcp/deployment/#debug-mode","title":"Debug Mode","text":"<p>Enable debug logging for specific services:</p> <pre><code># Add to service environment\nenvironment:\n  - DEBUG=mcp*\n  - LOG_LEVEL=debug\n</code></pre>"},{"location":"mcp/deployment/#security-best-practices","title":"Security Best Practices","text":""},{"location":"mcp/deployment/#token-management","title":"Token Management","text":"<ol> <li>Use Environment Variables: Never hardcode tokens in configuration files</li> <li>Minimal Permissions: Grant only necessary scopes to API tokens</li> <li>Token Rotation: Regularly rotate access tokens</li> <li>Secret Management: Use Docker secrets for production deployments</li> </ol>"},{"location":"mcp/deployment/#network-security","title":"Network Security","text":"<ol> <li>Isolated Networks: MCP servers run on separate network from main services</li> <li>Access Controls: Filesystem server has directory-based access restrictions</li> <li>Health Checks: Regular health monitoring to detect compromised services</li> </ol>"},{"location":"mcp/deployment/#container-security","title":"Container Security","text":"<ol> <li>Non-root Users: Containers run with non-privileged users where possible</li> <li>Read-only Mounts: Sensitive directories mounted as read-only</li> <li>Resource Limits: Memory and CPU limits prevent resource exhaustion</li> </ol>"},{"location":"mcp/deployment/#integration-examples","title":"Integration Examples","text":""},{"location":"mcp/deployment/#flowiseai-integration","title":"FlowiseAI Integration","text":"<pre><code>// FlowiseAI Custom Tool Node\nconst mcpCall = async (serverUrl, tool, params) =&gt; {\n  const response = await fetch(`${serverUrl}/tools/${tool}`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ arguments: params })\n  });\n  return response.json();\n};\n\n// Example: Create GitHub issue from FlowiseAI\nconst result = await mcpCall('http://mcp-github:8000', 'create_issue', {\n  owner: 'user',\n  repo: 'project',\n  title: 'Automated Issue',\n  body: 'Created from FlowiseAI workflow'\n});\n</code></pre>"},{"location":"mcp/deployment/#litellm-proxy-integration","title":"LiteLLM Proxy Integration","text":"<pre><code># Custom MCP tool integration\nimport requests\n\ndef github_mcp_tool(action, **params):\n    \"\"\"Call GitHub MCP server from LiteLLM\"\"\"\n    response = requests.post(\n        f'http://localhost:9008/tools/{action}',\n        json={'arguments': params}\n    )\n    return response.json()\n\n# Usage in LiteLLM proxy\nresult = github_mcp_tool('list_repositories', owner='username')\n</code></pre>"},{"location":"mcp/deployment/#performance-optimization","title":"Performance Optimization","text":""},{"location":"mcp/deployment/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Memory: 512MB per server (adjust based on usage)</li> <li>CPU: 0.5 cores per server for normal workloads</li> <li>Storage: SSD recommended for Memory server persistence</li> </ul>"},{"location":"mcp/deployment/#scaling-considerations","title":"Scaling Considerations","text":"<ul> <li>Horizontal: Deploy multiple instances behind load balancer</li> <li>Vertical: Increase container resources for high-traffic servers</li> <li>Caching: Implement Redis caching for frequently accessed data</li> </ul> <p>This deployment guide ensures proper setup, monitoring, and maintenance of the MCP ecosystem within AI Dev Local.</p>"},{"location":"mcp/github/","title":"GitHub MCP Server","text":"<p>The GitHub MCP Server provides integration with GitHub for repository management, issue tracking, pull request management, and more.</p>"},{"location":"mcp/github/#features","title":"Features","text":"<ul> <li>Repository Operations: Create, list, fork, and manage repositories on GitHub.</li> <li>Issue Management: Create, list, and manage GitHub issues with ease.</li> <li>Pull Requests: Open, list, and manage pull requests across repositories.</li> <li>Code Security: Integrate with GitHub's code scanning and security features.</li> </ul>"},{"location":"mcp/github/#http-wrapper","title":"HTTP Wrapper","text":"<p>To enable HTTP transport compatibility, the GitHub MCP server uses a Node.js based HTTP wrapper that bridges HTTP requests to the server's stdio communication model. This allows seamless integration with other MCP servers and clients using standard web technologies.</p>"},{"location":"mcp/github/#key-advantages","title":"Key Advantages:","text":"<ul> <li>HTTP Compatibility: Provides a RESTful API, making it easier to integrate with web-based applications and services.</li> <li>Toolset Expansion: Access to 70+ GitHub tools and resources via HTTP endpoints.</li> </ul>"},{"location":"mcp/github/#wrapper-details","title":"Wrapper Details:","text":"<ul> <li>Built using Express.js for lightweight and performant communication.</li> <li>Provides endpoints for core GitHub operations, including repository, issue, and pull request management.</li> <li>Supports dynamic tool execution and real-time response streaming.</li> <li>Includes comprehensive logging and error handling.</li> </ul>"},{"location":"mcp/github/#configuration","title":"Configuration","text":"<p>Ensure you have configured the following environment variables:</p> <ul> <li><code>GITHUB_PERSONAL_ACCESS_TOKEN</code>: Your personal access token for authenticating with GitHub's API.</li> <li><code>GITHUB_TOOLSETS</code>: A comma-separated list of toolsets to enable (e.g., <code>repos,issues,pull_requests</code>).</li> <li><code>GITHUB_READ_ONLY</code>: Option to set the server in read-only mode (default: <code>false</code>).</li> </ul>"},{"location":"mcp/github/#usage-examples","title":"Usage Examples","text":""},{"location":"mcp/github/#listing-repositories","title":"Listing Repositories","text":"<pre><code>mcp-call list_repositories\n</code></pre>"},{"location":"mcp/github/#creating-an-issue","title":"Creating an Issue","text":"<pre><code>mcp-call create_issue --title \"Bug Report\" --body \"Description\" --repo \"owner/repo\"\n</code></pre>"},{"location":"mcp/github/#managing-pull-requests","title":"Managing Pull Requests","text":"<pre><code>mcp-call create_pull_request --base \"main\" --head \"feature\" --title \"New Feature\"\n</code></pre>"},{"location":"mcp/github/#security-considerations","title":"Security Considerations","text":"<ul> <li>Token Security: Keep your GitHub personal access token confidential.</li> <li>Minimal Permissions: Use permissions scopes closely matching your requirements.</li> </ul>"},{"location":"mcp/github/#further-reading","title":"Further Reading","text":"<ul> <li>GitHub API Documentation</li> <li>Creating a personal access token</li> </ul>"},{"location":"mcp/gitlab/","title":"GitLab MCP Server","text":"<p>The GitLab MCP Server provides integration with GitLab for managing repositories, issues, merge requests, and CI/CD operations.</p>"},{"location":"mcp/gitlab/#features","title":"Features","text":"<ul> <li>Repository Management: Create, list, and manage GitLab repositories.</li> <li>Issue Tracking: Create, list, and track issues across projects.</li> <li>Merge Requests: Manage merge requests across branches and projects.</li> <li>CI/CD: Access pipeline and job information directly from GitLab.</li> </ul>"},{"location":"mcp/gitlab/#configuration","title":"Configuration","text":"<p>Ensure you have the following environment variables configured:</p> <ul> <li><code>GITLAB_TOKEN</code>: Your personal access token for authenticating with the GitLab API.</li> <li><code>GITLAB_URL</code>: The base URL for the GitLab instance (default: <code>https://gitlab.com</code>).</li> <li><code>GITLAB_API_VERSION</code>: API version to use (default: <code>v4</code>).</li> </ul>"},{"location":"mcp/gitlab/#usage-examples","title":"Usage Examples","text":""},{"location":"mcp/gitlab/#listing-repositories","title":"Listing Repositories","text":"<pre><code>mcp-call list_projects\n</code></pre>"},{"location":"mcp/gitlab/#creating-an-issue","title":"Creating an Issue","text":"<pre><code>mcp-call create_issue --title \"New Bug Report\" --project_id 123 --description \"Bug details...\"\n</code></pre>"},{"location":"mcp/gitlab/#handling-merge-requests","title":"Handling Merge Requests","text":"<pre><code>mcp-call create_merge_request --source_branch \"feature\" --target_branch \"main\" --title \"Feature Addition\"\n</code></pre>"},{"location":"mcp/gitlab/#security-considerations","title":"Security Considerations","text":"<ul> <li>Token Security: Keep your GitLab token secure and do not share it.</li> <li>Read-Only Mode: Use read-only tokens or permissions for safer operations if write access is not needed.</li> </ul>"},{"location":"mcp/gitlab/#further-reading","title":"Further Reading","text":"<ul> <li>GitLab API Documentation</li> <li>GitLab Personal Access Tokens</li> </ul>"},{"location":"mcp/postgresql/","title":"PostgreSQL MCP Server","text":"<p>The PostgreSQL MCP Server provides robust integration for database operations within the MCP ecosystem, allowing secure and efficient access to PostgreSQL databases through Server-Sent Events (SSE) transport.</p>"},{"location":"mcp/postgresql/#features","title":"Features","text":"<ul> <li>Database Queries: Execute SQL queries and transactions with ease</li> <li>Schema Exploration: Inspect and explore database schemas and structures</li> <li>Data Management: Perform CRUD operations on PostgreSQL data</li> <li>Event Streaming: Leverage Server-Sent Events (SSE) for real-time data updates</li> <li>Restricted Access: Runs in restricted mode for enhanced security</li> <li>Multi-Network: Connected to both MCP network and main AI Dev Local network</li> </ul>"},{"location":"mcp/postgresql/#server-configuration","title":"Server Configuration","text":"<p>Image: <code>crystaldba/postgres-mcp:latest</code> Port: 9006 Transport: Server-Sent Events (SSE) Access Mode: Restricted</p>"},{"location":"mcp/postgresql/#docker-compose-configuration","title":"Docker Compose Configuration","text":"<pre><code>mcp-postgres:\n  image: crystaldba/postgres-mcp:latest\n  restart: unless-stopped\n  command: [\"--access-mode=restricted\", \"--transport=sse\"]\n  environment:\n    - DATABASE_URI=postgresql://postgres:postgres@postgres:5432/postgres\n    - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/postgres\n    - POSTGRES_HOST=postgres\n    - POSTGRES_PORT=5432\n    - POSTGRES_DB=postgres\n    - POSTGRES_USER=postgres\n    - POSTGRES_PASSWORD=postgres\n  ports:\n    - \"${MCP_POSTGRES_PORT:-9006}:8000\"\n  networks:\n    - mcp-network\n    - ai-dev-local\n</code></pre>"},{"location":"mcp/postgresql/#environment-variables","title":"Environment Variables","text":"<p>Ensure you have configured the following environment variables:</p> <ul> <li><code>DATABASE_URI</code>: URI connection string for PostgreSQL (default: <code>postgresql://postgres:postgres@postgres:5432/postgres</code>)</li> <li><code>DATABASE_URL</code>: Alternative connection string format</li> <li><code>POSTGRES_HOST</code>: The hostname of the PostgreSQL server (default: <code>postgres</code>)</li> <li><code>POSTGRES_PORT</code>: Port on which the PostgreSQL server is running (default: <code>5432</code>)</li> <li><code>POSTGRES_DB</code>: Database name (default: <code>postgres</code>)</li> <li><code>POSTGRES_USER</code>: Database username (default: <code>postgres</code>)</li> <li><code>POSTGRES_PASSWORD</code>: Database password (default: <code>postgres</code>)</li> </ul>"},{"location":"mcp/postgresql/#available-tools","title":"Available Tools","text":"<p>The PostgreSQL MCP server provides the following tools for database operations:</p>"},{"location":"mcp/postgresql/#core-database-operations","title":"Core Database Operations","text":"<ul> <li><code>query</code>: Execute SQL SELECT queries</li> <li><code>execute</code>: Execute SQL statements (INSERT, UPDATE, DELETE)</li> <li><code>list_tables</code>: List all tables in the database</li> <li><code>describe_table</code>: Get table schema and column information</li> </ul>"},{"location":"mcp/postgresql/#schema-management","title":"Schema Management","text":"<ul> <li><code>create_table</code>: Create new database tables</li> <li><code>drop_table</code>: Delete existing tables</li> <li><code>get_schema_info</code>: Retrieve database schema information</li> </ul>"},{"location":"mcp/postgresql/#data-operations","title":"Data Operations","text":"<ul> <li><code>insert</code>: Insert new records into tables</li> <li><code>update</code>: Update existing records</li> <li><code>delete</code>: Remove records from tables</li> </ul>"},{"location":"mcp/postgresql/#usage-examples","title":"Usage Examples","text":""},{"location":"mcp/postgresql/#running-a-query","title":"Running a Query","text":"<pre><code># Via HTTP endpoint\ncurl -X POST http://localhost:9006/tools/query \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"arguments\": {\"sql\": \"SELECT * FROM users LIMIT 10\"}}'\n\n# Via MCP client\nmcp-call query --sql \"SELECT * FROM users WHERE active = true\"\n</code></pre>"},{"location":"mcp/postgresql/#describe-a-table","title":"Describe a Table","text":"<pre><code># Get table structure\ncurl -X POST http://localhost:9006/tools/describe_table \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"arguments\": {\"table_name\": \"users\"}}'\n\n# Via MCP client\nmcp-call describe_table --table \"users\"\n</code></pre>"},{"location":"mcp/postgresql/#list-all-tables","title":"List All Tables","text":"<pre><code># Get all tables in database\ncurl -X POST http://localhost:9006/tools/list_tables \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n\n# Via MCP client\nmcp-call list_tables\n</code></pre>"},{"location":"mcp/postgresql/#insert-data","title":"Insert Data","text":"<pre><code># Insert new record\ncurl -X POST http://localhost:9006/tools/insert \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"arguments\": {\n      \"table\": \"users\",\n      \"data\": {\n        \"name\": \"John Doe\",\n        \"email\": \"john@example.com\",\n        \"active\": true\n      }\n    }\n  }'\n</code></pre>"},{"location":"mcp/postgresql/#create-a-new-table","title":"Create a New Table","text":"<pre><code># Create table with schema\ncurl -X POST http://localhost:9006/tools/create_table \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"arguments\": {\n      \"name\": \"projects\",\n      \"schema\": \"CREATE TABLE projects (id SERIAL PRIMARY KEY, name VARCHAR(255) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\"\n    }\n  }'\n</code></pre>"},{"location":"mcp/postgresql/#health-monitoring","title":"Health Monitoring","text":""},{"location":"mcp/postgresql/#health-check","title":"Health Check","text":"<pre><code># Check PostgreSQL MCP server health\ncurl http://localhost:9006/health\n\n# Check PostgreSQL database connectivity\ndocker-compose -f docker-compose.mcp.yml exec mcp-postgres pg_isready -h postgres -U postgres\n</code></pre>"},{"location":"mcp/postgresql/#service-status","title":"Service Status","text":"<pre><code># Check service status\nai-dev-local mcp status | grep postgres\n\n# View service logs\ndocker-compose -f docker-compose.mcp.yml logs -f mcp-postgres\n</code></pre>"},{"location":"mcp/postgresql/#network-configuration","title":"Network Configuration","text":"<p>The PostgreSQL MCP server is connected to two networks:</p> <ol> <li><code>mcp-network</code>: For communication with other MCP servers</li> <li><code>ai-dev-local</code>: For connection to the main PostgreSQL database</li> </ol> <p>This dual-network setup ensures secure database access while maintaining MCP ecosystem connectivity.</p>"},{"location":"mcp/postgresql/#security-considerations","title":"Security Considerations","text":""},{"location":"mcp/postgresql/#access-control","title":"Access Control","text":"<ul> <li>Restricted Mode: Server runs with <code>--access-mode=restricted</code> for enhanced security</li> <li>Network Isolation: Runs on isolated Docker networks</li> <li>Connection Security: Uses internal Docker networking for database connections</li> </ul>"},{"location":"mcp/postgresql/#database-security","title":"Database Security","text":"<ul> <li>Credentials: Database credentials are managed via environment variables</li> <li>Network Access: Database accessible only within Docker network</li> <li>Least Privilege: Grant minimal necessary permissions to database users</li> </ul>"},{"location":"mcp/postgresql/#best-practices","title":"Best Practices","text":"<ul> <li>Environment Variables: Never hardcode database credentials</li> <li>TLS Connections: Enable SSL/TLS for database connections in production</li> <li>Regular Updates: Keep PostgreSQL and MCP server images updated</li> <li>Monitoring: Monitor database performance and access patterns</li> </ul>"},{"location":"mcp/postgresql/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mcp/postgresql/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Connection Failures <pre><code># Test database connectivity\ndocker-compose -f docker-compose.mcp.yml exec mcp-postgres \\\n  psql postgresql://postgres:postgres@postgres:5432/postgres -c \"SELECT 1;\"\n</code></pre></p> </li> <li> <p>Service Not Starting <pre><code># Check logs for errors\ndocker-compose -f docker-compose.mcp.yml logs mcp-postgres\n\n# Restart service\ndocker-compose -f docker-compose.mcp.yml restart mcp-postgres\n</code></pre></p> </li> <li> <p>SSE Transport Issues <pre><code># Verify SSE endpoint\ncurl -H \"Accept: text/event-stream\" http://localhost:9006/sse\n</code></pre></p> </li> </ol>"},{"location":"mcp/postgresql/#integration-examples","title":"Integration Examples","text":""},{"location":"mcp/postgresql/#flowiseai-integration","title":"FlowiseAI Integration","text":"<pre><code>// FlowiseAI Custom Database Node\nconst executeQuery = async (sql, params = {}) =&gt; {\n  const response = await fetch('http://mcp-postgres:8000/tools/query', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ arguments: { sql, params } })\n  });\n  return response.json();\n};\n\n// Example: Get user analytics\nconst analytics = await executeQuery(\n  'SELECT COUNT(*) as total_users, AVG(age) as avg_age FROM users WHERE active = $1',\n  [true]\n);\n</code></pre>"},{"location":"mcp/postgresql/#litellm-integration","title":"LiteLLM Integration","text":"<pre><code># Custom database tool for LiteLLM\nimport requests\n\ndef db_query_tool(sql, **params):\n    \"\"\"Execute database query via MCP\"\"\"\n    response = requests.post(\n        'http://localhost:9006/tools/query',\n        json={'arguments': {'sql': sql, 'params': params}}\n    )\n    return response.json()\n\n# Usage example\nresult = db_query_tool(\n    'SELECT * FROM projects WHERE status = %(status)s',\n    status='active'\n)\n</code></pre>"},{"location":"mcp/postgresql/#further-reading","title":"Further Reading","text":"<ul> <li>PostgreSQL Documentation</li> <li>PostgreSQL MCP Server Repository</li> <li>Model Context Protocol Specification</li> <li>pgAdmin Tool for database management and visualization</li> <li>Docker PostgreSQL Official Image</li> </ul>"},{"location":"mcp/sonarqube/","title":"SonarQube MCP Server","text":"<p>The SonarQube MCP Server provides integration with SonarQube for code quality analysis, security scanning, and technical debt management.</p>"},{"location":"mcp/sonarqube/#features","title":"Features","text":"<ul> <li>Code Quality Analysis: Access comprehensive code quality metrics and reports.</li> <li>Security Scanning: Identify security vulnerabilities and hotspots in your codebase.</li> <li>Technical Debt Management: Track and manage technical debt across projects.</li> <li>Quality Gates: Monitor quality gates and compliance status.</li> </ul>"},{"location":"mcp/sonarqube/#configuration","title":"Configuration","text":"<p>Ensure you have configured the following environment variables:</p> <ul> <li><code>SONARQUBE_URL</code>: The URL of your SonarQube instance (default: <code>http://localhost:9000</code>).</li> <li><code>SONARQUBE_TOKEN</code>: Your authentication token for accessing the SonarQube API.</li> <li><code>SONARQUBE_ORGANIZATION</code>: Your organization key (required for SonarCloud).</li> </ul>"},{"location":"mcp/sonarqube/#usage-examples","title":"Usage Examples","text":""},{"location":"mcp/sonarqube/#listing-projects","title":"Listing Projects","text":"<pre><code>mcp-call list_projects\n</code></pre>"},{"location":"mcp/sonarqube/#getting-project-metrics","title":"Getting Project Metrics","text":"<pre><code>mcp-call get_measures --project_key \"my-project\" --metrics \"coverage,bugs,vulnerabilities\"\n</code></pre>"},{"location":"mcp/sonarqube/#searching-for-issues","title":"Searching for Issues","text":"<pre><code>mcp-call search_issues --project_key \"my-project\" --types \"BUG,VULNERABILITY\"\n</code></pre>"},{"location":"mcp/sonarqube/#getting-security-hotspots","title":"Getting Security Hotspots","text":"<pre><code>mcp-call get_hotspots --project_key \"my-project\"\n</code></pre>"},{"location":"mcp/sonarqube/#available-metrics","title":"Available Metrics","text":"<p>SonarQube provides numerous metrics for code analysis:</p> <ul> <li>Reliability: <code>bugs</code>, <code>reliability_rating</code></li> <li>Security: <code>vulnerabilities</code>, <code>security_rating</code>, <code>security_hotspots</code></li> <li>Maintainability: <code>code_smells</code>, <code>sqale_rating</code>, <code>technical_debt</code></li> <li>Coverage: <code>coverage</code>, <code>line_coverage</code>, <code>branch_coverage</code></li> <li>Duplications: <code>duplicated_lines_density</code>, <code>duplicated_blocks</code></li> <li>Size: <code>lines</code>, <code>ncloc</code>, <code>classes</code>, <code>functions</code></li> </ul>"},{"location":"mcp/sonarqube/#quality-gate-status","title":"Quality Gate Status","text":"<p>Monitor your project's quality gate status:</p> <pre><code>mcp-call get_project --project_key \"my-project\"\n</code></pre> <p>The quality gate status indicates whether your project meets the defined quality criteria.</p>"},{"location":"mcp/sonarqube/#security-considerations","title":"Security Considerations","text":"<ul> <li>Token Security: Keep your SonarQube token secure and limit its permissions.</li> <li>Network Access: Ensure proper network security between your application and SonarQube instance.</li> <li>Data Privacy: Be mindful of code analysis data being transmitted to external SonarQube instances.</li> </ul>"},{"location":"mcp/sonarqube/#integration-with-cicd","title":"Integration with CI/CD","text":"<p>The SonarQube MCP server works well with CI/CD pipelines:</p> <ol> <li>Analysis Results: Retrieve analysis results after code scans</li> <li>Quality Gates: Check quality gate status before deployment</li> <li>Issue Tracking: Monitor new issues introduced in recent commits</li> </ol>"},{"location":"mcp/sonarqube/#further-reading","title":"Further Reading","text":"<ul> <li>SonarQube Documentation</li> <li>SonarQube Web API</li> <li>SonarQube Authentication</li> </ul>"},{"location":"services/flowiseai/","title":"FlowiseAI Service","text":"<p>FlowiseAI is a powerful visual workflow builder that enables developers to create LLM-based applications through an intuitive drag-and-drop interface. It supports integration with multiple AI services and provides a low-code solution for complex AI workflows.</p>"},{"location":"services/flowiseai/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[User Interface] --&gt;|Drag &amp; Drop| B[Workflow Designer]\n    B --&gt;|Configure| C[Node Components]\n    C --&gt;|Connect| D[LLM Providers]\n    C --&gt;|Connect| E[Data Sources]\n    C --&gt;|Connect| F[Output Handlers]\n    D --&gt; G[OpenAI]\n    D --&gt; H[Anthropic]\n    D --&gt; I[Local Models]\n    E --&gt; J[Vector Stores]\n    E --&gt; K[Databases]\n    F --&gt; L[APIs]\n    F --&gt; M[Webhooks]\n\n    %% Styling\n    classDef uiClass fill:#e3f2fd,stroke:#0d47a1,stroke-width:2px,color:#000\n    classDef designerClass fill:#f1f8e9,stroke:#33691e,stroke-width:2px,color:#000\n    classDef componentClass fill:#fff8e1,stroke:#f57f17,stroke-width:2px,color:#000\n    classDef providerClass fill:#fce4ec,stroke:#ad1457,stroke-width:2px,color:#000\n    classDef dataClass fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000\n    classDef outputClass fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px,color:#000\n    classDef serviceClass fill:#ede7f6,stroke:#4527a0,stroke-width:2px,color:#000\n\n    class A uiClass\n    class B designerClass\n    class C componentClass\n    class D,G,H,I providerClass\n    class E,J,K dataClass\n    class F,L,M outputClass</code></pre>"},{"location":"services/flowiseai/#key-features","title":"Key Features","text":""},{"location":"services/flowiseai/#visual-workflow-builder","title":"Visual Workflow Builder","text":"<ul> <li>Drag-and-drop interface for creating complex AI workflows</li> <li>Pre-built components for common AI tasks</li> <li>Real-time flow execution and debugging</li> <li>Visual data flow representation</li> </ul>"},{"location":"services/flowiseai/#llm-integrations","title":"LLM Integrations","text":"<ul> <li>Support for multiple LLM providers (OpenAI, Anthropic, Cohere, etc.)</li> <li>Local model integration via Ollama</li> <li>Custom model endpoint configuration</li> <li>Chain multiple models in sequence</li> </ul>"},{"location":"services/flowiseai/#data-connectors","title":"Data Connectors","text":"<ul> <li>Vector database integration (Pinecone, Weaviate, etc.)</li> <li>Document loaders (PDF, Word, web scraping)</li> <li>Database connectors (PostgreSQL, MongoDB)</li> <li>API integration capabilities</li> </ul>"},{"location":"services/flowiseai/#advanced-capabilities","title":"Advanced Capabilities","text":"<ul> <li>Memory management for conversational flows</li> <li>Custom function nodes with JavaScript/Python</li> <li>Conditional logic and branching</li> <li>Error handling and retry mechanisms</li> </ul>"},{"location":"services/flowiseai/#configuration-schema","title":"Configuration Schema","text":""},{"location":"services/flowiseai/#environment-variables","title":"Environment Variables","text":"<pre><code># Basic Configuration\nPORT=3000\nDATABASE_TYPE=postgres\nDATABASE_HOST=postgres\nDATABASE_USER=postgres\nDATABASE_PASSWORD=postgres\nDATABASE_NAME=flowise\n\n# Authentication\nJWT_AUTH_TOKEN_SECRET=${JWT_AUTH_TOKEN_SECRET}\nFLOWISE_USERNAME=${FLOWISE_USERNAME}\nFLOWISE_PASSWORD=${FLOWISE_PASSWORD}\n\n# Storage Paths\nSECRETKEY_PATH=/root/.flowise\nLOG_PATH=/root/.flowise/logs\nBLOB_STORAGE_PATH=/root/.flowise/storage\n\n# Telemetry Settings\nDISABLE_FLOWISE_TELEMETRY=true\n</code></pre>"},{"location":"services/flowiseai/#node-configuration-example","title":"Node Configuration Example","text":"<pre><code>{\n  \"id\": \"openai-node\",\n  \"type\": \"LLMChain\",\n  \"data\": {\n    \"model\": \"gpt-3.5-turbo\",\n    \"temperature\": 0.7,\n    \"maxTokens\": 1000,\n    \"prompt\": \"Answer the following question: {question}\"\n  },\n  \"inputs\": [\"question\"],\n  \"outputs\": [\"response\"]\n}\n</code></pre>"},{"location":"services/flowiseai/#access","title":"Access","text":"<p>FlowiseAI is accessible at:</p> <pre><code>http://localhost:3001/\n</code></pre>"},{"location":"services/flowiseai/#supported-integrations","title":"Supported Integrations","text":""},{"location":"services/flowiseai/#llm-providers","title":"LLM Providers","text":"<ul> <li>OpenAI (GPT-3.5, GPT-4)</li> <li>Anthropic (Claude)</li> <li>Cohere</li> <li>Hugging Face</li> <li>Azure OpenAI</li> <li>Local models via Ollama</li> </ul>"},{"location":"services/flowiseai/#vector-databases","title":"Vector Databases","text":"<ul> <li>Pinecone</li> <li>Weaviate</li> <li>Qdrant</li> <li>Chroma</li> <li>Supabase</li> </ul>"},{"location":"services/flowiseai/#document-loaders","title":"Document Loaders","text":"<ul> <li>PDF files</li> <li>CSV/Excel files</li> <li>Web scraping</li> <li>Notion pages</li> <li>GitHub repositories</li> </ul>"},{"location":"services/flowiseai/#online-resources","title":"Online Resources","text":"<ul> <li>GitHub Repository: FlowiseAI GitHub</li> <li>Official Website: FlowiseAI.com</li> <li>Documentation: FlowiseAI Docs</li> <li>Community: Discord Server</li> </ul>"},{"location":"services/flowiseai/#use-cases","title":"Use Cases","text":"<ul> <li>Chatbots: Build intelligent conversational agents</li> <li>Document Q&amp;A: Create systems for querying large document collections</li> <li>Content Generation: Automate content creation workflows</li> <li>Data Analysis: Build AI-powered analytics pipelines</li> <li>API Integration: Connect multiple services with AI processing</li> </ul> <p>FlowiseAI is perfect for developers, product managers, and AI enthusiasts who want to rapidly prototype and deploy AI applications without extensive coding.</p>"},{"location":"services/langfuse/","title":"Langfuse Service","text":"<p>Langfuse provides comprehensive observability for large language models (LLMs). It enables you to track usage, analyze performance metrics, and debug workflow issues within AI-driven applications.</p>"},{"location":"services/langfuse/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TD;\n    A[User Interaction] --&gt;|Requests| B[API Gateway];\n    B --&gt;|Forward| C[Langfuse Service];\n    C --&gt;|Metrics| D[Analytics Dashboard];\n    C --&gt;|Logs| E[Log Storage];\n    C --&gt;|Alerts| F[Alert System];\n\n    %% Styling\n    classDef userClass fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef gatewayClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n    classDef serviceClass fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px,color:#000\n    classDef storageClass fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000\n    classDef alertClass fill:#ffebee,stroke:#b71c1c,stroke-width:2px,color:#000\n\n    class A userClass\n    class B gatewayClass\n    class C serviceClass\n    class D,E storageClass\n    class F alertClass</code></pre>"},{"location":"services/langfuse/#features","title":"Features","text":"<ul> <li>Real-time LLM usage tracking</li> <li>Comprehensive performance analysis</li> <li>AI workflow debugging</li> <li>Query and event logging</li> <li>Custom alerts and notifications</li> </ul>"},{"location":"services/langfuse/#configuration","title":"Configuration","text":""},{"location":"services/langfuse/#telemetry-settings","title":"Telemetry Settings","text":"<pre><code># Disable telemetry for privacy\nTELEMETRY_ENABLED=false\n</code></pre>"},{"location":"services/langfuse/#access","title":"Access","text":"<p>The Langfuse dashboard is accessible at:</p> <pre><code>http://localhost:3000/\n</code></pre>"},{"location":"services/langfuse/#online-resources","title":"Online Resources","text":"<ul> <li>GitHub Repository: Langfuse GitHub</li> <li>Web Documentation: Langfuse Docs</li> </ul> <p>Langfuse is ideal for teams needing detailed insights into LLM usage patterns and operational metrics.</p>"},{"location":"services/litellm/","title":"LiteLLM Proxy Service","text":"<p>LiteLLM Proxy serves as a unified API gateway that provides a standardized OpenAI-compatible interface for 100+ large language model providers. It enables seamless switching between different LLM providers while maintaining consistent API contracts and adding enterprise features like load balancing, rate limiting, and cost tracking.</p>"},{"location":"services/litellm/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Client Applications] --&gt;|OpenAI Format| B[LiteLLM Proxy]\n    B --&gt;|Route &amp; Transform| C[Request Router]\n    C --&gt;|Load Balance| D[Provider Pool]\n    D --&gt; E[OpenAI GPT-4]\n    D --&gt; F[Anthropic Claude]\n    D --&gt; G[Google Gemini]\n    D --&gt; H[Cohere Command]\n    D --&gt; I[Local Ollama]\n    B --&gt;|Log &amp; Monitor| J[Analytics DB]\n    B --&gt;|Cache Responses| K[Redis Cache]\n    B --&gt;|Rate Limit| L[Rate Limiter]\n\n    %% Styling\n    classDef clientClass fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000\n    classDef proxyClass fill:#e3f2fd,stroke:#1565c0,stroke-width:3px,color:#000\n    classDef routerClass fill:#fff8e1,stroke:#f9a825,stroke-width:2px,color:#000\n    classDef poolClass fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000\n    classDef openaiClass fill:#ffebee,stroke:#d32f2f,stroke-width:2px,color:#000\n    classDef anthropicClass fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef googleClass fill:#e0f2f1,stroke:#00796b,stroke-width:2px,color:#000\n    classDef cohereClass fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000\n    classDef localClass fill:#ede7f6,stroke:#512da8,stroke-width:2px,color:#000\n    classDef analyticsClass fill:#e8eaf6,stroke:#3f51b5,stroke-width:2px,color:#000\n    classDef cacheClass fill:#ffebee,stroke:#e91e63,stroke-width:2px,color:#000\n    classDef limitClass fill:#e0f7fa,stroke:#00838f,stroke-width:2px,color:#000\n\n    class A clientClass\n    class B proxyClass\n    class C routerClass\n    class D poolClass\n    class E openaiClass\n    class F anthropicClass\n    class G googleClass\n    class H cohereClass\n    class I localClass\n    class J analyticsClass\n    class K cacheClass\n    class L limitClass</code></pre>"},{"location":"services/litellm/#key-features","title":"Key Features","text":""},{"location":"services/litellm/#unified-api-interface","title":"Unified API Interface","text":"<ul> <li>OpenAI-compatible endpoints for all supported providers</li> <li>Consistent request/response format across different models</li> <li>Automatic parameter translation between provider schemas</li> <li>Streaming support for real-time responses</li> </ul>"},{"location":"services/litellm/#enterprise-grade-functionality","title":"Enterprise-Grade Functionality","text":"<ul> <li>Load balancing across multiple model instances</li> <li>Intelligent failover when providers are unavailable</li> <li>Rate limiting per user, API key, or model</li> <li>Cost tracking and budget enforcement</li> <li>Request/response caching for improved performance</li> </ul>"},{"location":"services/litellm/#multi-provider-support","title":"Multi-Provider Support","text":"<ul> <li>100+ LLM providers supported out of the box</li> <li>Custom endpoint configuration for proprietary models</li> <li>Model routing strategies (cost-based, performance-based)</li> <li>Provider-specific optimizations and retry logic</li> </ul>"},{"location":"services/litellm/#observability-analytics","title":"Observability &amp; Analytics","text":"<ul> <li>Real-time usage dashboards with Langfuse integration</li> <li>Detailed cost breakdowns per model and user</li> <li>Performance metrics (latency, throughput, error rates)</li> <li>Audit logs for compliance and debugging</li> </ul>"},{"location":"services/litellm/#configuration-schema","title":"Configuration Schema","text":""},{"location":"services/litellm/#environment-variables","title":"Environment Variables","text":"<pre><code># Core Settings\nLITELLM_MASTER_KEY=sk-your-master-key-here\nDATABASE_URL=postgresql://postgres:postgres@postgres:5432/litellm\n\n# Redis Configuration\nREDIS_HOST=redis\nREDIS_PORT=6379\nREDIS_PASSWORD=\n\n# UI Authentication\nUI_USERNAME=${LITELLM_UI_USERNAME}\nUI_PASSWORD=${LITELLM_UI_PASSWORD}\n\n# Provider API Keys\nOPENAI_API_KEY=sk-proj-your-openai-key\nANTHROPIC_API_KEY=sk-ant-your-anthropic-key\nGEMINI_API_KEY=your-gemini-key\nCOHERE_API_KEY=your-cohere-key\n\n# Langfuse Integration\nLANGFUSE_PUBLIC_KEY=pk-your-langfuse-public-key\nLANGFUSE_SECRET_KEY=sk-your-langfuse-secret-key\nLANGFUSE_HOST=http://langfuse:3000\n\n# Logging\nLITELLM_LOG=INFO\nLITELLM_DEBUG=false\n\n# Telemetry Settings\nLITELLM_TELEMETRY=false\n</code></pre>"},{"location":"services/litellm/#model-configuration-litellm_configyaml","title":"Model Configuration (litellm_config.yaml)","text":"<pre><code>model_list:\n  # OpenAI Models\n  - model_name: gpt-4\n    litellm_params:\n      model: openai/gpt-4\n      api_key: os.environ/OPENAI_API_KEY\n      max_tokens: 4096\n      temperature: 0.7\n\n  - model_name: gpt-3.5-turbo\n    litellm_params:\n      model: openai/gpt-3.5-turbo\n      api_key: os.environ/OPENAI_API_KEY\n      max_tokens: 2048\n\n  # Anthropic Models\n  - model_name: claude-3-sonnet\n    litellm_params:\n      model: anthropic/claude-3-sonnet-20240229\n      api_key: os.environ/ANTHROPIC_API_KEY\n      max_tokens: 2048\n\n  # Google Models\n  - model_name: gemini-pro\n    litellm_params:\n      model: gemini/gemini-pro\n      api_key: os.environ/GEMINI_API_KEY\n\n  # Local Ollama Models\n  - model_name: llama2\n    litellm_params:\n      model: ollama/llama2\n      api_base: http://host.docker.internal:11434\n\n# Router Configuration\nrouter_settings:\n  routing_strategy: usage-based-routing\n  model_group_alias:\n    gpt-4-group: [\"gpt-4\", \"gpt-4-turbo\"]\n    claude-group: [\"claude-3-opus\", \"claude-3-sonnet\", \"claude-3-haiku\"]\n  fallbacks:\n    - gpt-4: [\"gpt-3.5-turbo\"]\n    - claude-3-opus: [\"claude-3-sonnet\", \"gpt-4\"]\n\n# General Settings\ngeneral_settings:\n  master_key: os.environ/LITELLM_MASTER_KEY\n  database_url: os.environ/DATABASE_URL\n\n  # Cost &amp; Budget Controls\n  track_cost_per_model: true\n  max_budget: 100.0\n  budget_duration: 30d\n\n  # Caching\n  cache: true\n  cache_params:\n    type: redis\n    host: os.environ/REDIS_HOST\n    port: os.environ/REDIS_PORT\n    ttl: 600\n\n  # Rate Limiting\n  tpm_limit: 10000  # tokens per minute\n  rpm_limit: 100    # requests per minute\n\n  # Callbacks for Observability\n  success_callback: [\"langfuse\"]\n  failure_callback: [\"langfuse\"]\n\n  # Security\n  allowed_ips: [\"*\"]\n  blocked_ips: []\n</code></pre>"},{"location":"services/litellm/#api-request-schema","title":"API Request Schema","text":"<pre><code>{\n  \"model\": \"gpt-4\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are a helpful assistant.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello, how are you?\"\n    }\n  ],\n  \"temperature\": 0.7,\n  \"max_tokens\": 1000,\n  \"stream\": false\n}\n</code></pre>"},{"location":"services/litellm/#api-response-schema","title":"API Response Schema","text":"<pre><code>{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1677652288,\n  \"model\": \"gpt-4\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! I'm doing well, thank you for asking.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 20,\n    \"completion_tokens\": 12,\n    \"total_tokens\": 32\n  }\n}\n</code></pre>"},{"location":"services/litellm/#access","title":"Access","text":"<p>LiteLLM Proxy is accessible at:</p> <pre><code>http://localhost:4000/\n</code></pre>"},{"location":"services/litellm/#key-endpoints","title":"Key Endpoints","text":"<ul> <li>Chat Completions: <code>POST /v1/chat/completions</code></li> <li>Completions: <code>POST /v1/completions</code></li> <li>Models List: <code>GET /v1/models</code></li> <li>Health Check: <code>GET /health</code></li> <li>UI Dashboard: <code>GET /ui</code> (admin interface)</li> <li>Metrics: <code>GET /metrics</code> (Prometheus format)</li> </ul>"},{"location":"services/litellm/#supported-providers","title":"Supported Providers","text":""},{"location":"services/litellm/#major-cloud-providers","title":"Major Cloud Providers","text":"<ul> <li>OpenAI: GPT-3.5, GPT-4, GPT-4 Vision, DALL-E</li> <li>Anthropic: Claude 3 (Haiku, Sonnet, Opus)</li> <li>Google: Gemini Pro, Gemini Pro Vision, PaLM</li> <li>Cohere: Command, Command-Light, Embed</li> <li>Azure OpenAI: All OpenAI models via Azure</li> <li>AWS Bedrock: Titan, Claude, Llama models</li> </ul>"},{"location":"services/litellm/#open-source-local","title":"Open Source &amp; Local","text":"<ul> <li>Ollama: Local model serving</li> <li>Hugging Face: Transformers models</li> <li>vLLM: High-performance inference</li> <li>Together AI: Open source models</li> <li>Replicate: Community-hosted models</li> </ul>"},{"location":"services/litellm/#specialized-providers","title":"Specialized Providers","text":"<ul> <li>Stability AI: Stable Diffusion models</li> <li>Mistral AI: Mistral 7B, Mixtral 8x7B</li> <li>AI21: Jurassic models</li> <li>Aleph Alpha: Luminous models</li> </ul>"},{"location":"services/litellm/#advanced-features","title":"Advanced Features","text":""},{"location":"services/litellm/#load-balancing-strategies","title":"Load Balancing Strategies","text":"<pre><code>router_settings:\n  routing_strategy: \"usage-based-routing\"  # or \"simple-shuffle\", \"least-busy\", \"latency-based\"\n  cooldown_time: 1  # seconds between retries\n  retry_policy:\n    max_retries: 3\n    base_delay: 1\n    max_delay: 10\n</code></pre>"},{"location":"services/litellm/#cost-controls","title":"Cost Controls","text":"<pre><code>general_settings:\n  max_budget: 100.0\n  budget_duration: \"30d\"\n  budget_reset_at: \"00:00\"\n  cost_per_token:\n    gpt-4: 0.00006  # per token\n    gpt-3.5-turbo: 0.000002\n</code></pre>"},{"location":"services/litellm/#rate-limiting","title":"Rate Limiting","text":"<pre><code>general_settings:\n  tpm_limit: 10000  # tokens per minute\n  rpm_limit: 100    # requests per minute\n  max_parallel_requests: 10\n  user_rate_limit:\n    \"user-1\": {\"tpm\": 5000, \"rpm\": 50}\n</code></pre>"},{"location":"services/litellm/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"services/litellm/#health-check-response","title":"Health Check Response","text":"<pre><code>{\n  \"status\": \"healthy\",\n  \"healthy_endpoints\": [\n    {\n      \"model\": \"gpt-4\",\n      \"status\": \"healthy\",\n      \"latency_ms\": 245\n    }\n  ],\n  \"unhealthy_endpoints\": [],\n  \"version\": \"1.0.0\",\n  \"uptime\": \"2h 15m 30s\"\n}\n</code></pre>"},{"location":"services/litellm/#usage-analytics","title":"Usage Analytics","text":"<ul> <li>Request volume per model and time period</li> <li>Cost analysis with breakdown by user/model</li> <li>Performance metrics (P95, P99 latency)</li> <li>Error rate tracking with categorization</li> <li>Token usage patterns and optimization insights</li> </ul>"},{"location":"services/litellm/#troubleshooting","title":"Troubleshooting","text":""},{"location":"services/litellm/#common-issues","title":"Common Issues","text":"<ol> <li>\"Invalid HTTP request received\" warnings</li> <li>Usually caused by invalid API keys or authentication issues</li> <li> <p>See LiteLLM Troubleshooting guide</p> </li> <li> <p>Model unavailable errors</p> </li> <li>Check provider API key configuration</li> <li>Verify model name spelling and availability</li> <li> <p>Review rate limiting settings</p> </li> <li> <p>High latency issues</p> </li> <li>Enable response caching</li> <li>Implement load balancing</li> <li>Check provider geographic regions</li> </ol>"},{"location":"services/litellm/#debug-commands","title":"Debug Commands","text":"<pre><code># Check health status\ncurl -H \"Authorization: Bearer $LITELLM_MASTER_KEY\" http://localhost:4000/health\n\n# List available models\ncurl -H \"Authorization: Bearer $LITELLM_MASTER_KEY\" http://localhost:4000/v1/models\n\n# View metrics\ncurl http://localhost:4000/metrics\n</code></pre>"},{"location":"services/litellm/#cli-integration-for-ollama-management","title":"CLI Integration for Ollama Management","text":"<p>The AI Dev Local CLI provides seamless integration for managing Ollama models with LiteLLM:</p> <pre><code># Browse available Ollama models\nai-dev-local ollama list-available\nai-dev-local ollama list-available --category code\nai-dev-local ollama list-available --search llama\n\n# Pull and install Ollama models\nai-dev-local ollama pull llama2:7b\nai-dev-local ollama pull codellama:7b\n\n# Sync installed Ollama models to LiteLLM configuration\nai-dev-local ollama sync-litellm\n\n# Preview changes before applying\nai-dev-local ollama sync-litellm --dry-run\n\n# Restart LiteLLM to apply new configuration\ndocker-compose restart litellm\n</code></pre> <p>Automatic Configuration Management: - Detects all installed Ollama models automatically - Updates <code>litellm_config.yaml</code> with current Ollama models - Removes outdated Ollama model entries - Preserves all non-Ollama model configurations - Updates router group aliases for proper load balancing - Creates timestamped backups before making changes</p> <p>Typical Workflow: 1. Start services with Ollama: <code>ai-dev-local start --ollama</code> 2. Browse available models: <code>ai-dev-local ollama list-available --category code</code> 3. Install desired models: <code>ai-dev-local ollama pull codellama:7b</code> 4. Sync to LiteLLM: <code>ai-dev-local ollama sync-litellm</code> 5. Restart LiteLLM: <code>docker-compose restart litellm</code> 6. Verify in health check: Models should appear as healthy endpoints</p> <p>See the CLI Reference for complete documentation of all Ollama management commands.</p>"},{"location":"services/litellm/#online-resources","title":"Online Resources","text":"<ul> <li>GitHub Repository: LiteLLM GitHub</li> <li>Official Website: LiteLLM.ai</li> <li>Documentation: LiteLLM Docs</li> <li>API Reference: API Docs</li> <li>Community: Discord Server</li> <li>Docker Hub: LiteLLM Images</li> </ul>"},{"location":"services/litellm/#use-cases","title":"Use Cases","text":"<ul> <li>Multi-Provider Abstraction: Single API for multiple LLM providers</li> <li>Cost Optimization: Automatic routing to cost-effective models</li> <li>High Availability: Failover between providers for reliability</li> <li>Development &amp; Testing: Easy model comparison and A/B testing</li> <li>Enterprise Deployment: Centralized LLM access with governance</li> <li>Microservices Architecture: Unified LLM gateway for distributed systems</li> </ul>"},{"location":"services/litellm/#performance-optimizations","title":"Performance Optimizations","text":"<ul> <li>Response Caching: Redis-based caching for repeated queries</li> <li>Connection Pooling: Efficient HTTP connection management</li> <li>Batch Processing: Support for batch API requests</li> <li>Streaming Responses: Real-time response streaming</li> <li>Geographic Routing: Route to nearest provider endpoints</li> </ul> <p>LiteLLM Proxy is essential for organizations requiring a robust, scalable, and cost-effective solution for managing multiple LLM providers through a single, standardized interface.</p>"},{"location":"services/ollama/","title":"Ollama Service Documentation","text":""},{"location":"services/ollama/#overview","title":"Overview","text":"<p>Ollama is a lightweight, extensible framework for building and running large language models (LLMs) locally. It provides a simple API for creating, running, and managing models on your machine, making it easy to integrate local AI capabilities into your applications without relying on cloud services.</p>"},{"location":"services/ollama/#key-features","title":"Key Features","text":"<ul> <li>Local Model Execution: Run LLMs entirely on your local machine without internet connectivity</li> <li>Model Management: Easy downloading, updating, and organization of various LLM models</li> <li>REST API: Simple HTTP API compatible with OpenAI's chat completions format</li> <li>Multi-format Support: Supports GGUF, GGML, and other quantized model formats</li> <li>Hardware Optimization: Automatic GPU acceleration when available (CUDA, Metal, ROCm)</li> <li>Model Customization: Create custom models with Modelfiles</li> <li>Concurrent Sessions: Handle multiple model inference requests simultaneously</li> <li>Memory Management: Efficient model loading and unloading based on usage</li> </ul>"},{"location":"services/ollama/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    %% Client Layer\n    subgraph clients [\"Client Applications\"]\n        py[\"\ud83d\udc0d Python SDK\"]\n        js[\"\ud83d\udfe8 JavaScript SDK\"]\n        go[\"\ud83d\udd35 Go SDK\"]\n        curl[\"\ud83c\udf10 HTTP/REST\"]\n    end\n\n    %% API Layer\n    subgraph api [\"Ollama Server\"]\n        gateway[\"\ud83d\udeaa API Gateway&lt;br/&gt;(OpenAI Compatible)\"]\n\n        subgraph management [\"Model Management\"]\n            loader[\"\ud83d\udce6 Model Loader\"]\n            memory[\"\ud83e\udde0 Memory Manager\"]\n            sessions[\"\ud83d\udd04 Session Handler\"]\n        end\n\n        subgraph inference [\"Inference Engine\"]\n            llama[\"\u26a1 llama.cpp\"]\n            gpu[\"\ud83c\udfae GPU Acceleration\"]\n            quant[\"\ud83d\udcca Quantization\"]\n        end\n    end\n\n    %% Storage Layer\n    subgraph storage [\"Local Storage\"]\n        models[\"\ud83d\udcc1 Downloaded Models&lt;br/&gt;(~/.ollama)\"]\n        blobs[\"\ud83d\uddc3\ufe0f Model Blobs\"]\n        manifests[\"\ud83d\udccb Manifests\"]\n        modelfiles[\"\ud83d\udcdd Custom Modelfiles\"]\n    end\n\n    %% Hardware Layer\n    subgraph hardware [\"Hardware Resources\"]\n        cpu[\"\ud83d\udcbb CPU\"]\n        ram[\"\ud83d\udd32 RAM\"]\n        vram[\"\ud83c\udfaf GPU VRAM\"]\n        disk[\"\ud83d\udcbe Disk Storage\"]\n    end\n\n    %% Connections\n    clients --&gt; gateway\n    gateway --&gt; management\n    gateway --&gt; inference\n    management --&gt; loader\n    management --&gt; memory\n    management --&gt; sessions\n    inference --&gt; llama\n    inference --&gt; gpu\n    inference --&gt; quant\n    loader --&gt; storage\n    memory --&gt; hardware\n    gpu --&gt; vram\n    llama --&gt; cpu\n    llama --&gt; ram\n    storage --&gt; disk\n\n    %% Styling\n    classDef clientStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:#000\n    classDef serverStyle fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000\n    classDef storageStyle fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px,color:#000\n    classDef hardwareStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px,color:#000\n    classDef componentStyle fill:#fff8e1,stroke:#f57f17,stroke-width:1px,color:#000\n\n    class clients,py,js,go,curl clientStyle\n    class api,gateway serverStyle\n    class management,inference,loader,memory,sessions,llama,gpu,quant componentStyle\n    class storage,models,blobs,manifests,modelfiles storageStyle\n    class hardware,cpu,ram,vram,disk hardwareStyle</code></pre>"},{"location":"services/ollama/#configuration","title":"Configuration","text":""},{"location":"services/ollama/#environment-variables","title":"Environment Variables","text":"<pre><code># Server Configuration\nOLLAMA_HOST=0.0.0.0:11434           # Server bind address\nOLLAMA_ORIGINS=*                     # CORS allowed origins\nOLLAMA_MODELS=~/.ollama/models       # Model storage directory\nOLLAMA_KEEP_ALIVE=5m                 # Model keep-alive duration\n\n# Hardware Configuration\nOLLAMA_NUM_PARALLEL=4                # Number of parallel requests\nOLLAMA_MAX_LOADED_MODELS=3           # Maximum models in memory\nOLLAMA_FLASH_ATTENTION=1             # Enable flash attention\nOLLAMA_LLM_LIBRARY=cpu               # Force CPU inference\n\n# GPU Configuration\nCUDA_VISIBLE_DEVICES=0,1             # CUDA GPU selection\nOLLAMA_GPU_OVERHEAD=0                # GPU memory overhead (bytes)\n\n# Privacy and Security\nOLLAMA_TELEMETRY=false               # Disable telemetry\nOLLAMA_DEBUG=false                   # Enable debug logging\n</code></pre>"},{"location":"services/ollama/#docker-configuration","title":"Docker Configuration","text":"<pre><code>version: '3.8'\nservices:\n  ollama:\n    image: ollama/ollama:latest\n    container_name: ollama\n    ports:\n      - \"${OLLAMA_PORT:-11434}:11434\"\n    volumes:\n      - ollama_models:/root/.ollama\n      - /dev/nvidia.com:/dev/nvidia.com  # GPU access\n    environment:\n      - OLLAMA_HOST=0.0.0.0:11434\n      - OLLAMA_TELEMETRY=false\n      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-5m}\n      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:11434/api/tags\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\nvolumes:\n  ollama_models:\n</code></pre>"},{"location":"services/ollama/#available-models","title":"Available Models","text":""},{"location":"services/ollama/#popular-models","title":"Popular Models","text":"Model Size Description Use Case llama3.2:3b ~2GB Fast, efficient model Quick responses, coding llama3.2:8b ~4.7GB Balanced performance General purpose llama3.1:70b ~40GB High-quality responses Complex reasoning codellama:7b ~3.8GB Code-specialized Programming tasks mistral:7b ~4.1GB Fast, multilingual General chat phi3:3.8b ~2.3GB Microsoft's efficient model Resource-constrained gemma2:9b ~5.5GB Google's Gemma Balanced performance"},{"location":"services/ollama/#specialized-models","title":"Specialized Models","text":"<ul> <li>embedding: Text embeddings for semantic search</li> <li>nomic-embed-text: High-quality text embeddings</li> <li>mxbai-embed-large: Large context embeddings</li> </ul>"},{"location":"services/ollama/#usage-examples","title":"Usage Examples","text":""},{"location":"services/ollama/#basic-chat-completion","title":"Basic Chat Completion","text":"<pre><code>import requests\nimport os\n\ndef chat_with_ollama(message, model=\"llama3.2:3b\"):\n    url = f\"{os.getenv('OLLAMA_HOST', 'http://localhost:11434')}/api/chat\"\n\n    payload = {\n        \"model\": model,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": message}\n        ],\n        \"stream\": False\n    }\n\n    response = requests.post(url, json=payload)\n    return response.json()[\"message\"][\"content\"]\n\n# Example usage\nresponse = chat_with_ollama(\"Explain quantum computing\")\nprint(response)\n</code></pre>"},{"location":"services/ollama/#streaming-response","title":"Streaming Response","text":"<pre><code>import requests\nimport json\n\ndef stream_chat(message, model=\"llama3.2:3b\"):\n    url = f\"{os.getenv('OLLAMA_HOST', 'http://localhost:11434')}/api/chat\"\n\n    payload = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": message}],\n        \"stream\": True\n    }\n\n    with requests.post(url, json=payload, stream=True) as response:\n        for line in response.iter_lines():\n            if line:\n                chunk = json.loads(line.decode('utf-8'))\n                if not chunk.get('done'):\n                    print(chunk['message']['content'], end='', flush=True)\n</code></pre>"},{"location":"services/ollama/#openai-compatible-api","title":"OpenAI-Compatible API","text":"<pre><code>from openai import OpenAI\n\n# Point to local Ollama server\nclient = OpenAI(\n    base_url=f\"{os.getenv('OLLAMA_HOST', 'http://localhost:11434')}/v1\",\n    api_key=\"ollama\"  # Required but ignored\n)\n\nresponse = client.chat.completions.create(\n    model=\"llama3.2:3b\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to calculate fibonacci\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"services/ollama/#model-management","title":"Model Management","text":"<pre><code># Pull a model\ncurl -X POST http://localhost:11434/api/pull \\\n  -d '{\"name\": \"llama3.2:3b\"}'\n\n# List installed models\ncurl http://localhost:11434/api/tags\n\n# Delete a model\ncurl -X DELETE http://localhost:11434/api/delete \\\n  -d '{\"name\": \"llama3.2:3b\"}'\n</code></pre>"},{"location":"services/ollama/#cli-commands","title":"CLI Commands","text":""},{"location":"services/ollama/#basic-operations","title":"Basic Operations","text":"<pre><code># Start Ollama server\nollama serve\n\n# Pull and run a model\nollama run llama3.2:3b\n\n# List available models\nollama list\n\n# Show model information\nollama show llama3.2:3b\n\n# Pull a specific model\nollama pull mistral:7b\n\n# Remove a model\nollama rm llama3.2:3b\n\n# Copy a model\nollama cp llama3.2:3b my-model\n</code></pre>"},{"location":"services/ollama/#creating-custom-models","title":"Creating Custom Models","text":"<pre><code># Create a Modelfile\ncat &gt; Modelfile &lt;&lt; 'EOF'\nFROM llama3.2:3b\nSYSTEM \"You are a helpful coding assistant specialized in Python.\"\nPARAMETER temperature 0.7\nPARAMETER top_p 0.9\nEOF\n\n# Build custom model\nollama create coding-assistant -f Modelfile\n\n# Use custom model\nollama run coding-assistant \"Write a sorting algorithm\"\n</code></pre>"},{"location":"services/ollama/#api-endpoints","title":"API Endpoints","text":""},{"location":"services/ollama/#core-endpoints","title":"Core Endpoints","text":"<ul> <li><code>POST /api/generate</code> - Generate text completion</li> <li><code>POST /api/chat</code> - Chat with model</li> <li><code>POST /api/pull</code> - Download a model</li> <li><code>POST /api/push</code> - Upload a model</li> <li><code>GET /api/tags</code> - List local models</li> <li><code>POST /api/delete</code> - Delete a model</li> <li><code>POST /api/copy</code> - Copy a model</li> <li><code>POST /api/show</code> - Show model information</li> <li><code>POST /api/embeddings</code> - Generate embeddings</li> </ul>"},{"location":"services/ollama/#openai-compatible-endpoints","title":"OpenAI-Compatible Endpoints","text":"<ul> <li><code>GET /v1/models</code> - List available models</li> <li><code>POST /v1/chat/completions</code> - Chat completions</li> <li><code>POST /v1/completions</code> - Text completions</li> <li><code>POST /v1/embeddings</code> - Generate embeddings</li> </ul>"},{"location":"services/ollama/#performance-optimization","title":"Performance Optimization","text":""},{"location":"services/ollama/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>Minimum: 4GB RAM, 2GB disk space</li> <li>Recommended: 16GB+ RAM, GPU with 8GB+ VRAM</li> <li>Optimal: 32GB+ RAM, RTX 4090/A100 GPU</li> </ul>"},{"location":"services/ollama/#model-selection-guidelines","title":"Model Selection Guidelines","text":"<ul> <li>3B models: Fast responses, basic reasoning</li> <li>7B models: Good balance of speed and quality</li> <li>13B+ models: Higher quality, slower responses</li> <li>70B+ models: Best quality, requires significant resources</li> </ul>"},{"location":"services/ollama/#memory-management","title":"Memory Management","text":"<pre><code># Configure model keep-alive\nexport OLLAMA_KEEP_ALIVE=10m\n\n# Limit concurrent models\nexport OLLAMA_MAX_LOADED_MODELS=2\n\n# Control parallel requests\nexport OLLAMA_NUM_PARALLEL=2\n</code></pre>"},{"location":"services/ollama/#troubleshooting","title":"Troubleshooting","text":""},{"location":"services/ollama/#common-issues","title":"Common Issues","text":"<ol> <li>Model Loading Errors</li> <li>Check available disk space</li> <li>Verify model file integrity</li> <li> <p>Restart Ollama service</p> </li> <li> <p>GPU Not Detected</p> </li> <li>Install appropriate GPU drivers</li> <li>Set CUDA_VISIBLE_DEVICES</li> <li> <p>Check GPU memory availability</p> </li> <li> <p>Memory Issues</p> </li> <li>Reduce OLLAMA_NUM_PARALLEL</li> <li>Use smaller models</li> <li> <p>Increase system swap</p> </li> <li> <p>API Connection Issues</p> </li> <li>Verify OLLAMA_HOST setting</li> <li>Check firewall rules</li> <li>Ensure service is running</li> </ol>"},{"location":"services/ollama/#debug-mode","title":"Debug Mode","text":"<pre><code># Enable debug logging\nexport OLLAMA_DEBUG=1\nollama serve\n\n# Check service status\ncurl http://localhost:11434/api/tags\n</code></pre>"},{"location":"services/ollama/#security-considerations","title":"Security Considerations","text":""},{"location":"services/ollama/#network-security","title":"Network Security","text":"<ul> <li>Bind to localhost (127.0.0.1) for local-only access</li> <li>Use reverse proxy with authentication for remote access</li> <li>Configure CORS origins appropriately</li> <li>Monitor API access logs</li> </ul>"},{"location":"services/ollama/#model-security","title":"Model Security","text":"<ul> <li>Verify model checksums after download</li> <li>Use trusted model sources</li> <li>Scan custom models for malicious content</li> <li>Implement rate limiting for public APIs</li> </ul>"},{"location":"services/ollama/#privacy-features","title":"Privacy Features","text":"<ul> <li>All processing happens locally</li> <li>No data sent to external services</li> <li>Telemetry disabled by default</li> <li>Models stored locally in ~/.ollama</li> </ul>"},{"location":"services/ollama/#resources","title":"Resources","text":""},{"location":"services/ollama/#official-links","title":"Official Links","text":"<ul> <li>Website: https://ollama.com/</li> <li>GitHub: https://github.com/ollama/ollama</li> <li>Model Library: https://ollama.com/library</li> <li>Documentation: https://github.com/ollama/ollama/tree/main/docs</li> </ul>"},{"location":"services/ollama/#community-resources","title":"Community Resources","text":"<ul> <li>Discord: https://discord.gg/ollama</li> <li>Reddit: https://reddit.com/r/ollama</li> <li>Model Hub: https://huggingface.co/models?library=gguf</li> </ul>"},{"location":"services/ollama/#integration-examples","title":"Integration Examples","text":"<ul> <li>Python SDK: https://github.com/ollama/ollama-python</li> <li>JavaScript SDK: https://github.com/ollama/ollama-js</li> <li>Go SDK: https://github.com/ollama/ollama/tree/main/api</li> <li>REST API Examples: https://github.com/ollama/ollama/blob/main/docs/api.md</li> </ul>"},{"location":"services/open-webui/","title":"Open WebUI Service","text":"<p>Open WebUI is a feature-rich, self-hosted web interface that provides a ChatGPT-like experience for interacting with various large language models. It offers an extensible platform supporting multiple LLM providers, local models, and advanced chat features.</p>"},{"location":"services/open-webui/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Web Browser] --&gt;|HTTPS/HTTP| B[Open WebUI Frontend]\n    B --&gt;|API Calls| C[Open WebUI Backend]\n    C --&gt;|Proxy Requests| D[LiteLLM Proxy]\n    C --&gt;|Direct Connection| E[Ollama]\n    D --&gt; F[OpenAI API]\n    D --&gt; G[Anthropic API]\n    D --&gt; H[Other Providers]\n    C --&gt;|Store Data| I[SQLite/PostgreSQL]\n    C --&gt;|File Storage| J[Local Storage]\n\n    %% Styling\n    classDef browserClass fill:#e8eaf6,stroke:#283593,stroke-width:2px,color:#000\n    classDef frontendClass fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000\n    classDef backendClass fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px,color:#000\n    classDef proxyClass fill:#fff3e0,stroke:#ef6c00,stroke-width:2px,color:#000\n    classDef localClass fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef apiClass fill:#ffebee,stroke:#c62828,stroke-width:2px,color:#000\n    classDef storageClass fill:#fafafa,stroke:#424242,stroke-width:2px,color:#000\n\n    class A browserClass\n    class B frontendClass\n    class C backendClass\n    class D proxyClass\n    class E localClass\n    class F,G,H apiClass\n    class I,J storageClass</code></pre>"},{"location":"services/open-webui/#key-features","title":"Key Features","text":""},{"location":"services/open-webui/#multi-provider-support","title":"Multi-Provider Support","text":"<ul> <li>Seamless integration with multiple LLM providers</li> <li>Model switching without service restart</li> <li>Provider failover and load balancing</li> <li>Custom API endpoint configuration</li> </ul>"},{"location":"services/open-webui/#advanced-chat-interface","title":"Advanced Chat Interface","text":"<ul> <li>ChatGPT-like conversational experience</li> <li>Markdown rendering with syntax highlighting</li> <li>LaTeX math equation support</li> <li>Code execution capabilities</li> <li>Image generation integration</li> </ul>"},{"location":"services/open-webui/#file-management","title":"File Management","text":"<ul> <li>Document upload and processing</li> <li>PDF, DOCX, and text file support</li> <li>Image upload for vision models</li> <li>Conversation export (JSON, Markdown)</li> </ul>"},{"location":"services/open-webui/#user-management","title":"User Management","text":"<ul> <li>Multi-user support with authentication</li> <li>Role-based access control</li> <li>User registration and profile management</li> <li>API key management</li> </ul>"},{"location":"services/open-webui/#customization-options","title":"Customization Options","text":"<ul> <li>Custom model parameters (temperature, max tokens)</li> <li>System prompts and templates</li> <li>Custom CSS and theming</li> <li>Plugin system for extensions</li> </ul>"},{"location":"services/open-webui/#configuration-schema","title":"Configuration Schema","text":""},{"location":"services/open-webui/#environment-variables","title":"Environment Variables","text":"<pre><code># API Configuration\nOLLAMA_BASE_URL=http://host.docker.internal:11434\nOPENAI_API_BASE_URL=http://litellm:4000/v1\nOPENAI_API_KEY=******************\n\n# Security Configuration\nWEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}\nWEBUI_JWT_SECRET_KEY=${WEBUI_JWT_SECRET_KEY}\n\n# User Management\nDEFAULT_USER_ROLE=admin\nENABLE_SIGNUP=true\nENABLE_LOGIN_FORM=true\n\n# Model Configuration\nDEFAULT_MODELS=gpt-3.5-turbo,gpt-4,claude-3-sonnet\n\n# Interface Settings\nWEBUI_NAME=AI Dev Local\n</code></pre>"},{"location":"services/open-webui/#model-configuration-example","title":"Model Configuration Example","text":"<pre><code>{\n  \"models\": [\n    {\n      \"id\": \"gpt-4\",\n      \"name\": \"GPT-4\",\n      \"provider\": \"openai\",\n      \"parameters\": {\n        \"temperature\": 0.7,\n        \"max_tokens\": 4096,\n        \"top_p\": 1.0\n      }\n    },\n    {\n      \"id\": \"claude-3-sonnet\",\n      \"name\": \"Claude 3 Sonnet\",\n      \"provider\": \"anthropic\",\n      \"parameters\": {\n        \"temperature\": 0.5,\n        \"max_tokens\": 2048\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"services/open-webui/#user-profile-schema","title":"User Profile Schema","text":"<pre><code>{\n  \"id\": \"user-123\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"role\": \"user\",\n  \"preferences\": {\n    \"theme\": \"dark\",\n    \"language\": \"en\",\n    \"default_model\": \"gpt-4\",\n    \"show_timestamps\": true\n  },\n  \"api_keys\": {\n    \"openai\": \"sk-...\",\n    \"anthropic\": \"sk-ant-...\"\n  }\n}\n</code></pre>"},{"location":"services/open-webui/#default-configuration","title":"Default Configuration","text":"<ul> <li>Default Role: admin</li> <li>Signup Enabled: Yes</li> <li>Login Form: Enabled</li> <li>Default Models: gpt-3.5-turbo, gpt-4</li> </ul>"},{"location":"services/open-webui/#access","title":"Access","text":"<p>Open WebUI is accessible at:</p> <pre><code>http://localhost:8081/\n</code></pre>"},{"location":"services/open-webui/#supported-features","title":"Supported Features","text":""},{"location":"services/open-webui/#model-providers","title":"Model Providers","text":"<ul> <li>OpenAI: GPT-3.5, GPT-4, GPT-4 Vision</li> <li>Anthropic: Claude 3 (Haiku, Sonnet, Opus)</li> <li>Google: Gemini Pro, Gemini Pro Vision</li> <li>Local Models: Via Ollama integration</li> <li>Custom APIs: Any OpenAI-compatible endpoint</li> </ul>"},{"location":"services/open-webui/#file-types","title":"File Types","text":"<ul> <li>Documents: PDF, DOCX, TXT, MD</li> <li>Images: PNG, JPG, GIF, WebP (for vision models)</li> <li>Data: CSV, JSON, XML</li> <li>Code: Various programming languages</li> </ul>"},{"location":"services/open-webui/#export-formats","title":"Export Formats","text":"<ul> <li>Conversations: JSON, Markdown, PDF</li> <li>Data: CSV for analytics</li> <li>Images: Generated images and diagrams</li> </ul>"},{"location":"services/open-webui/#advanced-features","title":"Advanced Features","text":""},{"location":"services/open-webui/#function-calling","title":"Function Calling","text":"<ul> <li>Built-in web search capabilities</li> <li>Calculator and math functions</li> <li>Weather and news APIs</li> <li>Custom function definitions</li> </ul>"},{"location":"services/open-webui/#memory-system","title":"Memory System","text":"<ul> <li>Conversation history storage</li> <li>Cross-session memory retention</li> <li>Searchable chat history</li> <li>Context-aware responses</li> </ul>"},{"location":"services/open-webui/#plugin-system","title":"Plugin System","text":"<ul> <li>Community-developed plugins</li> <li>Custom integration capabilities</li> <li>API extensions</li> <li>Third-party service connectors</li> </ul>"},{"location":"services/open-webui/#online-resources","title":"Online Resources","text":"<ul> <li>GitHub Repository: Open WebUI GitHub</li> <li>Official Website: OpenWebUI.com</li> <li>Documentation: Open WebUI Docs</li> <li>Community: Discord Server</li> <li>Docker Hub: Open WebUI Images</li> </ul>"},{"location":"services/open-webui/#use-cases","title":"Use Cases","text":"<ul> <li>Personal AI Assistant: Private ChatGPT-like interface</li> <li>Team Collaboration: Shared AI workspace for teams</li> <li>Development Tool: Code generation and debugging assistant</li> <li>Research Platform: Academic and professional research aid</li> <li>Customer Support: AI-powered support interface</li> <li>Content Creation: Writing and creative assistance</li> </ul>"},{"location":"services/open-webui/#security-features","title":"Security Features","text":"<ul> <li>Authentication: JWT-based user authentication</li> <li>Authorization: Role-based access control</li> <li>Data Privacy: Local data storage options</li> <li>API Security: Secure API key management</li> <li>CORS Protection: Configurable CORS policies</li> </ul> <p>Open WebUI is ideal for individuals and organizations seeking a powerful, self-hosted alternative to commercial AI chat interfaces, with full control over data and customization options.</p>"}]}